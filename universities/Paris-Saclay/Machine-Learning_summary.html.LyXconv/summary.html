<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">

<html>
<head><title>BDMA - Machine Learning</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="TeX4ht (https://tug.org/tex4ht/)" name="generator"/>
<meta content="TeX4ht (https://tug.org/tex4ht/)" name="originator"/>
<!-- html -->
<meta content="summary.tex" name="src"/>
<link href="summary.css" rel="stylesheet" type="text/css"/>
</head><body>
<div class="maketitle">
<h2 class="titleHead">BDMA - Machine Learning</h2>
<div class="author"><span class="ecrm-1200">Jose Antonio Lorencio Abril</span></div>
<br/>
<div class="date"><span class="ecrm-1200">Fall 2023</span></div>
</div>
<div class="center">
<!--l. 101--><p class="noindent">
<!--l. 102--><p class="noindent"><img alt="PIC" src="0_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___cision-Modeling_LectureNotes_source_CS-logo.png"/>
</p></p></div>
<div class="flushright">
<!--l. 105--><p class="noindent">
 Professor: Tom Dupuis
</p></div>
<div class="flushright">
<!--l. 109--><p class="noindent">
 Student e-mail: jose-antonio.lorencio-abril@student-cs.fr
</p></div>
<!--l. 113--><p class="noindent">
<!--l. 120--><p class="noindent">This
is
a
summary
of
the
course
<span class="ecti-1000">Machine</span>
<span class="ecti-1000">Learning</span>
taught
at
the
Université
Paris
Saclay
-
CentraleSupélec
by
Professor
Tom
Dupuis
in
the
academic
year
23/24.
Most
of
the
content
of
this
document
is
adapted
from
the
course
notes
by
Dupuis,
<span class="cite">[<a href="#XDupuis2023">1</a>]</span>,
so
I
won’t
be
citing
it
all
the
time.
Other
references
will
be
provided
                                                                                            
                                                                                            
when
used.
<h3 class="likesectionHead"><a id="x1-1000"></a>Contents</h3>
<div class="tableofcontents">
<span class="partToc">I  <a href="#x1-2000I" id="QQ2-1-2">Deep Learning</a></span>
<br/> <span class="sectionToc">1 <a href="#x1-30001" id="QQ2-1-3">Introduction</a></span>
<br/>  <span class="subsectionToc">1.1 <a href="#x1-40001.1" id="QQ2-1-4">AI History</a></span>
<br/> <span class="sectionToc">2 <a href="#x1-50002" id="QQ2-1-5">Machine Learning Basics</a></span>
<br/>  <span class="subsectionToc">2.1 <a href="#x1-60002.1" id="QQ2-1-6">Linear Algebra Basics</a></span>
<br/>  <span class="subsectionToc">2.2 <a href="#x1-70002.2" id="QQ2-1-7">Probability Basics</a></span>
<br/>  <span class="subsectionToc">2.3 <a href="#x1-80002.3" id="QQ2-1-8">Machine Learning Basics</a></span>
<br/> <span class="sectionToc">3 <a href="#x1-90003" id="QQ2-1-10">Deep Neural Networks</a></span>
<br/>  <span class="subsectionToc">3.1 <a href="#x1-100003.1" id="QQ2-1-11">Perceptron</a></span>
<br/>  <span class="subsectionToc">3.2 <a href="#x1-110003.2" id="QQ2-1-12">Multi-layer perceptron</a></span>
<br/>  <span class="subsectionToc">3.3 <a href="#x1-120003.3" id="QQ2-1-13">Cost Functions</a></span>
<br/>  <span class="subsectionToc">3.4 <a href="#x1-190003.4" id="QQ2-1-20">Why deep NN?</a></span>
<br/>  <span class="subsectionToc">3.5 <a href="#x1-200003.5" id="QQ2-1-21">Gradient-based Learning</a></span>
<br/><span class="partToc">II  <a href="#x1-22000II" id="QQ2-1-23">Reinforcement Learning</a></span>
<br/> <span class="sectionToc">4 <a href="#x1-230004" id="QQ2-1-24">Introduction</a></span>
<br/> <span class="sectionToc">5 <a href="#x1-240005" id="QQ2-1-25">Definition and components</a></span>
<br/>  <span class="subsectionToc">5.1 <a href="#x1-250005.1" id="QQ2-1-26">Maximising the value by taking actions</a></span>
<br/>  <span class="subsectionToc">5.2 <a href="#x1-260005.2" id="QQ2-1-27">Markov Decision Processes</a></span>
<br/>  <span class="subsectionToc">5.3 <a href="#x1-270005.3" id="QQ2-1-28">Policies</a></span>
<br/>  <span class="subsectionToc">5.4 <a href="#x1-280005.4" id="QQ2-1-29">Value Functions</a></span>
<br/>  <span class="subsectionToc">5.5 <a href="#x1-300005.5" id="QQ2-1-31">Model</a></span>
<br/>  <span class="subsectionToc">5.6 <a href="#x1-310005.6" id="QQ2-1-32">Agent categories</a></span>
<br/>  <span class="subsectionToc">5.7 <a href="#x1-320005.7" id="QQ2-1-33">Subproblems of RL</a></span>
<br/> <span class="sectionToc">6 <a href="#x1-330006" id="QQ2-1-34">Markov Decision Processes</a></span>
</div>
<!--l. 124--><p class="noindent">
<h1 class="partHead"><span class="titlemark">Part I<br/></span><a id="x1-2000I"></a>Deep Learning</h1>
<h3 class="sectionHead"><span class="titlemark">1   </span> <a id="x1-30001"></a>Introduction</h3>
<!--l. 130--><p class="noindent"><span class="ecbx-1000">Artificial Intelligence </span>is a wide concept, encompassing different aspects and fields. We can understand the term AI as
the multidisciplinary field of study that aims at recreating human intelligence using artificial means. This is a bit
abstract, and, in fact, there is no single definition for what this means. Intelligence is not fully understood, and thus it
is hard to assess whether an artificial invention has achieved intelligence, further than intuitively thinking
so.
<!--l. 138--><p class="noindent">For instance, AI involves a whole variety of fields:
     <ul class="itemize1">
<li class="itemize">
<!--l. 140--><p class="noindent">Perception
     </p></li>
<li class="itemize">
<!--l. 141--><p class="noindent">Knowledge
     </p></li>
<li class="itemize">
<!--l. 142--><p class="noindent">Cognitive System
     </p></li>
<li class="itemize">
<!--l. 143--><p class="noindent">Planning
     </p></li>
<li class="itemize">
<!--l. 144--><p class="noindent">Robotics
     </p></li>
<li class="itemize">
<!--l. 145--><p class="noindent">Machine Learning (Neural Networks)
     </p></li>
<li class="itemize">
<!--l. 146--><p class="noindent">Natural Language Processing</p></li></ul>
<!--l. 148--><p class="noindent">Leveraging all of these, people try to recreate or even surpass human performance in different tasks. For example, a
computer program that can play chess better than any human could ever possibly play, such as Stockfish, or a
system that is able to understand our messages and reply, based on the knowledge that it has learnt in the
past, such as ChatGPT and similar tools. Other examples are self-driving cars, auto-controlled robots,
etc.
<!--l. 156--><p class="noindent">Therefore, AI is a very wide term, which merges many different scientific fields. <span class="ecbx-1000">Machine Learning</span>, on the
other side, is a narrower term, which deals with the study of the techniques that we can use to make a
computer learn to perform some task. It takes concepts from Statistics, Optimization Theory, Computer
Science, Algorithms, etc. A relevant subclass of Machine Learning, which has come to be one of the most
prominent fields of research in the recent years, is <span class="ecbx-1000">Neural Networks </span>or <span class="ecbx-1000">Deep Learning</span>, which consists on
an ML technique based on the human brain. Many amazing use cases that we see everywhere, like Siri
(Apple assistant), Cortana (Windows assistant), Amazon recommender system, Dall-E (OpenAI image
generation system), etc. Not only this, but the trend is growing, and the interest in DL is continuously
increasing.
                                                                                            
                                                                                            
<!--l. 169--><p class="noindent">This is partly also due to the increase in computing resources, and the continuous optimization that different techniques
are constantly experiencing. For instance, for a model trained on one trillion data points, in 2021 the training process
required around 16500x less compute than a model trained in 2012.
<!--l. 175--><p class="noindent">But not everything is sweet and roses when using DL. Since these systems are being involved in decision making
processes, there are some questions that arise, like whose responsibility is it when a model fails? Moreover, data is
needed to train the models, so it is relevant to address how datasets should be collected, and to respect the privacy of
the people that produce data. In addition, the recent technologies that are able to generate new content and to modify
real content, make it a new issue that AI can create false information, mistrust, and even violence or
paranoia.
<!--l. 185--><p class="noindent">Nonetheless, let’s not focus on the negative, there are lots of nice applications of DL, and it is a key
component to deal with data, achieving higher performance than traditional ML techniques for huge amount of
data.
<!--l. 190--><p class="noindent">
<h4 class="subsectionHead"><span class="titlemark">1.1   </span> <a id="x1-40001.1"></a>AI History</h4>
<!--l. 192--><p class="noindent">In 1950, Alan turing aimed to answer the question ’<span class="ecti-1000">Can machines think?</span>’ through a test, which came to be named the
<span class="ecbx-1000">Turing Test</span>, and consists in a 3 players game. First, a similar game is the following: 2 talkers, a man and a female,
and 1 interrogator. The interrogator asks questions to the talkers, with the aim of determining who is the man and
who is the female. The man tries to trick the interrogator, while the woman tries to help him to identify
her.
<!--l. 200--><p class="noindent">Then, the Turing Test consists in replacing the man by an artificial machine. Turing thought that a machine that could
trick a human interrogator, should be considered intelligent.
<!--l. 204--><p class="noindent">Later, in 1956, in the Dartmouth Workshop organized by IBM, the term <span class="ecbx-1000">Artificial Intelligence </span>was first used to
describe <span class="ecti-1000">every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be</span>
<span class="ecti-1000">made to simulate it</span>.
<!--l. 209--><p class="noindent">From this year on, there was a focus on researching about <span class="ecbx-1000">Symbolic AI</span>, specially in three areas of
research:
     <ul class="itemize1">
<li class="itemize">
<!--l. 212--><p class="noindent">Reasoning as search: a different set of actions leads to a certain goal, so we can try to find the best choice
     of action to obtain the best possible outcome.
     </p></li>
<li class="itemize">
<!--l. 215--><p class="noindent">Natural Language: different tools were developed, following grammar and language rules.
     </p></li>
<li class="itemize">
<!--l. 217--><p class="noindent">Micro world: small block based worlds, that the system can identify and move.</p></li></ul>
<!--l. 220--><p class="noindent">In 1958, the <span class="ecbx-1000">Perceptron </span>was conceived, giving birth to what is called the connectionism, an approach to AI based on
the human brain, and a big hype that encouraged funding to support AI research. At this era, scientists
experience a bit of lack of perspective, thinking that the power of AI was much higher than it was. For
instance, H. A. Simon stated in 1965 that ’<span class="ecti-1000">machines will be capable, within twenty years, of doing any work</span>
<span class="ecti-1000">a man can do.</span>’ We can relate to our time, with the huge hype that AI is experiencing, as well as the
many apocaliptic theories that some people are making. Maybe we are again overestimating the power of
AI.
<!--l. 231--><p class="noindent">The time from 1974 to 1980 is seen as the first winter of AI, in which research was slowed down and funding was
reduced. This was due to several problems found at the time:
                                                                                            
                                                                                            
     <ul class="itemize1">
<li class="itemize">
<!--l. 235--><p class="noindent">There were few computational resources.
     </p></li>
<li class="itemize">
<!--l. 236--><p class="noindent">The models at the time were not scalable.
     </p></li>
<li class="itemize">
<!--l. 237--><p class="noindent">The Moravec’s paradox: it is comparatively easy to make computers exhibit adult level performance on
     intelligence test or playing checkers, and difficult or impossible to give them the skills of a one-year-old
     when it comes to perception and mobility.
     </p></li>
<li class="itemize">
<!--l. 241--><p class="noindent">Marvin Minsky made some devastating critics to connectionism, compared to symbolic, rule-based
     models:
         <ul class="itemize2">
<li class="itemize">
<!--l. 244--><p class="noindent">Limited capacity: Minsky showed that single-layer perceptrons (a simple kind of neural network)
         could not solve certain classes of problems, like the XOR problem. While it was later shown that
         multi-layer perceptrons could solve these problems, Minsky’s work resulted in a shift away from
         neural networks for a time.
         </p></li>
<li class="itemize">
<!--l. 249--><p class="noindent">Lack of clear symbols: Minsky believed that human cognition operates at a higher level with symbols
         and structures (like frames and scripts), rather than just distributed patterns of activation. He often
         argued that connectionist models lacked a clear way to represent these symbolic structures.
         </p></li>
<li class="itemize">
<!--l. 254--><p class="noindent">Generalization and Abstraction: Minsky was concerned that connectionist models struggled with
         generalizing beyond specific training examples or abstracting high-level concepts from raw data.
         </p></li>
<li class="itemize">
<!--l. 257--><p class="noindent">Inefficiency: Minsky pointed out that many problems which seemed simple for symbolic models could
         be extremely computationally intensive for connectionist models.
         </p></li>
<li class="itemize">
<!--l. 260--><p class="noindent">Lack of explanation: Connectionist models, especially when they become complex, can be seen as
         "black boxes", making it difficult to interpret how they arrive at specific conclusions.
         </p></li>
<li class="itemize">
<!--l. 263--><p class="noindent">Over-reliance on learning: Minsky believed that not all knowledge comes from learning from scratch,
         and some of it might be innate or structured in advance. He felt connectionism put too much emphasis
         on learning from raw data.</p></li></ul>
</p></li></ul>
<!--l. 269--><p class="noindent">In 1980, there was a boom in expert knowledge systems that made AI recover interest. An <span class="ecbx-1000">expert system </span>solves
specific tasks following an ensemble of rules based on knowledge facilitated by experts. A remarkable use case
was the XCON sorting system, developed for the Digital Equipment Corporation, which helped them
save 40M$ per year. In addition, connectionism also came again on scene, thanks to the development of
<span class="ecbx-1000">backpropagation </span>applied to neurons, by Geoffrey Hinton. All these achievement made funding to come back to the
field.
                                                                                            
                                                                                            
<!--l. 278--><p class="noindent">Nonetheless, there came a second winter of AI, from 1987 to 1994, mainly because several companies were disappointed
and AI was seen as a technology that couldn’t solve wide varieties of tasks. The funding was withdrawn from the field
and a lot AI companies went bankrupt.
<!--l. 283--><p class="noindent">Luckily, from 1995 there started a new return of AI in the industry. The Moore’s Law states that speed and memory of
computer doubles every two years, and so computing power and memory was rapidly increasing, making the use of AI
systems more feasible each year. During this time, many new concepts were introduced, such as <span class="ecbx-1000">intelligent agents </span>as
systems that perceive their environment and take actions which maximize their chances of success; or different
<span class="ecbx-1000">probabilistic reasoning tools </span>such as Bayesian networks, hidden Markov models, information theory, SVM,... In
addition, AI researchers started to reframe their work in terms of mathematics, computer science, physics, etc., making
the field more attractive for funding. A remarkable milestone during this time was the victory of Deep Blue against
Garry Kasparov.
<!--l. 296--><p class="noindent">The last era of AI comes from 2011 to today, with the advent and popularization of <span class="ecbx-1000">Deep Learning </span>(DL), which are
deep graph processing layers mimicking human neurons interactions. This happened thanks to the advances of hardware
technologies, that have enabled the enormous computing requirements needed for DL. The huge hype comes from the
spectacular results shown by this kind of systems in a huge variety of tasks, such as computer vision, natural language
processing, anomaly detection,...
<!--l. 305--><p class="noindent">In summary, we can see how the history of AI has been a succession of hype and dissapointment cycles, with many
actors involved and the industry as a very important part of the process.
<!--l. 309--><p class="noindent">
<h3 class="sectionHead"><span class="titlemark">2   </span> <a id="x1-50002"></a>Machine Learning Basics</h3>
<!--l. 311--><p class="noindent">In this section, we review some notation, and basic knowledge of Linear Algebra, Probability and Machine
Learning.
<!--l. 314--><p class="noindent">
<h4 class="subsectionHead"><span class="titlemark">2.1   </span> <a id="x1-60002.1"></a>Linear Algebra Basics</h4>
<!--l. 316--><p class="noindent">A <span class="ecbx-1000">scalar </span>is a number, either real and usually denoted <span class="cmmi-10">x </span><span class="cmsy-10">∈ </span><span class="msbm-10">ℝ</span>, or natural and denoted <span class="cmmi-10">n </span><span class="cmsy-10">∈ </span><span class="msbm-10">ℕ</span>. A <span class="ecbx-1000">vector </span>is an array of
numbers, usually real, <span class="cmmi-10">x </span><span class="cmsy-10">∈ </span><span class="msbm-10">ℝ</span><sup><span class="cmmi-7">n</span></sup><span class="cmmi-10">, </span>or
<div class="math-display">
<img alt="    ⌊    ⌋
      x1
    || x2 ||
x = |⌈  ... |⌉ .
      xn
" class="math-display" src="summary0x.png"/></div>
<!--l. 327--><p class="noindent">A <span class="ecbx-1000">matrix </span>is a 2-dimensional array of numbers, <span class="cmmi-10">A </span><span class="cmsy-10">∈ </span><span class="msbm-10">ℝ</span><sup><span class="cmmi-7">n</span><span class="cmsy-7">×</span><span class="cmmi-7">m</span></sup>, or
                                                                                            
                                                                                            
<div class="math-display">
<img alt="    ⌊                ⌋
       A11  ...  A1n
A = |⌈   ...   ...   ...  |⌉ .
       Am1  ... Amn
" class="math-display" src="summary1x.png"/></div>
<!--l. 336--><p class="noindent">A <span class="ecbx-1000">tensor </span>is an <span class="cmmi-10">n</span>-dimensional array of numbers, for example <span class="cmmi-10">A </span><span class="cmsy-10">∈ </span><span class="msbm-10">ℝ</span><sup><span class="cmmi-7">m</span><span class="cmsy-7">×</span><span class="cmmi-7">k</span><span class="cmsy-7">×</span><span class="cmmi-7">p</span></sup> is a 3-dimensional tensor.
<!--l. 339--><p class="noindent">Usually, we will be working with matrices, which can be operated in different ways:
     <ul class="itemize1">
<li class="itemize">
<!--l. 342--><p class="noindent">Transposition: <span class="cmmi-10">A</span><sup><span class="cmmi-7">T</span></sup> is the transposed of <span class="cmmi-10">A</span>, defined as <img align="middle" alt="( T)
A" class="left" src="summary2x.png"/><sub><span class="cmmi-7">ij</span></sub> <span class="cmr-10">= </span><span class="cmmi-10">A</span><sub><span class="cmmi-7">j,i</span></sub>.
     </p></li>
<li class="itemize">
<!--l. 343--><p class="noindent">Multiplication: Let <span class="cmmi-10">A </span><span class="cmsy-10">∈ </span><span class="msbm-10">ℝ</span><sup><span class="cmmi-7">m</span><span class="cmsy-7">×</span><span class="cmmi-7">k</span></sup><span class="cmmi-10">,B </span><span class="cmsy-10">∈ </span><span class="msbm-10">ℝ</span><sup><span class="cmmi-7">k</span><span class="cmsy-7">×</span><span class="cmmi-7">n</span></sup>, their multiplication, <span class="cmmi-10">C </span><span class="cmsy-10">∈ </span><span class="msbm-10">ℝ</span><sup><span class="cmmi-7">m</span><span class="cmsy-7">×</span><span class="cmmi-7">n</span></sup> is defined as
<div class="math-display">
<img alt="                              (          )
                               ∑
C = A ⋅B = AB  = (Cij)i≤m,j≤n =      AikBkj         .
                                k         i≤m,j≤n
" class="math-display" src="summary3x.png"/></div>
<!--l. 348--><p class="noindent">Note that the following holds for every matrix <span class="cmmi-10">A,B</span>:
<div class="math-display">
<img alt="(AB )T = BT AT.
" class="math-display" src="summary4x.png"/></div>
</p></p></li>
<li class="itemize">
<!--l. 352--><p class="noindent">Point-wise operations: if we have two matrices of the same size, <span class="cmmi-10">A,B </span><span class="cmsy-10">∈ </span><span class="msbm-10">ℝ</span><sup><span class="cmmi-7">m</span><span class="cmsy-7">×</span><span class="cmmi-7">n</span></sup>, we can use apply scalar
     operator point-wise to each pair of elements in the same position in the two matrices. For example, the
     sum or the substraction of matrices.</p></li></ul>
<!--l. 357--><p class="noindent">There are also special matrices:
                                                                                            
                                                                                            
     <ul class="itemize1">
<li class="itemize">
<!--l. 359--><p class="noindent">Identity matrix: the identity matrix is a square matrix that preserves any vector it is multiplied with. For
     vectors of size <span class="cmmi-10">n</span>, the identity matrix <span class="cmmi-10">I</span><sub><span class="cmmi-7">n</span></sub> verifies
<div class="math-display">
<img alt="              n
Inx = x,∀x ∈ ℝ .
" class="math-display" src="summary5x.png"/></div>
</p></li>
<li class="itemize">
<!--l. 365--><p class="noindent">Inverse matrix: the inverse of a square matrix, <span class="cmmi-10">A </span><span class="cmsy-10">∈ </span><span class="msbm-10">ℝ</span><sup><span class="cmmi-7">n</span><span class="cmsy-7">×</span><span class="cmmi-7">n</span></sup>, when it exists, is defined as the only matrix
     <span class="cmmi-10">A</span><sup><span class="cmsy-7">-</span><span class="cmr-7">1</span></sup> such that
<div class="math-display">
<img alt="  -1       -1
A   A = AA   = In.
" class="math-display" src="summary6x.png"/></div>
</p></li></ul>
<!--l. 371--><p class="noindent">Another important concept is that of the norm, which is basically measuring how far a point is from the origin of the
space and can be used to measure distances:
<div class="tcolorbox tcolorbox" id="tcolobox-1">
<div class="tcolorbox-title">
</div>
<div class="tcolorbox-content"><div class="newtheorem">
<!--l. 376--><p class="noindent"><span class="head">
<a id="x1-6001r1"></a>
<span class="ecbx-1000">Definition 2.1.</span> </span>A <span class="ecbx-1000">norm </span>is a function <span class="cmmi-10">f </span>that measures the size of vectors, and must have the following
properties:
     <ul class="itemize1">
<li class="itemize">
<!--l. 380--><p class="noindent"><span class="cmmi-10">f</span><img align="middle" alt="(x)" class="left" src="summary7x.png"/> <span class="cmr-10">= 0</span> <span class="cmsy-10">⇐⇒</span> <span class="cmmi-10">x </span><span class="cmr-10">= 0</span><span class="cmmi-10">,</span>
</p></li>
<li class="itemize">
<!--l. 381--><p class="noindent"><span class="cmmi-10">f</span><img align="middle" alt="(x + y)" class="left" src="summary8x.png"/> <span class="cmsy-10">≤ </span><span class="cmmi-10">f</span><img align="middle" alt="(x)" class="left" src="summary9x.png"/> <span class="cmr-10">+ </span><span class="cmmi-10">f</span><img align="middle" alt="(y)" class="left" src="summary10x.png"/><span class="cmmi-10">, </span>and
     </p></li>
<li class="itemize">
<!--l. 382--><p class="noindent"><span class="cmsy-10">∀</span><span class="cmmi-10">α </span><span class="cmsy-10">∈ </span><span class="msbm-10">ℝ</span><span class="cmmi-10">,f</span><img align="middle" alt="(αx)" class="left" src="summary11x.png"/> <span class="cmr-10">= </span><img align="middle" alt="|α |" class="left" src="summary12x.png"/><span class="cmmi-10">f</span><img align="middle" alt="(x)" class="left" src="summary13x.png"/><span class="cmmi-10">.</span></p></li></ul>
</p></div>
<!--l. 384--><p class="noindent">
</p></div>
</div>
<!--l. 387--><p class="noindent">A very important family of norms is the <span class="cmmi-10">L</span><sup><span class="cmmi-7">p</span></sup> norm, defined as
<div class="math-display">
<img alt="      ( ∑     ) 1p
∥x∥ =      |xi|p   .
   p     i
" class="math-display" src="summary14x.png"/></div>
<!--l. 391--><p class="noindent">The <span class="ecbx-1000">Euclidean norm </span>is the <span class="cmmi-10">L</span><sup><span class="cmr-7">2</span></sup> norm, noted <img align="middle" alt="∥x∥" class="left" src="summary15x.png"/> and equivalent to computing <img alt="√xT-x" class="sqrt" src="summary16x.png"/>. In Machine Learning, it is not
uncommon to find the use of the squared Euclidean norm, since it maintains the ordinals and is easier to operate with.
The <span class="ecbx-1000">Manhattan norm </span>is the <span class="cmmi-10">L</span><sup><span class="cmr-7">1</span></sup> norm, and it is used when the difference between zero and nonzero elements is
important. Finally, the <span class="ecbx-1000">Max norm </span>is the <span class="cmmi-10">L</span><sup><span class="cmsy-7">∞</span></sup>, or <img align="middle" alt="∥x ∥" class="left" src="summary17x.png"/><sub><span class="cmsy-7">∞</span></sub> <span class="cmr-10">=</span> <span class="cmr-10">max</span><sub><span class="cmmi-7">i</span></sub><img align="middle" alt="|x |
 i" class="left" src="summary18x.png"/>.
<!--l. 399--><p class="noindent">
<h4 class="subsectionHead"><span class="titlemark">2.2   </span> <a id="x1-70002.2"></a>Probability Basics</h4>
<!--l. 401--><p class="noindent">A <span class="ecbx-1000">random variable</span>, <span class="cmmi-10">X</span>, is a variable that can take different values, <span class="cmmi-10">x</span>, randomly. They can be <span class="ecbx-1000">discrete</span>, like the
number drawn from a dice, or <span class="ecbx-1000">continuous</span>, like the humidity in the air.
<!--l. 406--><p class="noindent">A probability distribution, <span class="cmmi-10">p</span>, is a <span class="ecbx-1000">Probability Mass Function (PMF) </span>for discrete variables, and a <span class="ecbx-1000">Probability</span>
<span class="ecbx-1000">Density Function (PDF) </span>for continuous random variables. It must satisfy:
                                                                                            
                                                                                            
     <ul class="itemize1">
<li class="itemize">
<!--l. 410--><p class="noindent">The domain of <span class="cmmi-10">p </span>describe all possible states of <span class="cmmi-10">X</span>.
     </p></li>
<li class="itemize">
<!--l. 411--><p class="noindent"><span class="cmsy-10">∀</span><span class="cmmi-10">x </span><span class="cmsy-10">∈ </span><span class="cmmi-10">X,p</span><img align="middle" alt="(x)" class="left" src="summary19x.png"/> <span class="cmsy-10">≥ </span><span class="cmr-10">0</span>.
     </p></li>
<li class="itemize">
<!--l. 412--><p class="noindent"><span class="cmex-10">∫</span>
<sub><span class="cmmi-7">x</span><span class="cmsy-7">∈</span><span class="cmmi-7">X</span></sub><span class="cmmi-10">p</span><img align="middle" alt="(x)" class="left" src="summary20x.png"/><span class="cmmi-10">dx </span><span class="cmr-10">= 1</span>.</p></li></ul>
<!--l. 414--><p class="noindent">It is usual to have two (or more) random variables, <span class="cmmi-10">X </span>and <span class="cmmi-10">Y </span>, and to be interested in the probability distribution
of their combination, <span class="cmmi-10">p</span><img align="middle" alt="(x,y)" class="left" src="summary21x.png"/>. In this context, we define the <span class="ecbx-1000">marginal probability </span>of the variable <span class="cmmi-10">X</span>
as
<div class="math-display">
<img alt="          ∫
p(X = x) =     p(x,y)dy,∀x ∈ X.
           y∈Y
" class="math-display" src="summary22x.png"/></div>
<!--l. 421--><p class="noindent">The <span class="ecbx-1000">conditional probability </span>of the variable <span class="cmmi-10">Y </span>conditioned to <span class="cmmi-10">X </span><span class="cmr-10">= </span><span class="cmmi-10">x </span>is
<div class="math-display">
<img alt="                 p(Y = y,X = x)
p(Y = y|X = x) = --P-(X-=-x)---.
" class="math-display" src="summary23x.png"/></div>
<!--l. 426--><p class="noindent">Finally, there is the <span class="ecbx-1000">chain rule of conditional probabilities</span>, in which we start with <span class="cmmi-10">n </span>random variables, <span class="cmmi-10">X</span><sub><span class="cmr-7">1</span></sub><span class="cmmi-10">,...,X</span><sub><span class="cmmi-7">n</span></sub>,
and it follows:
<div class="math-display">
<img alt="                                 ∏n
p (X1 = x1,...,Xn = xn) = p (X1 = x1)  p(Xi = xi|X1 = x1,...,Xi- 1 = xi-1).
                                 i=2
" class="math-display" src="summary24x.png"/></div>
<div class="newtheorem">
<!--l. 433--><p class="noindent"><span class="head">
<a id="x1-7001r1"></a>
<span class="ecbx-1000">Example 2.1.</span> </span>For example, let’s say <span class="cmmi-10">X </span><span class="cmr-10">= </span><img align="middle" alt="{1,2,3}" class="left" src="summary25x.png"/>, <span class="cmmi-10">Y </span><span class="cmr-10">= </span><img align="middle" alt="{1,2}" class="left" src="summary26x.png"/> and <span class="cmmi-10">Z </span><span class="cmr-10">= </span><img align="middle" alt="{1,2}" class="left" src="summary27x.png"/> with the following probabilities:
<div class="center">
<!--l. 436--><p class="noindent">
<div class="tabular"> <table class="tabular" id="TBL-2"><colgroup id="TBL-2-1g"><col id="TBL-2-1"/></colgroup><colgroup id="TBL-2-2g"><col id="TBL-2-2"/></colgroup><colgroup id="TBL-2-3g"><col id="TBL-2-3"/></colgroup><colgroup id="TBL-2-4g"><col id="TBL-2-4"/></colgroup><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-2-1-" style="vertical-align:baseline;"><td class="td11" id="TBL-2-1-1" style="white-space:nowrap; text-align:center;"> <span class="cmmi-10">X  </span></td><td class="td11" id="TBL-2-1-2" style="white-space:nowrap; text-align:center;"> <span class="cmmi-10">Y  </span></td><td class="td11" id="TBL-2-1-3" style="white-space:nowrap; text-align:center;"> <span class="cmmi-10">Z  </span></td><td class="td11" id="TBL-2-1-4" style="white-space:nowrap; text-align:center;"> <span class="cmmi-10">p</span><img align="middle" alt="(x,y,z)" class="left" src="summary28x.png"/> </td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-2-2-" style="vertical-align:baseline;"><td class="td11" id="TBL-2-2-1" style="white-space:nowrap; text-align:center;">  1   </td><td class="td11" id="TBL-2-2-2" style="white-space:nowrap; text-align:center;"> 1  </td><td class="td11" id="TBL-2-2-3" style="white-space:nowrap; text-align:center;"> 1  </td><td class="td11" id="TBL-2-2-4" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="1
6" class="frac" src="summary29x.png"/> </td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-2-3-" style="vertical-align:baseline;"><td class="td11" id="TBL-2-3-1" style="white-space:nowrap; text-align:center;">  1   </td><td class="td11" id="TBL-2-3-2" style="white-space:nowrap; text-align:center;"> 1  </td><td class="td11" id="TBL-2-3-3" style="white-space:nowrap; text-align:center;"> 2  </td><td class="td11" id="TBL-2-3-4" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="1
6" class="frac" src="summary30x.png"/> </td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-2-4-" style="vertical-align:baseline;"><td class="td11" id="TBL-2-4-1" style="white-space:nowrap; text-align:center;">  1   </td><td class="td11" id="TBL-2-4-2" style="white-space:nowrap; text-align:center;"> 2  </td><td class="td11" id="TBL-2-4-3" style="white-space:nowrap; text-align:center;"> 1  </td><td class="td11" id="TBL-2-4-4" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="1-
12" class="frac" src="summary31x.png"/> </td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-2-5-" style="vertical-align:baseline;"><td class="td11" id="TBL-2-5-1" style="white-space:nowrap; text-align:center;">  1   </td><td class="td11" id="TBL-2-5-2" style="white-space:nowrap; text-align:center;"> 2  </td><td class="td11" id="TBL-2-5-3" style="white-space:nowrap; text-align:center;"> 2  </td><td class="td11" id="TBL-2-5-4" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="1-
24" class="frac" src="summary32x.png"/> </td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-2-6-" style="vertical-align:baseline;"><td class="td11" id="TBL-2-6-1" style="white-space:nowrap; text-align:center;">  2   </td><td class="td11" id="TBL-2-6-2" style="white-space:nowrap; text-align:center;"> 1  </td><td class="td11" id="TBL-2-6-3" style="white-space:nowrap; text-align:center;"> 1  </td><td class="td11" id="TBL-2-6-4" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="1-
24" class="frac" src="summary33x.png"/> </td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-2-7-" style="vertical-align:baseline;"><td class="td11" id="TBL-2-7-1" style="white-space:nowrap; text-align:center;">  2   </td><td class="td11" id="TBL-2-7-2" style="white-space:nowrap; text-align:center;"> 1  </td><td class="td11" id="TBL-2-7-3" style="white-space:nowrap; text-align:center;"> 2  </td><td class="td11" id="TBL-2-7-4" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="1-
20" class="frac" src="summary34x.png"/> </td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-2-8-" style="vertical-align:baseline;"><td class="td11" id="TBL-2-8-1" style="white-space:nowrap; text-align:center;">  2   </td><td class="td11" id="TBL-2-8-2" style="white-space:nowrap; text-align:center;"> 2  </td><td class="td11" id="TBL-2-8-3" style="white-space:nowrap; text-align:center;"> 1  </td><td class="td11" id="TBL-2-8-4" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="1-
20" class="frac" src="summary35x.png"/> </td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-2-9-" style="vertical-align:baseline;"><td class="td11" id="TBL-2-9-1" style="white-space:nowrap; text-align:center;">  2   </td><td class="td11" id="TBL-2-9-2" style="white-space:nowrap; text-align:center;"> 2  </td><td class="td11" id="TBL-2-9-3" style="white-space:nowrap; text-align:center;"> 2  </td><td class="td11" id="TBL-2-9-4" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="1-
20" class="frac" src="summary36x.png"/> </td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-2-10-" style="vertical-align:baseline;"><td class="td11" id="TBL-2-10-1" style="white-space:nowrap; text-align:center;">  3   </td><td class="td11" id="TBL-2-10-2" style="white-space:nowrap; text-align:center;"> 1  </td><td class="td11" id="TBL-2-10-3" style="white-space:nowrap; text-align:center;"> 1  </td><td class="td11" id="TBL-2-10-4" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="1
6" class="frac" src="summary37x.png"/> </td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-2-11-" style="vertical-align:baseline;"><td class="td11" id="TBL-2-11-1" style="white-space:nowrap; text-align:center;">  3   </td><td class="td11" id="TBL-2-11-2" style="white-space:nowrap; text-align:center;"> 1  </td><td class="td11" id="TBL-2-11-3" style="white-space:nowrap; text-align:center;"> 2  </td><td class="td11" id="TBL-2-11-4" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="1-
12" class="frac" src="summary38x.png"/> </td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-2-12-" style="vertical-align:baseline;"><td class="td11" id="TBL-2-12-1" style="white-space:nowrap; text-align:center;">  3   </td><td class="td11" id="TBL-2-12-2" style="white-space:nowrap; text-align:center;"> 2  </td><td class="td11" id="TBL-2-12-3" style="white-space:nowrap; text-align:center;"> 1  </td><td class="td11" id="TBL-2-12-4" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="1-
20" class="frac" src="summary39x.png"/> </td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-2-13-" style="vertical-align:baseline;"><td class="td11" id="TBL-2-13-1" style="white-space:nowrap; text-align:center;">  3   </td><td class="td11" id="TBL-2-13-2" style="white-space:nowrap; text-align:center;"> 2  </td><td class="td11" id="TBL-2-13-3" style="white-space:nowrap; text-align:center;"> 2  </td><td class="td11" id="TBL-2-13-4" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="1-
20" class="frac" src="summary40x.png"/> </td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-2-14-" style="vertical-align:baseline;"><td class="td11" id="TBL-2-14-1" style="white-space:nowrap; text-align:center;"> </td>
</tr></table></div>
</p></div>
<!--l. 508--><p class="noindent">Then, the marginal probabilities for the variable <span class="cmmi-10">X </span>are
<div class="math-display">
<img alt="P (X = 1) = 1+ 1 + -1 + 1-=  11,
            6  6   12   24   24
" class="math-display" src="summary41x.png"/></div>
<!--l. 512--><p class="noindent">
<div class="math-display">
<img alt="           -1   1-   1-  -1   -23
P (X  = 2) = 24 + 20 + 20 + 20 = 120,
" class="math-display" src="summary42x.png"/></div>
<!--l. 515--><p class="noindent">
<div class="math-display">
<img alt="P (X = 3) = 1+ -1 + -1 + 1-=  21=  7-.
            6  12   20   20   60   20
" class="math-display" src="summary43x.png"/></div>
<!--l. 519--><p class="noindent">The conditional probability for the event <img align="middle" alt="{Y = 1|X = 3}" class="left" src="summary44x.png"/> is:
<div class="math-display">
<img alt="                 P-(Y-=-1,X--=-3)  -16 +-112  14-   5
P (Y = 1|X  = 3) =    P (X  = 3)   =   -7   = 7- = 7.
                                    20     20
" class="math-display" src="summary45x.png"/></div>
<!--l. 525--><p class="noindent">The conditional probability for the event <img align="middle" alt="{Z = 1|X = 3,Y = 1}" class="left" src="summary46x.png"/> is:
<div class="math-display">
<img alt="                       P (X-=-3,Y-=-1,Z-=-1)  16-  2
P (Z = 1|X = 3,Y = 1) =   P (X = 3,Y = 1)    = 14 = 3.
" class="math-display" src="summary47x.png"/></div>
<!--l. 531--><p class="noindent">The probability of the event <img align="middle" alt="{X  = 3,Y = 1,Z = 1}" class="left" src="summary48x.png"/> could be computed from the conditional probabilities as follows, in
case we only knew these:
                                                                                            
                                                                                            
<table class="align-star">
<tr><td class="align-odd"><span class="cmmi-10">P</span><img align="middle" alt="(X = 3,Y = 1,Z = 1)" class="left" src="summary49x.png"/> <span class="cmr-10">=</span></td> <td class="align-even"><span class="cmmi-10">P</span><img align="middle" alt="(X = 3)" class="left" src="summary50x.png"/> <span class="cmsy-10">⋅ </span><span class="cmmi-10">P</span><img align="middle" alt="(Y = 1|X = 3)" class="left" src="summary51x.png"/> <span class="cmsy-10">⋅ </span><span class="cmmi-10">P</span><img align="middle" alt="(Z = 1|X = 3,Y = 1)" class="left" src="summary52x.png"/></td> <td class="align-label"></td> <td class="align-label">
</td></tr><tr><td class="align-odd"> <span class="cmr-10">=</span></td> <td class="align-even"><img align="middle" alt="-7
20" class="frac" src="summary53x.png"/> <span class="cmsy-10">⋅</span><img align="middle" alt="5
7" class="frac" src="summary54x.png"/> <span class="cmsy-10">⋅</span><img align="middle" alt="2
3" class="frac" src="summary55x.png"/> <span class="cmr-10">=</span> <img align="middle" alt="10
60" class="frac" src="summary56x.png"/> <span class="cmr-10">=</span> <img align="middle" alt="1
6" class="frac" src="summary57x.png"/><span class="cmmi-10">.</span></td> <td class="align-label"></td> <td class="align-label"></td></tr></table>
</p></p></p></p></p></p></p></div>
<!--l. 538--><p class="noindent">
When there are several variables, it is possible that the value of one of them is dependant, somehow, on the values that
the other variables take; or that it is not:
<div class="tcolorbox tcolorbox" id="tcolobox-2">
<div class="tcolorbox-title">
</div>
<div class="tcolorbox-content"><div class="newtheorem">
<!--l. 544--><p class="noindent"><span class="head">
<a id="x1-7002r2"></a>
<span class="ecbx-1000">Definition 2.2.</span> </span>Two random variables <span class="cmmi-10">X </span>and <span class="cmmi-10">Y  </span>are <span class="ecbx-1000">independant</span>, denoted <span class="cmmi-10">X </span><span class="cmsy-10">⊥ </span><span class="cmmi-10">Y </span>, if <span class="cmsy-10">∀</span><span class="cmmi-10">x </span><span class="cmsy-10">∈ </span><span class="cmmi-10">X,y </span><span class="cmsy-10">∈</span>
<span class="cmmi-10">Y,p</span><img align="middle" alt="(X  = x,Y = y)" class="left" src="summary58x.png"/> <span class="cmr-10">= </span><span class="cmmi-10">p</span><img align="middle" alt="(X = x)" class="left" src="summary59x.png"/> <span class="cmsy-10">⋅ </span><span class="cmmi-10">p</span><img align="middle" alt="(Y = y)" class="left" src="summary60x.png"/><span class="cmmi-10">.</span>
<!--l. 548--><p class="noindent"><span class="cmmi-10">X  </span>and  <span class="cmmi-10">Y  </span>are  <span class="ecbx-1000">conditionally  independent  </span>given  the  random  variable  <span class="cmmi-10">Z</span>,  written  <span class="cmmi-10">X  </span><span class="cmsy-10">⊥</span> <sub><span class="cmmi-7">Z</span></sub><span class="cmmi-10">Y  </span>if
<span class="cmsy-10">∀</span><span class="cmmi-10">x </span><span class="cmsy-10">∈ </span><span class="cmmi-10">X,y </span><span class="cmsy-10">∈ </span><span class="cmmi-10">Y,z </span><span class="cmsy-10">∈ </span><span class="cmmi-10">Z</span>,
<div class="math-display">
<img alt="p (X  = x,Y = y|Z = z) = p (X = x|Z = z)⋅p (Y = y|Z = z).
" class="math-display" src="summary61x.png"/></div>
</p></p></div>
<!--l. 553--><p class="noindent">
</p></div>
</div>
<!--l. 556--><p class="noindent">In Statistics and Machine Learning, there are some measures that summarize information about random variables, and
that hold great importance.
<div class="tcolorbox tcolorbox" id="tcolobox-3">
<div class="tcolorbox-title">
</div>
<div class="tcolorbox-content"><div class="newtheorem">
<!--l. 560--><p class="noindent"><span class="head">
<a id="x1-7003r3"></a>
<span class="ecbx-1000">Definition 2.3.</span> </span>The <span class="ecbx-1000">expectation </span>of a function <span class="cmmi-10">f</span><img align="middle" alt="(x)" class="left" src="summary62x.png"/> where <span class="cmmi-10">x </span><span class="cmsy-10">~ </span><span class="cmmi-10">p</span><img align="middle" alt="(x )" class="left" src="summary63x.png"/> is the average value of <span class="cmmi-10">f </span>over <span class="cmmi-10">x</span>:
<div class="math-display">
<img alt="            ∫
Ex~p[f (x)] =    p(x)f (x)dx.
             x∈X
" class="math-display" src="summary64x.png"/></div>
<!--l. 566--><p class="noindent">The <span class="ecbx-1000">variance </span>of <span class="cmmi-10">f</span><img align="middle" alt="(x)" class="left" src="summary65x.png"/> measures how the values of <span class="cmmi-10">f </span>varies from its average:
<div class="math-display">
<img alt="             [               2]
V ar[f (x)] = E (f (x)- E [f (x)]) ,
" class="math-display" src="summary66x.png"/></div>
<!--l. 571--><p class="noindent">and the <span class="ecbx-1000">standard deviation </span>is the square root of the variance.
<!--l. 573--><p class="noindent">The <span class="ecbx-1000">covariance </span>of two random variables provides informaiton about how much two values are linearly
related. More generally, if we apply two functions <span class="cmmi-10">f</span><img align="middle" alt="(x)" class="left" src="summary67x.png"/><span class="cmmi-10">, </span>where <span class="cmmi-10">x </span><span class="cmsy-10">~ </span><span class="cmmi-10">p</span><img align="middle" alt="(x)" class="left" src="summary68x.png"/>, and <span class="cmmi-10">g</span><img align="middle" alt="(y)" class="left" src="summary69x.png"/>, where <span class="cmmi-10">y </span><span class="cmsy-10">~ </span><span class="cmmi-10">p</span><img align="middle" alt="(y)" class="left" src="summary70x.png"/>, the
covariance between them is:
<div class="math-display">
<img alt="Cov[f (x),g(y)] = E[(f (x)- E [f (x)])(g(y)- E[g(y)])].
" class="math-display" src="summary71x.png"/></div>
</p></p></p></p></div>
<!--l. 581--><p class="noindent">
</p></div>
</div>
<!--l. 585--><p class="noindent">
<h4 class="subsectionHead"><span class="titlemark">2.3   </span> <a id="x1-80002.3"></a>Machine Learning Basics</h4>
<!--l. 587--><p class="noindent">To finalize with this review chapter, we are going to remember some basic concepts of Machine Learning.
                                                                                            
                                                                                            
<!--l. 590--><p class="noindent">First, let’s give a definition of the concept:
<div class="tcolorbox tcolorbox" id="tcolobox-4">
<div class="tcolorbox-title">
</div>
<div class="tcolorbox-content"><div class="newtheorem">
<!--l. 593--><p class="noindent"><span class="head">
<a id="x1-8001r4"></a>
<span class="ecbx-1000">Definition 2.4.</span> </span>A computer program is said to <span class="ecbx-1000">learn </span>from experience <span class="cmmi-10">E </span>with respect to some class of
tasks <span class="cmmi-10">T </span>and performance measure <span class="cmmi-10">P</span>, if its performance at tasks in <span class="cmmi-10">T</span>, as measured by <span class="cmmi-10">P</span>, improves with
experience <span class="cmmi-10">E</span>.
</p></div>
<!--l. 598--><p class="noindent">
</p></div>
</div>
<ul class="itemize1">
<li class="itemize">
<!--l. 602--><p class="noindent">The <span class="ecbx-1000">task </span><span class="cmmi-10">T </span>can be classification, regression, translation, generation, anomaly detection,...
     </p></li>
<li class="itemize">
<!--l. 604--><p class="noindent">The <span class="ecbx-1000">performance measure </span><span class="cmmi-10">P </span>is specific to the tasks involved, and can be accuracy for classification,
     for example. It is measured on a <span class="ecbx-1000">test set</span>.
     </p></li>
<li class="itemize">
<!--l. 607--><p class="noindent">The <span class="ecbx-1000">experience </span><span class="cmmi-10">E </span>is divided into two main categories:
         <ul class="itemize2">
<li class="itemize">
<!--l. 609--><p class="noindent"><span class="ecbx-1000">Supervised learning</span>: a dataset of points associated with a label or a target determines the expected
         outcome of each event.
         </p></li>
<li class="itemize">
<!--l. 611--><p class="noindent"><span class="ecbx-1000">Unsupervised  learning</span>:  a  dataset  of  points  without  labels  or  targets,  in  which  the  desirable
         outcome needs to be define in some different way.</p></li></ul>
</p></li></ul>
<!--l. 616--><p class="noindent">Mathematically, we can formalize this as having a dataset of <span class="cmmi-10">m </span>points and <span class="cmmi-10">k </span>features, which can be represented as a
matrix <span class="cmmi-10">X </span><span class="cmsy-10">∈ </span><span class="msbm-10">ℝ</span><sup><span class="cmmi-7">m</span><span class="cmsy-7">×</span><span class="cmmi-7">k</span></sup>. In the case of supervised learning, <span class="cmmi-10">X </span>is associated with a vector of labels, <span class="cmmi-10">y</span>, and we aim to learn a
joint distribution, <span class="cmmi-10">p</span><img align="middle" alt="(X,y)" class="left" src="summary72x.png"/> to infer
<div class="math-display">
<img alt="                   p(x,y)
p (Y = y|X = x) = ∑--′p-(x,y′).
                   y
" class="math-display" src="summary73x.png"/></div>
<!--l. 624--><p class="noindent">The goal is then to find a function <img alt="ˆf" class="circ" src="summary74x.png"/> that associates each <span class="cmmi-10">x </span>to the best approximation of <span class="cmmi-10">y</span>, and that is capable of
generalizing to unseen data. Usually, <img alt="ˆf" class="circ" src="summary75x.png"/> is parameterized by a set of parameters, <span class="cmmi-10">θ</span>, which are learnt during
training.
<!--l. 629--><p class="noindent">The main challenge of an ML model is <span class="ecbx-1000">generalization </span>to unseen data estimated on test data after the training on
training data. <span class="ecbx-1000">Overfitting </span>occurs when the gap between training error and test error is too large, while
<span class="ecbx-1000">underfitting </span>occurs when the training error is too large. The <span class="ecbx-1000">capacity </span>of a model is the range of functions that
it is able to leanr and control how likely the model can overfit or underfit. This is visualized in Figure
<a href="#x1-80021">1<!--tex4ht:ref: fig:Appropriate-capacity,-overfittin --></a>.
<!--l. 637--><p class="noindent"><hr class="figure"/><div class="figure">
<a id="x1-80021"></a>
<!--l. 639--><p class="noindent"><img alt="PIC" src="1_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___achine-Learning_LectureNotes_source_pegado1.png"/>
<br/> <div class="caption"><span class="id">Figure 1: </span><span class="content">Appropriate capacity, overfitting and underfitting visualization.</span></div><!--tex4ht:label?: x1-80021 -->
<!--l. 643--><p class="noindent"></p></p></div><hr class="endfigure"/>
<!--l. 645--><p class="noindent">When we want to train a model, we will define the parameters that characterize it, and then we need to obtain the best
possible of the parameters, according to the data. For this, we use estimators:
<div class="tcolorbox tcolorbox" id="tcolobox-5">
<div class="tcolorbox-title">
</div>
<div class="tcolorbox-content"><div class="newtheorem">
<!--l. 650--><p class="noindent"><span class="head">
<a id="x1-8003r5"></a>
<span class="ecbx-1000">Definition 2.5.</span> </span>Given an unknown parameter <span class="cmmi-10">θ</span>, we estimate it through an <span class="ecbx-1000">estimator</span>, <img alt="ˆ
θ" class="circ" src="summary76x.png"/> . A <span class="ecbx-1000">point</span>
<span class="ecbx-1000">estimator </span>is a function of the data, <span class="cmmi-10">X</span>,
<div class="math-display">
<img alt=" ˆ
θ = g(X ).
" class="math-display" src="summary77x.png"/></div>
<!--l. 657--><p class="noindent">The <span class="ecbx-1000">bias </span>of an estimator is
<div class="math-display">
<img alt="    ( )    [ ]
bias ˆθ  = E ˆθ - θ.
" class="math-display" src="summary78x.png"/></div>
<!--l. 661--><p class="noindent">An estimator is <span class="ecbx-1000">unbiased </span>if <span class="cmmi-10">bias</span><img align="middle" alt="( )
 θˆ" class="left" src="summary79x.png"/> <span class="cmr-10">= 0</span>.
<!--l. 663--><p class="noindent">The <span class="ecbx-1000">variance </span>of an estimator is <span class="cmmi-10">V ar</span><img align="middle" alt="( )
 ˆθ" class="left" src="summary80x.png"/>.
</p></p></p></p></div>
<!--l. 664--><p class="noindent">
</p></div>
</div>
<!--l. 667--><p class="noindent">There are different ways to construct estimators, but one that is frequently used and that has solid mathematical
foundations is the <span class="ecbx-1000">maximum likelihood estimator</span>. Consider a dataset <span class="cmmi-10">X </span><span class="cmr-10">= </span><img align="middle" alt="{x1,...,xn}" class="left" src="summary81x.png"/> and <span class="cmmi-10">p</span><img align="middle" alt="(x;θ)" class="left" src="summary82x.png"/> a parametric
family of probability distribution that maps for each <span class="cmmi-10">x </span>the probability <span class="cmmi-10">p</span><sub><span class="cmmi-7">data</span></sub><img align="middle" alt="(x)" class="left" src="summary83x.png"/>. This is, for each <span class="cmmi-10">θ</span>, <span class="cmmi-10">p</span><img align="middle" alt="(x;θ)" class="left" src="summary84x.png"/> is a
probability density function. The maximum likelihood estimator is then
                                                                                            
                                                                                            
<table class="align-star">
<tr><td class="align-odd"><span class="cmmi-10">θ</span><sub><span class="cmmi-7">ML</span></sub> <span class="cmr-10">=</span></td> <td class="align-even"><span class="cmr-10">arg</span> <span class="cmr-10">max</span><sub><span class="cmmi-7">θ</span></sub><span class="cmmi-10">p</span><sub><span class="cmmi-7">model</span></sub><img align="middle" alt="(X; θ)" class="left" src="summary85x.png"/></td> <td class="align-label"></td> <td class="align-label">
</td></tr><tr><td class="align-odd"> <span class="cmr-10">=</span></td> <td class="align-even"><span class="cmr-10">arg</span> <span class="cmr-10">max</span><sub><span class="cmmi-7">θ</span></sub> <span class="cmex-10">∏</span>
<sub><span class="cmmi-7">i</span><span class="cmr-7">=1</span></sub><sup><span class="cmmi-7">n</span></sup><span class="cmmi-10">p</span><sub>
<span class="cmmi-7">model</span></sub><img align="middle" alt="(xi;θ)" class="left" src="summary86x.png"/><span class="cmmi-10">,</span></td> <td class="align-label"></td> <td class="align-label"></td></tr></table>
<!--l. 678--><p class="noindent">considering that all instances of data are independent and identically distributed (iid). It is also a common practice to
use the maximum <span class="ecbx-1000">log</span>-likelihood instead, removing the product and avoiding floating point issues, since when the
dataset is large, the product will rapidly go to 0. In addition, the logarithm does not modify the ordinals of the
function. Therefore, we can use:
<div class="math-display">
<img alt="             ∑n
θML = argmaxθ    log(pmodel(xi;θ)).
             i=1
" class="math-display" src="summary87x.png"/></div>
<h3 class="sectionHead"><span class="titlemark">3   </span> <a id="x1-90003"></a>Deep Neural Networks</h3>
<!--l. 691--><p class="noindent">
<h4 class="subsectionHead"><span class="titlemark">3.1   </span> <a id="x1-100003.1"></a>Perceptron</h4>
<!--l. 693--><p class="noindent">A deeper explanation of the perceptron can be read in my notes from another course,
<a class="url" href="https://lorenc1o.github.io/BDMA_Notes/universities/UPC/Machine_Learning_summary.pdf"><span class="ectt-1000">https://lorenc1o.github.io/BDMA_Notes/universities/UPC/Machine_Learning_summary.pdf</span></a>.
<!--l. 696--><p class="noindent">A perceptron is an algorithm for supervised learning of binary classifiers. That is, we have a dataset <span class="cmmi-10">X </span><span class="cmsy-10">∈ </span><span class="msbm-10">ℝ</span><sup><span class="cmmi-7">n</span><span class="cmsy-7">×</span><span class="cmmi-7">m</span></sup>
associated with a vector of labels <span class="cmmi-10">y </span><span class="cmsy-10">∈</span><img align="middle" alt="{0,1}" class="left" src="summary88x.png"/><sup><span class="cmmi-7">n</span></sup>. Then, the perceptron learns a function <img alt="ˆf" class="circ" src="summary89x.png"/> parametrized by a vector of
weights <span class="cmmi-10">w </span><span class="cmsy-10">∈ </span><span class="msbm-10">ℝ</span><sup><span class="cmmi-7">m</span></sup> and a bias <span class="cmmi-10">b</span>, such that:
<div class="math-display">
<img alt="       {
fˆ(x) = 1  if w ⋅x+ b &gt; 0 .
        0  ~
" class="math-display" src="summary90x.png"/></div>
<!--l. 708--><p class="noindent">Therefore, it is a linear classifier, which divides the input space into two regions separated by a hyperplane. This means
that a perceptron cannot separate non-linear data.
<!--l. 712--><p class="noindent">
<h4 class="subsectionHead"><span class="titlemark">3.2   </span> <a id="x1-110003.2"></a>Multi-layer perceptron</h4>
<!--l. 714--><p class="noindent">A deeper explanation of the MLP can be read in my notes from another course,
<a class="url" href="https://lorenc1o.github.io/BDMA_Notes/universities/UPC/Machine_Learning_summary.pdf"><span class="ectt-1000">https://lorenc1o.github.io/BDMA_Notes/universities/UPC/Machine_Learning_summary.pdf</span></a>.
<!--l. 717--><p class="noindent">When we say ’Deep’ neural network, we refer to a series of stacked perceptrons. However, just like this, the model is still
linear. This is why activation functions are introduced. An <span class="ecbx-1000">activation function </span>is a function that is applied to the
output of a perceptron, to make it non linear.
<!--l. 723--><p class="noindent">For example, ReLU is a piecewise-linear function defined as
<div class="math-display">
<img alt="          {
            z  if z ≥ 0
ReLU (z) =  0  ~      = max {z,0}.
" class="math-display" src="summary91x.png"/></div>
<!--l. 730--><p class="noindent">This function preserves much of the good oprimization properties of a linear function, i.e., it is differentiable (apart
from one point), and its derivative is constant.
<div class="newtheorem">
<!--l. 733--><p class="noindent"><span class="head">
<a id="x1-11001r1"></a>
<span class="ecbx-1000">Example 3.1.</span> </span>Learn the XOR function with a 2-layer MLP.
<!--l. 736--><p class="noindent">The XOR function is represented with the table:
<div class="center">
<!--l. 737--><p class="noindent">
<div class="tabular"> <table class="tabular" id="TBL-3"><colgroup id="TBL-3-1g"><col id="TBL-3-1"/></colgroup><colgroup id="TBL-3-2g"><col id="TBL-3-2"/></colgroup><colgroup id="TBL-3-3g"><col id="TBL-3-3"/></colgroup><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-3-1-" style="vertical-align:baseline;"><td class="td11" id="TBL-3-1-1" style="white-space:nowrap; text-align:center;"> <span class="cmmi-10">x</span><sub><span class="cmr-7">1</span></sub> </td><td class="td11" id="TBL-3-1-2" style="white-space:nowrap; text-align:center;"> <span class="cmmi-10">x</span><sub><span class="cmr-7">2</span></sub> </td><td class="td11" id="TBL-3-1-3" style="white-space:nowrap; text-align:center;"> <span class="cmmi-10">y  </span></td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-3-2-" style="vertical-align:baseline;"><td class="td11" id="TBL-3-2-1" style="white-space:nowrap; text-align:center;">  0   </td><td class="td11" id="TBL-3-2-2" style="white-space:nowrap; text-align:center;">  0   </td><td class="td11" id="TBL-3-2-3" style="white-space:nowrap; text-align:center;"> 0  </td></tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-3-3-" style="vertical-align:baseline;"><td class="td11" id="TBL-3-3-1" style="white-space:nowrap; text-align:center;"> 0 </td><td class="td11" id="TBL-3-3-2" style="white-space:nowrap; text-align:center;"> 1 </td><td class="td11" id="TBL-3-3-3" style="white-space:nowrap; text-align:center;"> 1</td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-3-4-" style="vertical-align:baseline;"><td class="td11" id="TBL-3-4-1" style="white-space:nowrap; text-align:center;">  1   </td><td class="td11" id="TBL-3-4-2" style="white-space:nowrap; text-align:center;">  0   </td><td class="td11" id="TBL-3-4-3" style="white-space:nowrap; text-align:center;"> 1  </td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-3-5-" style="vertical-align:baseline;"><td class="td11" id="TBL-3-5-1" style="white-space:nowrap; text-align:center;">  1   </td><td class="td11" id="TBL-3-5-2" style="white-space:nowrap; text-align:center;">  1   </td><td class="td11" id="TBL-3-5-3" style="white-space:nowrap; text-align:center;"> 0  </td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-3-6-" style="vertical-align:baseline;"><td class="td11" id="TBL-3-6-1" style="white-space:nowrap; text-align:center;"> </td>
</tr></table></div>
</p></div>
<!--l. 764--><p class="noindent">We want to use a 2-layer MLP to learn this function:
                                                                                            
                                                                                            
<div class="center">
<!--l. 765--><p class="noindent">
<!--l. 766--><p class="noindent"><img alt="PIC" src="2_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___ine-Learning_LectureNotes_source_xor_drawio.png"/>
</p></p></div>
<!--l. 769--><p class="noindent">In <span class="cmmi-10">h</span><sub><span class="cmr-7">1</span></sub>, it will be
<div class="math-display">
<img alt="w11x1 + w12x2 + b1,
" class="math-display" src="summary92x.png"/></div>
<!--l. 773--><p class="noindent">and in <span class="cmmi-10">h</span><sub><span class="cmr-7">2</span></sub>
<div class="math-display">
<img alt="w11x1 + w12x2 + b1.
" class="math-display" src="summary93x.png"/></div>
<!--l. 777--><p class="noindent">This can be represented as
<div class="math-display">
<img alt="      T
h = W h X + bh.
" class="math-display" src="summary94x.png"/></div>
<!--l. 781--><p class="noindent">Then, we apply ReLU
<div class="math-display">
<img alt="    (           )
max  0,WhTX + bh ,
" class="math-display" src="summary95x.png"/></div>
<!--l. 785--><p class="noindent">and finally the output layer
<div class="math-display">
<img alt="     T     (   T       )
y = W y max 0,W h X + bh + by.
" class="math-display" src="summary96x.png"/></div>
<!--l. 790--><p class="noindent">Let’s see the different inputs:
<div class="center">
<!--l. 791--><p class="noindent">
<div class="tabular"> <table class="tabular" id="TBL-4"><colgroup id="TBL-4-1g"><col id="TBL-4-1"/></colgroup><colgroup id="TBL-4-2g"><col id="TBL-4-2"/></colgroup><colgroup id="TBL-4-3g"><col id="TBL-4-3"/></colgroup><colgroup id="TBL-4-4g"><col id="TBL-4-4"/></colgroup><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-4-1-" style="vertical-align:baseline;"><td class="td11" id="TBL-4-1-1" style="white-space:nowrap; text-align:center;"> <span class="cmmi-10">x</span><sub><span class="cmr-7">1</span></sub> </td><td class="td11" id="TBL-4-1-2" style="white-space:nowrap; text-align:center;"> <span class="cmmi-10">x</span><sub><span class="cmr-7">2</span></sub> </td><td class="td11" id="TBL-4-1-3" style="white-space:nowrap; text-align:center;"> <span class="cmmi-10">h                   </span></td><td class="td11" id="TBL-4-1-4" style="white-space:nowrap; text-align:center;"> <span class="cmmi-10">y                          </span></td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-4-2-" style="vertical-align:baseline;"><td class="td11" id="TBL-4-2-1" style="white-space:nowrap; text-align:center;">  0   </td><td class="td11" id="TBL-4-2-2" style="white-space:nowrap; text-align:center;">  0   </td><td class="td11" id="TBL-4-2-3" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="(      )
 w11w12
 w21w22" class="left" src="summary97x.png"/><img align="middle" alt="(  )
  0
  0" class="left" src="summary98x.png"/> <span class="cmr-10">+ </span><img align="middle" alt="(  )
 b1
 b2" class="left" src="summary99x.png"/> <span class="cmr-10">= </span><img align="middle" alt="(  )
 b1
 b2" class="left" src="summary100x.png"/> </td><td class="td11" id="TBL-4-2-4" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="(wy wy)
   1 2" class="left" src="summary101x.png"/><img align="middle" alt="(          )
  max (0,b1)
  max (0,b2)" class="left" src="summary102x.png"/> <span class="cmr-10">+ </span><span class="cmmi-10">b</span><sub><span class="cmmi-7">y</span></sub> <span class="cmsy-10">≤ </span><span class="cmr-10">0          </span></td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-4-3-" style="vertical-align:baseline;"><td class="td11" id="TBL-4-3-1" style="white-space:nowrap; text-align:center;">  0   </td><td class="td11" id="TBL-4-3-2" style="white-space:nowrap; text-align:center;">  1   </td><td class="td11" id="TBL-4-3-3" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="(       )
 w12 +b1
 w22 +b2" class="left" src="summary103x.png"/> </td><td class="td11" id="TBL-4-3-4" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="(wywy)
  1 2" class="left" src="summary104x.png"/><img align="middle" alt="(              )
 max (0,w12 + b1)
 max (0,w22 + b2)" class="left" src="summary105x.png"/> <span class="cmr-10">+ </span><span class="cmmi-10">b</span><sub><span class="cmmi-7">y</span></sub> <span class="cmmi-10">&gt; </span><span class="cmr-10">0      </span></td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-4-4-" style="vertical-align:baseline;"><td class="td11" id="TBL-4-4-1" style="white-space:nowrap; text-align:center;">  1   </td><td class="td11" id="TBL-4-4-2" style="white-space:nowrap; text-align:center;">  0   </td><td class="td11" id="TBL-4-4-3" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="(       )
 w11 +b1
 w21 +b2" class="left" src="summary106x.png"/> </td><td class="td11" id="TBL-4-4-4" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="(wywy)
  1 2" class="left" src="summary107x.png"/><img align="middle" alt="(              )
 max (0,w11 + b1)
 max (0,w21 + b2)" class="left" src="summary108x.png"/> <span class="cmr-10">+ </span><span class="cmmi-10">b</span><sub><span class="cmmi-7">y</span></sub> <span class="cmmi-10">&gt; </span><span class="cmr-10">0      </span></td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-4-5-" style="vertical-align:baseline;"><td class="td11" id="TBL-4-5-1" style="white-space:nowrap; text-align:center;">  1   </td><td class="td11" id="TBL-4-5-2" style="white-space:nowrap; text-align:center;">  1   </td><td class="td11" id="TBL-4-5-3" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="(             )
 w11 + w12 + b1
 w21 + w22 + b2" class="left" src="summary109x.png"/> </td><td class="td11" id="TBL-4-5-4" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="(wywy )
  1  2" class="left" src="summary110x.png"/><img align="middle" alt="(                    )
 max (0,w11 + w12 +b1)
 max (0,w21 + w22 +b2)" class="left" src="summary111x.png"/> <span class="cmr-10">+ </span><span class="cmmi-10">b</span><sub><span class="cmmi-7">y</span></sub> <span class="cmsy-10">≤ </span><span class="cmr-10">0  </span></td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-4-6-" style="vertical-align:baseline;"><td class="td11" id="TBL-4-6-1" style="white-space:nowrap; text-align:center;"> </td>
</tr></table></div>
</p></div>
<!--l. 860--><p class="noindent">A solution is:
<div class="math-display">
<img alt="     ( 1  1 )      (  0  )       (  1 )
Wh =   1  1   ,bh =   - 1  ,Wy  =   - 2  ,b = 0.
" class="math-display" src="summary112x.png"/></div>
<!--l. 873--><p class="noindent">Let’s check:
<div class="center">
<!--l. 874--><p class="noindent">
<div class="tabular"> <table class="tabular" id="TBL-5"><colgroup id="TBL-5-1g"><col id="TBL-5-1"/></colgroup><colgroup id="TBL-5-2g"><col id="TBL-5-2"/></colgroup><colgroup id="TBL-5-3g"><col id="TBL-5-3"/></colgroup><colgroup id="TBL-5-4g"><col id="TBL-5-4"/></colgroup><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-5-1-" style="vertical-align:baseline;"><td class="td11" id="TBL-5-1-1" style="white-space:nowrap; text-align:center;"> <span class="cmmi-10">x</span><sub><span class="cmr-7">1</span></sub> </td><td class="td11" id="TBL-5-1-2" style="white-space:nowrap; text-align:center;"> <span class="cmmi-10">x</span><sub><span class="cmr-7">2</span></sub> </td><td class="td11" id="TBL-5-1-3" style="white-space:nowrap; text-align:center;"> <span class="cmmi-10">h        </span></td><td class="td11" id="TBL-5-1-4" style="white-space:nowrap; text-align:center;"> <span class="cmmi-10">y            </span></td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-5-2-" style="vertical-align:baseline;"><td class="td11" id="TBL-5-2-1" style="white-space:nowrap; text-align:center;">  0   </td><td class="td11" id="TBL-5-2-2" style="white-space:nowrap; text-align:center;">  0   </td><td class="td11" id="TBL-5-2-3" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="(   )
  0
  - 1" class="left" src="summary113x.png"/> </td><td class="td11" id="TBL-5-2-4" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="(   )
 1- 2" class="left" src="summary114x.png"/><img align="middle" alt=" ( )
  0
  0" class="left" src="summary115x.png"/> <span class="cmr-10">= 0 </span><span class="cmsy-10">≤ </span><span class="cmr-10">0  </span></td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-5-3-" style="vertical-align:baseline;"><td class="td11" id="TBL-5-3-1" style="white-space:nowrap; text-align:center;">  0   </td><td class="td11" id="TBL-5-3-2" style="white-space:nowrap; text-align:center;">  1   </td><td class="td11" id="TBL-5-3-3" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="(     )
   1
  1- 1" class="left" src="summary116x.png"/> </td><td class="td11" id="TBL-5-3-4" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="(   )
 1- 2" class="left" src="summary117x.png"/><img align="middle" alt=" ( )
  1
  0" class="left" src="summary118x.png"/> <span class="cmr-10">= 1 </span><span class="cmmi-10">&gt; </span><span class="cmr-10">0  </span></td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-5-4-" style="vertical-align:baseline;"><td class="td11" id="TBL-5-4-1" style="white-space:nowrap; text-align:center;">  1   </td><td class="td11" id="TBL-5-4-2" style="white-space:nowrap; text-align:center;">  0   </td><td class="td11" id="TBL-5-4-3" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="(     )
   1
  1- 1" class="left" src="summary119x.png"/> </td><td class="td11" id="TBL-5-4-4" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="(   )
 1- 2" class="left" src="summary120x.png"/><img align="middle" alt=" ( )
  1
  0" class="left" src="summary121x.png"/> <span class="cmr-10">= 1 </span><span class="cmmi-10">&gt; </span><span class="cmr-10">0  </span></td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-5-5-" style="vertical-align:baseline;"><td class="td11" id="TBL-5-5-1" style="white-space:nowrap; text-align:center;">  1   </td><td class="td11" id="TBL-5-5-2" style="white-space:nowrap; text-align:center;">  1   </td><td class="td11" id="TBL-5-5-3" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="(        )
   1+ 1
 1 +1 - 1" class="left" src="summary122x.png"/> </td><td class="td11" id="TBL-5-5-4" style="white-space:nowrap; text-align:center;"> <img align="middle" alt="(   )
 1- 2" class="left" src="summary123x.png"/><img align="middle" alt=" ( )
  2
  1" class="left" src="summary124x.png"/> <span class="cmr-10">= 0 </span><span class="cmsy-10">≤ </span><span class="cmr-10">0  </span></td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td></tr><tr id="TBL-5-6-" style="vertical-align:baseline;"><td class="td11" id="TBL-5-6-1" style="white-space:nowrap; text-align:center;"> </td>
</tr></table></div>
</p></div>
<!--l. 934--><p class="noindent">So, it works! Note that this solution is not unique!
<!--l. 936--><p class="noindent">What happens is actually that the solution for the XOR problem is not linearly separable:
<div class="center">
<!--l. 938--><p class="noindent">
<!--l. 939--><p class="noindent"><img alt="PIC" src="3_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___hine-Learning_LectureNotes_source_xor_space.png"/>
</p></p></div>
<!--l. 942--><p class="noindent">But, the hidden layer transforms this space, making the problem linearly separable, and therefore solvable in the last
layer:
<div class="center">
<!--l. 944--><p class="noindent">
<!--l. 945--><p class="noindent"><img alt="PIC" src="4_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___earning_LectureNotes_source_xor_space_trans.png"/>
</p></p></div>
</p></p></p></p></p></p></p></p></p></p></p></p></p></p></div>
<!--l. 947--><p class="noindent">
<!--l. 949--><p class="noindent">
<h4 class="subsectionHead"><span class="titlemark">3.3   </span> <a id="x1-120003.3"></a>Cost Functions</h4>
<!--l. 951--><p class="noindent">The cost function is important when working with neural networks, because our goal is to ultimately train the model to
solve some problem, and the cost functions will be the function that our model will aim at minimizing, thus guiding the
training process.
<!--l. 956--><p class="noindent">Usually, we will need to choose a cost function <span class="cmsy-10"><img alt="L" class="10x-x-4c" src="cmsy10-4c.png"> </img></span>that is suitable for our problem. Then, we will minimize <span class="cmsy-10"><img alt="L" class="10x-x-4c" src="cmsy10-4c.png"> </img></span>with
stochastic gradient descent, by:
     <ul class="itemize1">
<li class="itemize">
<!--l. 960--><p class="noindent">Training on a training dataset.
                                                                                            
                                                                                            
     </p></li>
<li class="itemize">
<!--l. 961--><p class="noindent">Estimating error on an evaluation dataset.
     </p></li>
<li class="itemize">
<!--l. 962--><p class="noindent">Computing the gradients using backpropagation.</p></li></ul>
<!--l. 964--><p class="noindent">In this process, we will aim to find good local minima, instead of global minimum. This is related to overfitting
(learning only the training data, losing generalization capabilities), and to the empirical fact that deep neural network
have surprisingly good local and non-global optima.
<!--l. 970--><p class="noindent">
<h5 class="subsubsectionHead"><span class="titlemark">3.3.1   </span> <a id="x1-130003.3.1"></a>Choice of cost function</h5>
<!--l. 972--><p class="noindent">In the general case, we use the maximum likelihood principle, taking the output types of the network into account. This
means that we assume our dataset <img align="middle" alt="{x1,...,xn}" class="left" src="summary125x.png"/> to be independently and identically distributed (i.i.d.) from an unknown
distribution, <span class="cmmi-10">p</span><sub><span class="cmmi-7">data</span></sub><img align="middle" alt="(x)" class="left" src="summary126x.png"/>. We choose a parametric model family <span class="cmmi-10">p</span><sub><span class="cmmi-7">model</span></sub><img align="middle" alt="(x;θ)" class="left" src="summary127x.png"/> represented as a neural network, which we use
to estimate an approximation of the true distribution. For this, we utilize the <span class="ecbx-1000">maximum likelihood estimator</span>,
defined as
<div class="math-display">
<img alt="             ∏n
θML = argmaxθ    pmodel(xi;θ).
             i=1
" class="math-display" src="summary128x.png"/></div>
<!--l. 983--><p class="noindent">Usually, to avoid floating point errors, the log-likelihood is used instead:
<div class="math-display">
<img alt="             ∑n
θML = argmaθx    log pmodel(xi;θ).
             i=1
" class="math-display" src="summary129x.png"/></div>
<!--l. 989--><p class="noindent">In the maximum likelihood estimation framework, we might apply activation functions to the output layer to get a
desired structure for our distribution. This choice will also influence the mathematical form of the cost function. For
example, we can use linear units for regression or for Gaussian distributions, sigmoid units for binary classification or
softmax units for multi-class classification.
                                                                                            
                                                                                            
<!--l. 996--><p class="noindent"><span class="paragraphHead"><a id="x1-140003.3.1"></a><span class="ecbx-1000">Linear units for regression</span></span>
A <span class="ecbx-1000">linear output layer </span>is such that, given the features <span class="cmmi-10">h</span>, the output is
<div class="math-display">
<img alt="ˆy = W Th+ b,
" class="math-display" src="summary130x.png"/></div>
<!--l. 1003--><p class="noindent">where <span class="cmmi-10">W </span>is the weights vector and <span class="cmmi-10">b </span>the bias.
<!--l. 1005--><p class="noindent">We can use this to predict real or vector valued variables, such as prices, biometrics,...
<!--l. 1008--><p class="noindent"><span class="paragraphHead"><a id="x1-150003.3.1"></a><span class="ecbx-1000">Linear unit for Gaussian distribution</span></span>
A <span class="ecbx-1000">Gaussian output unit </span>is such that, given features <span class="cmmi-10">h</span>, a linear layer produces a vector <span class="cmmi-10">ŷ</span> representing the mean and
the covariance matrix of a conditional Gaussian distribution:
<div class="math-display">
<img alt="p (y|x) = N (y;ˆy,I).
" class="math-display" src="summary131x.png"/></div>
<!--l. 1017--><p class="noindent">Covariance is usually not modelled or simplified to be diagonal (in which case we need to ensure that the output is
non-negative).
<!--l. 1020--><p class="noindent"><span class="paragraphHead"><a id="x1-160003.3.1"></a><span class="ecbx-1000">Binary classification</span></span>
In this case, the objective is to predict a binary variable, <span class="cmmi-10">y</span>: the neural network must predict <span class="cmmi-10">P</span><img align="middle" alt="(y = 1|x)" class="left" src="summary132x.png"/>. Thus, we must
ensure that the output is a probability, in the interval <img align="middle" alt="[0,1]" class="left" src="summary133x.png"/>. For this, we can take
<div class="math-display">
<img alt="P (y = 1|x) = max {0,min{1,W Th + b}}.
" class="math-display" src="summary134x.png"/></div>
<!--l. 1029--><p class="noindent">The problem with this approach is that if <span class="cmmi-10">W</span><sup><span class="cmmi-7">T</span></sup><span class="cmmi-10">h </span><span class="cmr-10">+ </span><span class="cmmi-10">b</span><img alt="∈∕" class="notin" src="summary135x.png"/><img align="middle" alt="[0,1]" class="left" src="summary136x.png"/> then the gradient is 0 and the training will stop. To solve
this issue, we can use a <span class="ecbx-1000">sigmoid unit</span>, which is
<div class="math-display">
<img alt="     ( T     )   -----1-----
ˆy = σ W  h+ b =  1+ e-W Th+b .
" class="math-display" src="summary137x.png"/></div>
<!--l. 1037--><p class="noindent"><span class="paragraphHead"><a id="x1-170003.3.1"></a><span class="ecbx-1000">Softmax unit for multi-class classification</span></span>
Now our objective is to classify the input data into one among <span class="cmmi-10">N &gt; </span><span class="cmr-10">2 </span>classes. We want to predict <span class="cmmi-10">ŷ</span> with <img alt="ˆyi" class="circ" src="summary138x.png"/><span class="cmmi-10">P</span><img align="middle" alt="(y = i|x)" class="left" src="summary139x.png"/>,
subject to <img alt="yˆi" class="circ" src="summary140x.png"/> <span class="cmsy-10">∈</span><img align="middle" alt="[0,1]" class="left" src="summary141x.png"/><span class="cmmi-10">,</span><span class="cmsy-10">∀</span><span class="cmmi-10">i </span>and <span class="cmex-10">∑</span>
<sub><span class="cmmi-7">i</span></sub><span class="cmmi-10">y</span><sub><span class="cmmi-7">i</span></sub> <span class="cmr-10">= 1</span>.
<!--l. 1043--><p class="noindent">In the output layer we can have <span class="cmmi-10">N </span>perceptrons, each of them computing <span class="cmmi-10">z</span><sub><span class="cmmi-7">i</span></sub> <span class="cmr-10">=</span> <span class="cmr-10">log</span> <span class="cmmi-10">P</span><img align="middle" alt="(y = i|x)" class="left" src="summary142x.png"/>, i.e., the <span class="ecbx-1000">logits</span>.
With this, we can apply the <span class="ecbx-1000">softmax output unit </span>to all of them, obtaining our vector of probabilities,
as
<div class="math-display">
<img alt="               ezi
softmax (z)i = ∑--ezj.
               j
" class="math-display" src="summary143x.png"/></div>
<!--l. 1052--><p class="noindent">
<h5 class="subsubsectionHead"><span class="titlemark">3.3.2   </span> <a id="x1-180003.3.2"></a>Cross-entropy</h5>
<!--l. 1054--><p class="noindent">In classification problems we want to estimate the probability of different outcomes. Let the estimated probability of
outcome <span class="cmmi-10">i </span>be <span class="cmmi-10">p</span><sub><span class="cmmi-7">model</span></sub><img align="middle" alt="(x = i)" class="left" src="summary144x.png"/> with to-be-optimized parameters <span class="cmmi-10">θ </span>and let the frequency of outcome <span class="cmmi-10">i </span>in the training set be
<span class="cmmi-10">p</span><img align="middle" alt="(x = i)" class="left" src="summary145x.png"/>. Given <span class="cmmi-10">N </span>conditionally independent samples in the training set, then the likelihood of the parameters <span class="cmmi-10">θ </span>of the
model <span class="cmmi-10">p</span><sub><span class="cmmi-7">model</span></sub><img align="middle" alt="(x = i)" class="left" src="summary146x.png"/> on the training set is:
<div class="math-display">
<img alt="       ∏
L(θ) =   pmodel(x = i)N⋅p(x=i) .
      i∈X
" class="math-display" src="summary147x.png"/></div>
<!--l. 1064--><p class="noindent">Therefore, the log-likelihood, divided by <span class="cmmi-10">N</span>, is
<div class="math-display">
<img alt=" 1            1    ∑                          ∑
N- log(L (θ)) = N-N ⋅   p(x = i)logpmodel(x = i) =   p (x = i)logpmodel(x = i).
                   i∈X                         i∈X
" class="math-display" src="summary148x.png"/></div>
<!--l. 1069--><p class="noindent">Cross-entropy minimization is frequently used in optimization and rare-event probability estimation. When comparing a
distribution <span class="cmmi-10">q </span>against a fixed reference distribution <span class="cmmi-10">p</span>, cross-entropy and KL divergence are identical up to an additive
constant (since <span class="cmmi-10">p </span>is fixed): According to the Gibbs’ inequality, both take on their minimal values when <span class="cmmi-10">p </span><span class="cmr-10">= </span><span class="cmmi-10">q</span>, which is <span class="cmr-10">0</span>
for KL divergence, and <span class="cmmi-10">H</span><img align="middle" alt="(p)" class="left" src="summary149x.png"/> for cross-entropy. In the engineering literature, the principle of minimizing KL divergence
(Kullback’s "Principle of Minimum Discrimination Information") is often called the Principle of Minimum
Cross-Entropy (MCE), or Minxent.
<!--l. 1080--><p class="noindent">
<h4 class="subsectionHead"><span class="titlemark">3.4   </span> <a id="x1-190003.4"></a>Why deep NN?</h4>
<!--l. 1082--><p class="noindent">Depth is the longest data path data can take from input to output. For a deep feed forward NN, depth is the number of
hidden layers plus the output layer. State-of-the-art architectures used in practice have dozens to hundreds of
layers.
<div class="tcolorbox tcolorbox" id="tcolobox-6">
<div class="tcolorbox-title">
</div>
<div class="tcolorbox-content"><div class="newtheorem">
<!--l. 1088--><p class="noindent"><span class="head">
<a id="x1-19001r1"></a>
<span class="ecbx-1000">Theorem 3.1.</span> </span><span class="ecti-1000">Universal Approximation Theorem</span>
<!--l. 1091--><p class="noindent"><span class="ecti-1000">Let </span><span class="cmmi-10">φ</span><img align="middle" alt="()" class="left" src="summary150x.png"/> <span class="ecti-1000">be a nonconstant, bounded, and monotonically increasing continuous function. Let </span><span class="cmmi-10">I</span><sub><span class="cmmi-7">m</span><sub><span class="cmr-5">0</span></sub></sub> <span class="ecti-1000">denote</span>
<span class="ecti-1000">the </span><span class="cmmi-10">m</span><sub><span class="cmr-7">0</span></sub><span class="cmsy-10">-</span><span class="ecti-1000">dimensional unit hypercube, </span><img align="middle" alt="[0,1]" class="left" src="summary151x.png"/><sup><span class="cmmi-7">m</span><sub><span class="cmr-5">0</span></sub></sup><span class="ecti-1000">. The space of continuous functions on </span><span class="cmmi-10">I</span><sub><span class="cmmi-7">m</span><sub>
<span class="cmr-5">0</span></sub></sub> <span class="ecti-1000">is denoted by</span>
<span class="cmmi-10">C</span><img align="middle" alt="(Im0)" class="left" src="summary152x.png"/><span class="ecti-1000">.</span>
<!--l. 1096--><p class="noindent"><span class="ecti-1000">Then, given any function </span><span class="cmmi-10">f </span><span class="cmsy-10">∈ </span><span class="cmmi-10">C</span><img align="middle" alt="(Im0)" class="left" src="summary153x.png"/> <span class="ecti-1000">and </span><span class="cmmi-10">ε &gt; </span><span class="cmr-10">0</span><span class="ecti-1000">, there exists an integer </span><span class="cmmi-10">m</span><sub><span class="cmr-7">1</span></sub> <span class="ecti-1000">and sets of real constants</span>
<span class="cmmi-10">α</span><sub><span class="cmmi-7">i</span></sub><span class="cmmi-10">,b</span><sub><span class="cmmi-7">i</span></sub> <span class="ecti-1000">and </span><span class="cmmi-10">w</span><sub><span class="cmmi-7">ij</span></sub> <span class="cmsy-10">∈ </span><span class="msbm-10">ℝ </span><span class="ecti-1000">where </span><span class="cmmi-10">i </span><span class="cmr-10">= 1</span><span class="cmmi-10">,...,m</span><sub><span class="cmr-7">1</span></sub> <span class="ecti-1000">and </span><span class="cmmi-10">j </span><span class="cmr-10">= 1</span><span class="cmmi-10">,...,m</span><sub><span class="cmr-7">0</span></sub> <span class="ecti-1000">such that we may define</span>
<div class="math-display">
<img alt="               (              )
       ∑m1       m∑0
F (x) =   αi ⋅φ(    wij ⋅xj + bi)
        i=1       j=1
" class="math-display" src="summary154x.png"/></div>
<!--l. 1103--><p class="noindent"><span class="ecti-1000">as an approximate realization of the function </span><span class="cmmi-10">f</span><span class="ecti-1000">, that is</span>
<div class="math-display">
<img alt="|F (x) - f (x)| &lt; ε,∀x ∈ Im.
" class="math-display" src="summary155x.png"/></div>
</p></p></p></p></div>
<!--l. 1107--><p class="noindent">
</p></div>
</div>
<!--l. 1110--><p class="noindent">This theorem is very relevant, because it says that for any mapping function <span class="cmmi-10">f </span>in supervised learning,
there exists a MLP with <span class="cmmi-10">m</span><sub><span class="cmr-7">1</span></sub> neurons in the hidden layer which is able to approximate it with a desired
precision.
<!--l. 1115--><p class="noindent">However, it only proves the existence of a shallow (just one hidden layer) MLP with <span class="cmmi-10">m</span><sub><span class="cmr-7">1</span></sub> neurons in the hidden layer
that can approximate the function, but it does not tell how to find this number.
<!--l. 1119--><p class="noindent">As a rule of thumb for the generalization error, it is
<div class="math-display">
<img alt="ε = V-Cdim-(M-LP-),
         N
" class="math-display" src="summary156x.png"/></div>
<!--l. 1123--><p class="noindent">where <span class="cmmi-10">V C</span><sub><span class="cmmi-7">dim</span></sub> is the Vapnik-Chervonenkis dimension, a measure of the capacity of a model. It refers to the largest set of
points that the model can shatter. It is not easy to compute, but a rough upper bound for a FFNN is <span class="cmmi-10">O</span><img align="middle" alt="(W logW )" class="left" src="summary157x.png"/>,
with <span class="cmmi-10">W </span>being the total number of weight in the network.
<!--l. 1129--><p class="noindent">Also, this theorem hints us that having more neurons in the hidden layers will give us better training error, but worse
generalization error: overfitting.
<!--l. 1133--><p class="noindent">However, for most functions <span class="cmmi-10">m</span><sub><span class="cmr-7">1</span></sub> is very high, and becomes quickly computationally intractable: so we need to go
deeper.
<div class="tcolorbox tcolorbox" id="tcolobox-7">
<div class="tcolorbox-title">
</div>
<div class="tcolorbox-content"><div class="newtheorem">
<!--l. 1137--><p class="noindent"><span class="head">
<a id="x1-19002r2"></a>
<span class="ecbx-1000">Theorem 3.2.</span> </span><span class="ecti-1000">No Free Lunch Theorem</span>
<!--l. 1140--><p class="noindent"><span class="ecti-1000">Multiple informal formulations:</span>
<ul class="itemize1">
<li class="itemize">
<!--l. 1142--><p class="noindent"><span class="ecti-1000">For  every  learning  algorithm  A  and  B,  there  are  as  many  problems  where  A  has  a  better</span>
<span class="ecti-1000">generalization error than problems where B has a better one.</span>
</p></li>
<li class="itemize">
<!--l. 1145--><p class="noindent"><span class="ecti-1000">All learning algorithms ahve the same generalization error if we average over all learning problems.</span>
</p></li>
<li class="itemize">
<!--l. 1147--><p class="noindent"><span class="ecti-1000">There is no universally better learning algorithm.</span></p></li></ul>
</p></p></div>
<!--l. 1149--><p class="noindent">
</p></div>
</div>
<div class="tcolorbox tcolorbox" id="tcolobox-8">
<div class="tcolorbox-title">
</div>
<div class="tcolorbox-content"><!--l. 1153--><p class="noindent"><span class="ecbx-1000">Depth Property</span>
<!--l. 1155--><p class="noindent">The number of polygonal regions generated by a MLP with a ReLU function, <span class="cmmi-10">d </span>inputs, <span class="cmmi-10">n </span>neurons per
hidden layer and <span class="cmmi-10">l </span>layers is
<div class="math-display">
<img alt="  ((  )       )
     n d(l-1) d
O    d      n  .
" class="math-display" src="summary158x.png"/></div>
</p></p></div>
</div>
<!--l. 1162--><p class="noindent">This number grows exponentially with depth. This means that adding depth basically allows for more transformations
of the input space.
<!--l. 1165--><p class="noindent">
<h4 class="subsectionHead"><span class="titlemark">3.5   </span> <a id="x1-200003.5"></a>Gradient-based Learning</h4>
<!--l. 1167--><p class="noindent">The <span class="ecbx-1000">gradient </span>of a function <span class="cmmi-10">f </span><span class="cmr-10">: </span><span class="msbm-10">ℝ</span><sup><span class="cmmi-7">n</span></sup> <span class="cmsy-10">→ </span><span class="msbm-10">ℝ</span>, is <span class="cmsy-10">∇</span><span class="cmmi-10">f </span><span class="cmr-10">: </span><span class="msbm-10">ℝ</span><sup><span class="cmmi-7">n</span></sup> <span class="cmsy-10">→ </span><span class="msbm-10">ℝ</span><sup><span class="cmmi-7">n</span></sup>, defined at the point <span class="cmmi-10">p </span><span class="cmr-10">= </span><img align="middle" alt="(x1,...,xn)" class="left" src="summary159x.png"/> as
<div class="math-display">
<img alt="        (  ∂∂fx1 (p))
∇f (p) = |   ..   | .
        (  ∂f.   )
           ∂xn-(p)
" class="math-display" src="summary160x.png"/></div>
<!--l. 1177--><p class="noindent">This is, it’s the local derivative or slope of each dimension at a certain point.
<!--l. 1180--><p class="noindent">Going in the opposite direction of the gradient is a naïve but practical guess of the direction of the local minimum.
This is the base for the gradient descent method.
<div class="tcolorbox tcolorbox" id="tcolobox-9">
<div class="tcolorbox-title">
</div>
<div class="tcolorbox-content"><!--l. 1185--><p class="noindent"><span class="ecbx-1000">Gradient-descent method </span>(Cauchy, 1847)
<!--l. 1187--><p class="noindent">A parametric function <span class="cmmi-10">f</span><img align="middle" alt="(θ)" class="left" src="summary161x.png"/> can be iteratively minimized by following the opposite direction of the
gradient:
<div class="math-display">
<img alt="θ   = θ - ε∇ f (θ),
 t+1    t    θ
" class="math-display" src="summary162x.png"/></div>
<!--l. 1192--><p class="noindent">where <span class="cmmi-10">ε &gt; </span><span class="cmr-10">0 </span>is the <span class="ecbx-1000">learning rate</span>.
<!--l. 1194--><p class="noindent">We stop iterating when the gradient is near to 0.  
</p></p></p></p></div>
</div>
<!--l. 1197--><p class="noindent">Notice that this is useless if we have a close form for the gradient! In that case it is easier to just minimize it. This is
useful when this is not the case, which always happens for neural networks.
<!--l. 1201--><p class="noindent">In addition, there are variations to the method, for example, we can vary <span class="cmmi-10">ε </span>during training.
<div class="tcolorbox tcolorbox" id="tcolobox-10">
<div class="tcolorbox-title">
</div>
<div class="tcolorbox-content"><!--l. 1205--><p class="noindent"><span class="ecbx-1000">Stochastic gradient descent</span>
<!--l. 1207--><p class="noindent">Given a cost function <span class="cmmi-10">f</span><img align="middle" alt="(θ)" class="left" src="summary163x.png"/>, parameters of the network are updated with
<div class="math-display">
<img alt="θ ← θ - ε∇ θf (θ).
" class="math-display" src="summary164x.png"/></div>
<!--l. 1213--><p class="noindent">For the negative log-likelihood (MLE), the function is:
<div class="math-display">
<img alt="       1 ∑m   (        )
f (θ) = m-   L  x(i),y(i),θ  ,
         i=1
" class="math-display" src="summary165x.png"/></div>
<!--l. 1217--><p class="noindent">so the estimated gradient is
<div class="math-display">
<img alt="         1 ∑m     (         )
∇θf (θ) = --   ∇ θL x (i),y(i),θ .
         m i=1
" class="math-display" src="summary166x.png"/></div>
</p></p></p></p></div>
</div>
<!--l. 1223--><p class="noindent">The <span id="textcolor1">problem</span> with this approach is that to take a single step of gradient descent, we must compute the loss
over the whole dataset everytime, making the method not scalable at all. This is called <span class="ecbx-1000">batch gradient</span>
<span class="ecbx-1000">descent</span>.
<!--l. 1228--><p class="noindent">One <span id="textcolor2">solution</span> is to compute the gradient with 1 sample only at each step, which is very noisy and inefficient, but works.
This is the <span class="ecbx-1000">stochastic gradient descent</span>.
<!--l. 1232--><p class="noindent">In the middle ground, we find the <span class="ecbx-1000">mini-batch gradient descent</span>, which divides the dataset into subsets, and updates
the parameters after processing each of these subsets. A batch is a collection is a collection of samples used at each
iteration for performing SDG in DL. A bigger batch provides a better gradient estimation, and therefore a faster
learning, but also implies more device memory and slower descent.
<!--l. 1240--><p class="noindent">Therefore, there is a tradeoff between money and performance at companies. In practice, a batch is set between 1 to 256
on one GPU.
<!--l. 1243--><p class="noindent">But there is an even <span id="textcolor3"><span class="ecbx-1000">greater problem</span></span>, the computation of the gradient is computationally very costly. To go around
                                                                                            
                                                                                            
this problem, <span class="ecbx-1000">back-propagation </span>was invented, as an efficient technique for gradient computation.
<!--l. 1248--><p class="noindent">
<h5 class="subsubsectionHead"><span class="titlemark">3.5.1   </span> <a id="x1-210003.5.1"></a>Back-propagation</h5>
<!--l. 1250--><p class="noindent">The back-propagation algorithm is based on the chain rule for the derivative of composite functions: if we have
<span class="cmmi-10">y </span><span class="cmr-10">= </span><span class="cmmi-10">g</span><img align="middle" alt="(x)" class="left" src="summary167x.png"/> and <span class="cmmi-10">z </span><span class="cmr-10">= </span><span class="cmmi-10">f</span><img align="middle" alt="(y)" class="left" src="summary168x.png"/> <span class="cmr-10">= </span><span class="cmmi-10">f</span><img align="middle" alt="(g(x))" class="left" src="summary169x.png"/> <span class="cmr-10">= </span><img align="middle" alt="(f ∘g)" class="left" src="summary170x.png"/><img align="middle" alt="(x)" class="left" src="summary171x.png"/>, then
<div class="math-display">
<img alt="df        ′      ′
dx (x) = f (g(x))g (x),
" class="math-display" src="summary172x.png"/></div>
<!--l. 1257--><p class="noindent">or, abusing notation,
<div class="math-display">
<img alt="dz-= dzdy-.
dx   dydx
" class="math-display" src="summary173x.png"/></div>
<!--l. 1261--><p class="noindent">This is generalized to multivariate funtions as follows: let <span class="cmmi-10">x </span><span class="cmsy-10">∈ </span><span class="msbm-10">ℝ</span><sup><span class="cmmi-7">m</span></sup><span class="cmmi-10">,y </span><span class="cmsy-10">∈ </span><span class="msbm-10">ℝ</span><sup><span class="cmmi-7">n</span></sup><span class="cmmi-10">,g </span><span class="cmr-10">: </span><span class="msbm-10">ℝ</span><sup><span class="cmmi-7">m</span></sup> <span class="cmsy-10">→ </span><span class="msbm-10">ℝ</span><sup><span class="cmmi-7">n</span></sup> and <span class="cmmi-10">f </span><span class="cmr-10">: </span><span class="msbm-10">ℝ</span><sup><span class="cmmi-7">n</span></sup> <span class="cmsy-10">→ </span><span class="msbm-10">ℝ</span>. If
<span class="cmmi-10">z </span><span class="cmr-10">= </span><span class="cmmi-10">f</span><img align="middle" alt="(y)" class="left" src="summary174x.png"/> <span class="cmr-10">= </span><span class="cmmi-10">f</span><img align="middle" alt="(g (x ))" class="left" src="summary175x.png"/>, then
<div class="math-display">
<img alt="∂z      ∑   ∂z ∂yj
∂xi (x) =  ∂yj ∂xi,
         j
" class="math-display" src="summary176x.png"/></div>
<!--l. 1267--><p class="noindent">or,
                                                                                            
                                                                                            
<div class="math-display">
<img alt="      (   )T
∇  z =  ∂y-  ∇  z,
  x     ∂x     y
" class="math-display" src="summary177x.png"/></div>
<!--l. 1271--><p class="noindent">where <img align="middle" alt="(  )
 ∂y
 ∂x" class="left" src="summary178x.png"/> is the Jacobian of <span class="cmmi-10">g</span>.
<!--l. 1274--><p class="noindent">Now, back-propagation is a recursive application of the chain rule, starting from the cost function. The algorithm works
as follows:
     <ol class="enumerate1">
<li class="enumerate" id="x1-21002x1">
<!--l. 1277--><p class="noindent"><span class="ecbx-1000">Forward pass</span>: a feedforward network takes as input <span class="cmmi-10">x </span>and produces the output <span class="cmmi-10">ŷ</span>. The information flows
     from layer to layer.
     </p></li>
<li class="enumerate" id="x1-21004x2">
<!--l. 1280--><p class="noindent"><span class="ecbx-1000">Cost function</span>: compute the error between expected output and actual output.
     </p></li>
<li class="enumerate" id="x1-21006x3">
<!--l. 1282--><p class="noindent"><span class="ecbx-1000">Back-propagate</span>: evaluate the individual gradient of each parameter and propagates them backwards
     to update them. For this, we use the concept of <span class="ecbx-1000">local derivative</span>: the derivative of connected nodes are
     computed locally on the edges of the graph. For non-connected nodes, we multiply the edges connected
     between the nodes, and we sum over all incoming edges.</p></li></ol>
<!--l. 1289--><p class="noindent">If we do this in a forward way, summing over all paths becomes intractable pretty quickly, while when doing it in a
backwards way, it allows to obtain the derivative of the output with respect to every node directly in one pass. This
leads to massive parallelization.
<!--l. 1294--><p class="noindent">I did a more detailed explanation, with visualizations in my previous notes,
<a class="url" href="https://lorenc1o.github.io/BDMA_Notes/universities/UPC/Machine_Learning_summary.pdf"><span class="ectt-1000">https://lorenc1o.github.io/BDMA_Notes/universities/UPC/Machine_Learning_summary.pdf</span></a>.
<!--l. 1297--><p class="noindent">
<h1 class="partHead"><span class="titlemark">Part II<br/></span><a id="x1-22000II"></a>Reinforcement Learning</h1>
<h3 class="sectionHead"><span class="titlemark">4   </span> <a id="x1-230004"></a>Introduction</h3>
<!--l. 1303--><p class="noindent">People and animals learn by interacting with the environment that sorround them, differing from certain other types of
learning. This process is active, rather than passive: the subject needs to perform interactions with the environment, to
obtain knowledge. Also, the interactions are usually sequential, with future interactions possibly depending on earlier
ones.
<!--l. 1310--><p class="noindent">Not only this, but we are goal oriented: we act towards an objective. And, more importantly, we can learn without
examples of optimal behavior! Instead, we optimise some reward signal obtained from the outcome of our
actions.
<!--l. 1315--><p class="noindent">It is in these observation that <span class="ecbx-1000">Reinforcement Learning (RL) </span>arises as a learning paradigm, based on the
<span class="ecbx-1000">interaction loop</span>: there is an agent in an environment; the agent can make actions in the environment, and get
observations from it.
<div class="center">
<!--l. 1319--><p class="noindent">
<!--l. 1320--><p class="noindent"><img alt="PIC" src="5_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___arning_LectureNotes_source_interaction_loop.png"/>
</p></p></div>
<!--l. 1323--><p class="noindent">RL relies on the reward hypothesis:
<div class="tcolorbox tcolorbox" id="tcolobox-11">
<div class="tcolorbox-title">
</div>
<div class="tcolorbox-content"><div class="newtheorem">
<!--l. 1326--><p class="noindent"><span class="head">
<a id="x1-23001r1"></a>
<span class="ecbx-1000">Conjecture 4.1.</span> </span><span class="ecti-1000">Reward Hypothesis</span>
<!--l. 1329--><p class="noindent"><span class="ecti-1000">Any goal can be formalized as the outcome of maximizing a cumulative reward.</span>
</p></p></div>
<!--l. 1331--><p class="noindent">
</p></div>
</div>
<!--l. 1334--><p class="noindent">This hypothesis basically says that every objective that an agent can have, can be stated in terms of maximizing a
reward associated to the actions of the agent with respect to this objective.
<!--l. 1338--><p class="noindent">For example, if the objective of the agent is to fly a helicopter from point A to point B, then the reward could be
negatively affected by the distance to point B, by the time taken to reach B,...
<!--l. 1342--><p class="noindent">Now, it is important to realize that there exist different reasons to learn:
     <ul class="itemize1">
<li class="itemize">
<!--l. 1345--><p class="noindent">Find solutions to problems.
     </p></li>
<li class="itemize">
<!--l. 1346--><p class="noindent">Adapt online to unforseen circumstances.</p></li></ul>
<!--l. 1348--><p class="noindent">Well, RL can provide algorithm for both cases! Note that the second point is not just about generalization, but also to
cope with the so-called data shift, efficiently, during operation.
<!--l. 1352--><p class="noindent">With all this, now we can define RL:
<div class="tcolorbox tcolorbox" id="tcolobox-12">
<div class="tcolorbox-title">
</div>
<div class="tcolorbox-content"><div class="newtheorem">
<!--l. 1355--><p class="noindent"><span class="head">
<a id="x1-23002r1"></a>
<span class="ecbx-1000">Definition 4.1.</span> </span>Reinforcement Learning is the science and framework of learning to make decisions
from interaction.
</p></div>
<!--l. 1358--><p class="noindent">
</p></div>
</div>
<!--l. 1361--><p class="noindent">This requires us to think about time, consequences of actions, experience gathering, future prediction,
uncertainty,...
<!--l. 1364--><p class="noindent">It has a huge potential scope and is a formalisation of the AI problem.
<!--l. 1366--><p class="noindent">
<h3 class="sectionHead"><span class="titlemark">5   </span> <a id="x1-240005"></a>Definition and components</h3>
<div class="tcolorbox tcolorbox" id="tcolobox-13">
<div class="tcolorbox-title">
</div>
<div class="tcolorbox-content"><div class="newtheorem">
<!--l. 1369--><p class="noindent"><span class="head">
<a id="x1-24001r1"></a>
<span class="ecbx-1000">Definition 5.1.</span> </span>The <span class="ecbx-1000">environment </span>is the world of the problem at hand, with <span class="ecbx-1000">agents </span>in it that can
perform actions, over time.
<!--l. 1373--><p class="noindent">At each time step <span class="cmmi-10">t</span>, the agent:
     <ul class="itemize1">
<li class="itemize">
<!--l. 1375--><p class="noindent">Receives observation <span class="cmmi-10">O</span><sub><span class="cmmi-7">t</span></sub> and reward <span class="cmmi-10">R</span><sub><span class="cmmi-7">t</span></sub> from the environment.
     </p></li>
<li class="itemize">
<!--l. 1376--><p class="noindent">Executes action <span class="cmmi-10">A</span><sub><span class="cmmi-7">t</span></sub>.</p></li></ul>
<!--l. 1378--><p class="noindent">And the environment:
     <ul class="itemize1">
<li class="itemize">
<!--l. 1380--><p class="noindent">Receives action <span class="cmmi-10">A</span><sub><span class="cmmi-7">t</span></sub>.
     </p></li>
<li class="itemize">
<!--l. 1381--><p class="noindent">Emits observation <span class="cmmi-10">O</span><sub><span class="cmmi-7">t</span><span class="cmr-7">+1</span></sub> and reward <span class="cmmi-10">R</span><sub><span class="cmmi-7">t</span><span class="cmr-7">+1</span></sub>.</p></li></ul>
</p></p></p></div>
<!--l. 1383--><p class="noindent">
</p></div>
</div>
<!--l. 1386--><p class="noindent">But, what is a reward?
<!--l. 1388--><p class="noindent">A <span class="ecbx-1000">reward</span>, <span class="cmmi-10">R</span><sub><span class="cmmi-7">t</span></sub>, is a scalar feedback signal which indicates how well the agent is doing at step <span class="cmmi-10">t</span>: it defines how well the
goal is being accomplished!
<!--l. 1392--><p class="noindent">Therefore, the agent’s job is to maximize the cummulative reward of the future steps, i.e.,
<div class="math-display">
<img alt="Gt = Rt+1 + Rt+2 + Rt+3 + ...
" class="math-display" src="summary179x.png"/></div>
<!--l. 1397--><p class="noindent">This is called the <span class="ecbx-1000">return</span>. But, when one thinks about it carefully, one realizes that it is hard to know the future
rewards with such precision. Therefore, it is also usual to use the <span class="ecbx-1000">value</span>, which is the expected return, taking into
account the current state, <span class="cmmi-10">s</span>:
                                                                                            
                                                                                            
<div class="math-display">
<img alt="v(s) = E [Gt |St = s].
" class="math-display" src="summary180x.png"/></div>
<!--l. 1405--><p class="noindent">This depends on the actions the agents takes, and the goal is to <span id="textcolor4">maximize it</span>! To achieve this, the agent must pick
suitable actions.
<!--l. 1409--><p class="noindent">Therefore, rewards and values define the utility of states and actions, and in this setup there is no supervised
feedback.
<!--l. 1412--><p class="noindent">Note, also, that this values can be defined recursively as
<div class="math-display">
<img alt="Gt = Rt+1 + Gt+1,
" class="math-display" src="summary181x.png"/></div>
<!--l. 1417--><p class="noindent">
<div class="math-display">
<img alt="v (s) = E[R   + v(S   )|S = s].
          t+1     t+1   t
" class="math-display" src="summary182x.png"/></div>
<!--l. 1421--><p class="noindent">The <span class="ecbx-1000">environment state </span>is the environment’s internal state, which is usually invisible or partially visible to the agent.
It is very important, but it can also contain lots of irrelevant information.
<!--l. 1425--><p class="noindent">An environment is <span class="ecbx-1000">fully observable </span>when the agent can see the full environment state, so every observation reveals
the whole environment state. That is, the agent state could just be the observation:
<div class="math-display">
<img alt="St = Ot.
" class="math-display" src="summary183x.png"/></div>
<!--l. 1432--><p class="noindent">Note that <span class="cmmi-10">S</span><sub><span class="cmmi-7">t</span></sub> is the agent state, not the environment state!
<!--l. 1434--><p class="noindent">
<h4 class="subsectionHead"><span class="titlemark">5.1   </span> <a id="x1-250005.1"></a>Maximising the value by taking actions</h4>
<!--l. 1436--><p class="noindent">As we have outlined, the goal is to select the actions that maximise the value. For this, we may need to take into
account that actions may have long term consequences, delaying rewards. Thus, it may be better to sacrifice immediate
reward to gain long-term reward.
<!--l. 1441--><p class="noindent">The decision making process that for a given state chooses which action to take is called a <span class="ecbx-1000">policy</span>.
<!--l. 1444--><p class="noindent">To decide which action to take, we can also condition the value on actions:
<div class="math-display">
<img alt="q(s,a) = E[Gt|St = s,At = a],
" class="math-display" src="summary184x.png"/></div>
<!--l. 1449--><p class="noindent">so, for a given state <span class="cmmi-10">s</span>, a possible set of actions <span class="cmmi-10">A</span><sub><span class="cmmi-7">t</span></sub><sup><span class="cmmi-7">s</span></sup>, we could decide which action to take as
<div class="math-display">
<img alt="at = maa∈xAst q(s,a).
" class="math-display" src="summary185x.png"/></div>
<!--l. 1455--><p class="noindent">Then, the <span class="ecbx-1000">history </span>is the full sequence of observation, actions and rewards:
<div class="math-display">
<img alt="Ht = O0,A0,R1,O1, ...,Ot-1,At-1,Rt,Ot.
" class="math-display" src="summary186x.png"/></div>
<!--l. 1462--><p class="noindent">
<h4 class="subsectionHead"><span class="titlemark">5.2   </span> <a id="x1-260005.2"></a>Markov Decision Processes</h4>
<!--l. 1464--><p class="noindent">Markov Decision Processes (MDPs) are a useful mathematical framework, defined as:
<div class="tcolorbox tcolorbox" id="tcolobox-14">
<div class="tcolorbox-title">
</div>
<div class="tcolorbox-content"><div class="newtheorem">
<!--l. 1468--><p class="noindent"><span class="head">
<a id="x1-26001r2"></a>
<span class="ecbx-1000">Definition 5.2.</span> </span>A decision process is Markov if
<div class="math-display">
<img alt="p (r,s|St,At) = p (r,s|Ht,At).
" class="math-display" src="summary187x.png"/></div>
</p></div>
<!--l. 1473--><p class="noindent">
</p></div>
</div>
<!--l. 1476--><p class="noindent">This means that the current state is the only information needed to make a decision, we don’t need the full story. For
example, think in a chess game: there are many ways to arrive to a certain position, but it really does not matter how
to got to the position, the past does not affect your choice now.
<!--l. 1482--><p class="noindent">In order for a process to be Markov, full observability is required. When the situation is of <span class="ecbx-1000">partial observability</span>, the
observations are not Markovian, so using the observation as state is not enough to make the decision. This is called a
<span class="ecbx-1000">partially observable Markov decision process (POMDP)</span>. Note that the environment state can still be
Markov, but the agent does not know it. In this case, we might be able to construct a Markov agent
state.
<!--l. 1490--><p class="noindent">In the general case, the agent state is a function of the history:
<div class="math-display">
<img alt="S   = u(S ,A ,R   ,O   ),
 t+1      t  t  t+1  t+1
" class="math-display" src="summary188x.png"/></div>
<!--l. 1494--><p class="noindent">where <span class="cmmi-10">u </span>is a <span class="ecbx-1000">state update function</span>.
<!--l. 1496--><p class="noindent">Usually, the agent state is much smaller than the environment state.
<div class="newtheorem">
<!--l. 1497--><p class="noindent"><span class="head">
<a id="x1-26002r1"></a>
<span class="ecbx-1000">Example 5.1.</span> </span>A not Markov process:
<!--l. 1500--><p class="noindent">Consider the following maze to be the full environment:
<div class="center">
<!--l. 1501--><p class="noindent">
<!--l. 1502--><p class="noindent"><img alt="PIC" src="6_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___achine-Learning_LectureNotes_source_pegado2.png"/>
</p></p></div>
<!--l. 1505--><p class="noindent">And consider the following observations:
<div class="center">
<!--l. 1506--><p class="noindent">
<!--l. 1507--><p class="noindent"><img alt="PIC" src="7_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___achine-Learning_LectureNotes_source_pegado3.png"/><img alt="PIC" src="8_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___achine-Learning_LectureNotes_source_pegado4.png"/>
</p></p></div>
<!--l. 1510--><p class="noindent">They are indistinguishable! This process is not Markov, because only taking into account the current state, we cannot
identify where we are.
</p></p></p></p></div>
<!--l. 1513--><p class="noindent">
To deal with partial observability, agent can construct suitable state representations. Some examples of agent states
are:
     <ul class="itemize1">
<li class="itemize">
<!--l. 1517--><p class="noindent">Last observation: <span class="cmmi-10">S</span><sub><span class="cmmi-7">t</span></sub> <span class="cmr-10">= </span><span class="cmmi-10">O</span><sub><span class="cmmi-7">t</span></sub> (might not be enough).
     </p></li>
<li class="itemize">
<!--l. 1518--><p class="noindent">Complete history: <span class="cmmi-10">S</span><sub><span class="cmmi-7">t</span></sub> <span class="cmr-10">= </span><span class="cmsy-10"><img alt="H" class="10x-x-48" src="cmsy10-48.png"/></span><sub><span class="cmmi-7">t</span></sub> (might be too large).
     </p></li>
<li class="itemize">
<!--l. 1519--><p class="noindent">A generic update: <span class="cmmi-10">S</span><sub><span class="cmmi-7">t</span></sub> <span class="cmr-10">= </span><span class="cmmi-10">u</span><img align="middle" alt="(St-1,At-1,Rt,Ot)" class="left" src="summary189x.png"/> (but how to design <span class="cmmi-10">u</span>?)</p></li></ul>
<!--l. 1522--><p class="noindent">Constructing a fully Markovian agent state is often not feasible and, more importantly, the state should allow for good
policies and value predictions.
<!--l. 1526--><p class="noindent">
<h4 class="subsectionHead"><span class="titlemark">5.3   </span> <a id="x1-270005.3"></a>Policies</h4>
<!--l. 1528--><p class="noindent">As we saw, a <span class="ecbx-1000">policy </span>defines the agent’s behavior: it is a map from the agent state to an action. Policies can be
deterministic,
                                                                                            
                                                                                            
<div class="math-display">
<img alt="A = π (S),
" class="math-display" src="summary190x.png"/></div>
<!--l. 1533--><p class="noindent">or stochastic,
<div class="math-display">
<img alt="π (A |S ) = p(A|S).
" class="math-display" src="summary191x.png"/></div>
<!--l. 1539--><p class="noindent">
<h4 class="subsectionHead"><span class="titlemark">5.4   </span> <a id="x1-280005.4"></a>Value Functions</h4>
<!--l. 1541--><p class="noindent">We saw the value before, which is the expected return. However, it is usual to introduce a <span class="ecbx-1000">discount</span>
<span class="ecbx-1000">factor</span>, <span class="cmmi-10">γ </span><span class="cmsy-10">∈</span><img align="middle" alt="[0,1]" class="left" src="summary192x.png"/>, which trades off importance of immediate and long-term rewards. This way, the value
becomes
<div class="math-display">
<img alt="vπ(s) = E [Gt|St = s,π] = E [Rt+1 + γRt+2 +γ2Rt+3 + ...|St = s,π].
" class="math-display" src="summary193x.png"/></div>
<!--l. 1548--><p class="noindent">The value depends on the policy, <span class="cmmi-10">π</span>, and can be used to evaluate the desirability of states, as well as to select between
actions.
<!--l. 1551--><p class="noindent">Note the role of the discount factor: the higher it is, the higher the focus on long term outcomes.
<!--l. 1554--><p class="noindent">Now, using the recursive expression of the return, <span class="cmmi-10">G</span><sub><span class="cmmi-7">t</span></sub> <span class="cmr-10">= </span><span class="cmmi-10">R</span><sub><span class="cmmi-7">t</span><span class="cmr-7">+1</span></sub> <span class="cmr-10">+ </span><span class="cmmi-10">γG</span><sub><span class="cmmi-7">t</span><span class="cmr-7">+1</span></sub>, we can rewrite the value as
<div class="math-display">
<img alt="vπ (s) = E [Rt+1 + γvπ(St+1)|St = a,At ~ π (s)],
" class="math-display" src="summary194x.png"/></div>
<!--l. 1559--><p class="noindent">where <span class="cmmi-10">A </span><span class="cmsy-10">~ </span><span class="cmmi-10">π</span><img align="middle" alt="(s)" class="left" src="summary195x.png"/> means <span class="cmmi-10">A </span>is chosen by policy <span class="cmmi-10">π </span>in state <span class="cmmi-10">s</span>. This is known as a <span class="ecbx-1000">Bellman equation</span>. A similar equation
holds for the optimal value, i.e., the highest possible value:
<div class="math-display">
<img alt="v*(s) = maax E[Rt+1 + γv* (St+1)|St = s,At = a].
" class="math-display" src="summary196x.png"/></div>
<!--l. 1565--><p class="noindent">Note how this does not depend on a policy, it is just the maximum achievable value from the current
state.
<!--l. 1568--><p class="noindent">
<h5 class="subsubsectionHead"><span class="titlemark">5.4.1   </span> <a id="x1-290005.4.1"></a>Value Function Approximations</h5>
<!--l. 1570--><p class="noindent">Agents often approximate value functions, and with an accurate value function approximation, the agent can behave
optimally, or very well, even in intractably big domains.
<!--l. 1574--><p class="noindent">
<h4 class="subsectionHead"><span class="titlemark">5.5   </span> <a id="x1-300005.5"></a>Model</h4>
<!--l. 1576--><p class="noindent">A <span class="ecbx-1000">model </span>predicts what the environment will do next. For example, <span class="cmsy-10"><img alt="P" class="10x-x-50" src="cmsy10-50.png"> </img></span>predicts the next state, given the current state
and an action:
<div class="math-display">
<img alt="P(s,a,s′) ≈ p (S   = s′|S = s,A = a) .
              t+1      t     t
" class="math-display" src="summary197x.png"/></div>
<!--l. 1582--><p class="noindent">Or <span class="cmsy-10"><img alt="R" class="10x-x-52" src="cmsy10-52.png"> </img></span>predicts the next immediate reward:
<div class="math-display">
<img alt="R (s,a) ≈ E [Rt+1|St = s,At = a].
" class="math-display" src="summary198x.png"/></div>
<!--l. 1586--><p class="noindent">Note that a model does not immediately give us a good policy! We still need to plan and see how actions and states are
related.
<div class="newtheorem">
<!--l. 1588--><p class="noindent"><span class="head">
<a id="x1-30001r2"></a>
<span class="ecbx-1000">Example 5.2.</span> </span>Consider the following maze, where the rewards are -1 per time-step, the actions are to go N, E, S and
W, and the states are the agent’s location:
<div class="center">
<!--l. 1592--><p class="noindent">
<div class="tabular"> <table class="tabular" id="TBL-6"><colgroup id="TBL-6-1g"><col id="TBL-6-1"/></colgroup><colgroup id="TBL-6-2g"><col id="TBL-6-2"/></colgroup><colgroup id="TBL-6-3g"><col id="TBL-6-3"/></colgroup><colgroup id="TBL-6-4g"><col id="TBL-6-4"/></colgroup><colgroup id="TBL-6-5g"><col id="TBL-6-5"/></colgroup><colgroup id="TBL-6-6g"><col id="TBL-6-6"/></colgroup><colgroup id="TBL-6-7g"><col id="TBL-6-7"/></colgroup><colgroup id="TBL-6-8g"><col id="TBL-6-8"/></colgroup><colgroup id="TBL-6-9g"><col id="TBL-6-9"/></colgroup><colgroup id="TBL-6-10g"><col id="TBL-6-10"/></colgroup><tr id="TBL-6-1-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-1-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-2-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-2-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-3-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-3-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-4-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-4-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-5-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-5-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-6-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-6-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-7-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-7-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-8-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-8-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-6-9-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-9-1" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-9-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-9-3" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-9-4" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-9-5" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-9-6" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-9-7" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-9-8" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-9-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-9-10" style="white-space:nowrap; text-align:center;"> </td></tr><tr id="TBL-6-10-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-10-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-11-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-11-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-12-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-12-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-13-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-13-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-14-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-14-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-15-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-15-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-16-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-16-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-17-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-17-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-6-18-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-18-1" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-18-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-18-3" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-18-4" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-18-5" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-18-6" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-18-7" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-18-8" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-18-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-18-10" style="white-space:nowrap; text-align:center;"> </td>
</tr><tr id="TBL-6-19-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-19-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-20-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-20-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-21-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-21-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-22-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-22-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-23-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-23-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-24-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-24-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-25-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-25-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-26-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-26-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-6-27-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-27-1" style="white-space:nowrap; text-align:center;"> Start  </td><td class="td11" id="TBL-6-27-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-27-3" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-27-4" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-27-5" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-27-6" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-27-7" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-27-8" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-27-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-27-10" style="white-space:nowrap; text-align:center;"> </td></tr><tr id="TBL-6-28-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-28-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-29-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-29-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-30-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-30-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-31-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-31-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-32-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-32-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-33-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-33-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-34-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-34-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-35-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-35-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-6-36-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-36-1" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-36-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-36-3" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-36-4" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-36-5" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-36-6" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-36-7" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-36-8" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-36-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-36-10" style="white-space:nowrap; text-align:center;"> </td>
</tr><tr id="TBL-6-37-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-37-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-38-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-38-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-39-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-39-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-40-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-40-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-41-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-41-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-42-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-42-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-43-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-43-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-44-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-44-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-6-45-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-45-1" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-45-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-45-3" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-45-4" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-45-5" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-45-6" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-45-7" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-45-8" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-45-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-45-10" style="white-space:nowrap; text-align:center;"> </td>
</tr><tr id="TBL-6-46-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-46-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-47-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-47-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-48-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-48-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-49-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-49-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-50-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-50-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-51-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-51-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-52-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-52-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-53-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-53-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-6-54-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-54-1" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-54-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-54-3" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-54-4" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-54-5" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-54-6" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-54-7" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-54-8" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-54-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-54-10" style="white-space:nowrap; text-align:center;"> </td>
</tr><tr id="TBL-6-55-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-55-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-56-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-56-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-57-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-57-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-58-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-58-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-59-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-59-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-60-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-60-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-61-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-61-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-62-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-62-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-6-63-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-63-1" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-63-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-63-3" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-63-4" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-63-5" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-63-6" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-63-7" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-63-8" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-63-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-63-10" style="white-space:nowrap; text-align:center;"> End  </td>
</tr><tr id="TBL-6-64-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-64-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-65-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-65-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-66-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-66-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-67-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-67-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-68-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-68-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-69-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-69-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-70-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-70-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-71-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-71-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-6-72-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-72-1" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-72-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-72-3" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-72-4" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-72-5" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-72-6" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-72-7" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-72-8" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-72-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-6-72-10" style="white-space:nowrap; text-align:center;"> </td>
</tr><tr id="TBL-6-73-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-73-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-74-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-74-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-75-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-75-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-76-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-76-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-77-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-77-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-78-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-78-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-79-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-79-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-6-80-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-80-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-6-81-" style="vertical-align:baseline;"><td class="td11" id="TBL-6-81-1" style="white-space:nowrap; text-align:center;"> </td>
</tr></table></div>
</p></div>
<!--l. 1686--><p class="noindent">The following arrows represent the policy, <span class="cmmi-10">π</span><img align="middle" alt="(s)" class="left" src="summary199x.png"/>, for each state <span class="cmmi-10">s</span>:
<div class="center">
<!--l. 1688--><p class="noindent">
<div class="tabular"> <table class="tabular" id="TBL-7"><colgroup id="TBL-7-1g"><col id="TBL-7-1"/></colgroup><colgroup id="TBL-7-2g"><col id="TBL-7-2"/></colgroup><colgroup id="TBL-7-3g"><col id="TBL-7-3"/></colgroup><colgroup id="TBL-7-4g"><col id="TBL-7-4"/></colgroup><colgroup id="TBL-7-5g"><col id="TBL-7-5"/></colgroup><colgroup id="TBL-7-6g"><col id="TBL-7-6"/></colgroup><colgroup id="TBL-7-7g"><col id="TBL-7-7"/></colgroup><colgroup id="TBL-7-8g"><col id="TBL-7-8"/></colgroup><colgroup id="TBL-7-9g"><col id="TBL-7-9"/></colgroup><colgroup id="TBL-7-10g"><col id="TBL-7-10"/></colgroup><tr id="TBL-7-1-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-1-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-2-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-2-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-3-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-3-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-4-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-4-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-5-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-5-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-6-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-6-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-7-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-7-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-8-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-8-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-7-9-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-9-1" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-9-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-9-3" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-9-4" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-9-5" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-9-6" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-9-7" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-9-8" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-9-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-9-10" style="white-space:nowrap; text-align:center;"> </td></tr><tr id="TBL-7-10-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-10-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-11-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-11-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-12-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-12-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-13-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-13-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-14-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-14-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-15-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-15-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-16-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-16-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-17-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-17-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-7-18-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-18-1" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-18-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-18-3" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="9_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___achine-Learning_LectureNotes_source_pegado5.png"/> </td><td class="td11" id="TBL-7-18-4" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="9_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___achine-Learning_LectureNotes_source_pegado5.png"/> </td><td class="td11" id="TBL-7-18-5" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="9_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___achine-Learning_LectureNotes_source_pegado5.png"/> </td><td class="td11" id="TBL-7-18-6" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="9_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___achine-Learning_LectureNotes_source_pegado5.png"/> </td><td class="td11" id="TBL-7-18-7" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="9_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___achine-Learning_LectureNotes_source_pegado5.png"/> </td><td class="td11" id="TBL-7-18-8" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="10_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado9.png"/> </td><td class="td11" id="TBL-7-18-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-18-10" style="white-space:nowrap; text-align:center;"> </td>
</tr><tr id="TBL-7-19-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-19-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-20-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-20-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-21-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-21-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-22-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-22-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-23-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-23-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-24-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-24-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-25-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-25-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-26-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-26-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-7-27-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-27-1" style="white-space:nowrap; text-align:center;"> Start  </td><td class="td11" id="TBL-7-27-2" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="9_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___achine-Learning_LectureNotes_source_pegado5.png"/> </td><td class="td11" id="TBL-7-27-3" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="11_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado6.png"/> </td><td class="td11" id="TBL-7-27-4" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-27-5" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-27-6" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="11_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado6.png"/> </td><td class="td11" id="TBL-7-27-7" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-27-8" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="10_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado9.png"/> </td><td class="td11" id="TBL-7-27-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-27-10" style="white-space:nowrap; text-align:center;"> </td></tr><tr id="TBL-7-28-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-28-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-29-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-29-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-30-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-30-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-31-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-31-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-32-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-32-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-33-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-33-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-34-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-34-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-35-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-35-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-7-36-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-36-1" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-36-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-36-3" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="11_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado6.png"/> </td><td class="td11" id="TBL-7-36-4" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="12_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado8.png"/> </td><td class="td11" id="TBL-7-36-5" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-36-6" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-36-7" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="10_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado9.png"/> </td><td class="td11" id="TBL-7-36-8" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="12_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado8.png"/> </td><td class="td11" id="TBL-7-36-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-36-10" style="white-space:nowrap; text-align:center;"> </td>
</tr><tr id="TBL-7-37-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-37-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-38-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-38-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-39-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-39-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-40-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-40-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-41-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-41-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-42-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-42-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-43-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-43-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-44-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-44-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-7-45-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-45-1" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-45-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-45-3" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-45-4" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="11_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado6.png"/> </td><td class="td11" id="TBL-7-45-5" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="12_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado8.png"/> </td><td class="td11" id="TBL-7-45-6" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-45-7" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="10_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado9.png"/> </td><td class="td11" id="TBL-7-45-8" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-45-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-45-10" style="white-space:nowrap; text-align:center;"> </td>
</tr><tr id="TBL-7-46-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-46-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-47-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-47-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-48-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-48-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-49-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-49-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-50-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-50-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-51-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-51-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-52-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-52-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-53-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-53-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-7-54-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-54-1" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-54-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-54-3" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="10_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado9.png"/> </td><td class="td11" id="TBL-7-54-4" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-54-5" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="11_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado6.png"/> </td><td class="td11" id="TBL-7-54-6" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-54-7" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="9_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___achine-Learning_LectureNotes_source_pegado5.png"/> </td><td class="td11" id="TBL-7-54-8" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="10_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado9.png"/> </td><td class="td11" id="TBL-7-54-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-54-10" style="white-space:nowrap; text-align:center;"> </td>
</tr><tr id="TBL-7-55-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-55-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-56-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-56-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-57-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-57-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-58-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-58-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-59-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-59-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-60-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-60-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-61-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-61-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-62-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-62-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-7-63-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-63-1" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-63-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-63-3" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="9_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___achine-Learning_LectureNotes_source_pegado5.png"/> </td><td class="td11" id="TBL-7-63-4" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="9_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___achine-Learning_LectureNotes_source_pegado5.png"/> </td><td class="td11" id="TBL-7-63-5" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="11_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado6.png"/> </td><td class="td11" id="TBL-7-63-6" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="12_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado8.png"/> </td><td class="td11" id="TBL-7-63-7" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-63-8" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="9_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___achine-Learning_LectureNotes_source_pegado5.png"/> </td><td class="td11" id="TBL-7-63-9" style="white-space:nowrap; text-align:center;"> <img alt="PIC" src="9_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___achine-Learning_LectureNotes_source_pegado5.png"/> </td><td class="td11" id="TBL-7-63-10" style="white-space:nowrap; text-align:center;"> End  </td>
</tr><tr id="TBL-7-64-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-64-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-65-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-65-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-66-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-66-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-67-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-67-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-68-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-68-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-69-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-69-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-70-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-70-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-71-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-71-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-7-72-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-72-1" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-72-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-72-3" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-72-4" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-72-5" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-72-6" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-72-7" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-72-8" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-72-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-7-72-10" style="white-space:nowrap; text-align:center;"> </td>
</tr><tr id="TBL-7-73-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-73-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-74-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-74-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-75-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-75-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-76-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-76-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-77-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-77-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-78-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-78-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-79-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-79-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-7-80-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-80-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-7-81-" style="vertical-align:baseline;"><td class="td11" id="TBL-7-81-1" style="white-space:nowrap; text-align:center;"> </td>
</tr></table></div>
</p></div>
<!--l. 1782--><p class="noindent">In the following one, the numbers represent the value <span class="cmmi-10">v</span><sub><span class="cmmi-7">π</span></sub><img align="middle" alt="(s)" class="left" src="summary200x.png"/> of each state <span class="cmmi-10">s</span>:
</p></p></p></div>
<!--l. 1784--><p class="noindent">
<div class="center">
<!--l. 1785--><p class="noindent">
<div class="tabular"> <table class="tabular" id="TBL-8"><colgroup id="TBL-8-1g"><col id="TBL-8-1"/></colgroup><colgroup id="TBL-8-2g"><col id="TBL-8-2"/></colgroup><colgroup id="TBL-8-3g"><col id="TBL-8-3"/></colgroup><colgroup id="TBL-8-4g"><col id="TBL-8-4"/></colgroup><colgroup id="TBL-8-5g"><col id="TBL-8-5"/></colgroup><colgroup id="TBL-8-6g"><col id="TBL-8-6"/></colgroup><colgroup id="TBL-8-7g"><col id="TBL-8-7"/></colgroup><colgroup id="TBL-8-8g"><col id="TBL-8-8"/></colgroup><colgroup id="TBL-8-9g"><col id="TBL-8-9"/></colgroup><colgroup id="TBL-8-10g"><col id="TBL-8-10"/></colgroup><tr id="TBL-8-1-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-1-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-2-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-2-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-3-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-3-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-4-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-4-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-5-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-5-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-6-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-6-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-7-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-7-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-8-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-8-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-8-9-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-9-1" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-9-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-9-3" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-9-4" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-9-5" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-9-6" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-9-7" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-9-8" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-9-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-9-10" style="white-space:nowrap; text-align:center;"> </td></tr><tr id="TBL-8-10-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-10-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-11-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-11-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-12-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-12-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-13-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-13-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-14-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-14-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-15-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-15-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-16-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-16-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-17-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-17-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-8-18-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-18-1" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-18-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-18-3" style="white-space:nowrap; text-align:center;"> -14  </td><td class="td11" id="TBL-8-18-4" style="white-space:nowrap; text-align:center;"> -13  </td><td class="td11" id="TBL-8-18-5" style="white-space:nowrap; text-align:center;"> -12  </td><td class="td11" id="TBL-8-18-6" style="white-space:nowrap; text-align:center;"> -11  </td><td class="td11" id="TBL-8-18-7" style="white-space:nowrap; text-align:center;"> -10  </td><td class="td11" id="TBL-8-18-8" style="white-space:nowrap; text-align:center;"> -9  </td><td class="td11" id="TBL-8-18-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-18-10" style="white-space:nowrap; text-align:center;"> </td>
</tr><tr id="TBL-8-19-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-19-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-20-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-20-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-21-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-21-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-22-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-22-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-23-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-23-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-24-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-24-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-25-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-25-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-26-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-26-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-8-27-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-27-1" style="white-space:nowrap; text-align:center;"> Start  </td><td class="td11" id="TBL-8-27-2" style="white-space:nowrap; text-align:center;"> -16  </td><td class="td11" id="TBL-8-27-3" style="white-space:nowrap; text-align:center;"> -15  </td><td class="td11" id="TBL-8-27-4" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-27-5" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-27-6" style="white-space:nowrap; text-align:center;"> -12  </td><td class="td11" id="TBL-8-27-7" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-27-8" style="white-space:nowrap; text-align:center;"> -8  </td><td class="td11" id="TBL-8-27-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-27-10" style="white-space:nowrap; text-align:center;"> </td></tr><tr id="TBL-8-28-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-28-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-29-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-29-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-30-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-30-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-31-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-31-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-32-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-32-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-33-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-33-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-34-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-34-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-35-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-35-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-8-36-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-36-1" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-36-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-36-3" style="white-space:nowrap; text-align:center;"> -16  </td><td class="td11" id="TBL-8-36-4" style="white-space:nowrap; text-align:center;"> -17  </td><td class="td11" id="TBL-8-36-5" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-36-6" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-36-7" style="white-space:nowrap; text-align:center;">  -6   </td><td class="td11" id="TBL-8-36-8" style="white-space:nowrap; text-align:center;"> -7  </td><td class="td11" id="TBL-8-36-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-36-10" style="white-space:nowrap; text-align:center;"> </td>
</tr><tr id="TBL-8-37-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-37-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-38-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-38-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-39-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-39-1" style="white-space:nowrap; text-align:center;"></td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-40-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-40-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-41-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-41-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-42-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-42-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-43-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-43-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-44-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-44-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-8-45-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-45-1" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-45-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-45-3" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-45-4" style="white-space:nowrap; text-align:center;"> -18  </td><td class="td11" id="TBL-8-45-5" style="white-space:nowrap; text-align:center;"> -19  </td><td class="td11" id="TBL-8-45-6" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-45-7" style="white-space:nowrap; text-align:center;">  -5   </td><td class="td11" id="TBL-8-45-8" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-45-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-45-10" style="white-space:nowrap; text-align:center;"> </td>
</tr><tr id="TBL-8-46-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-46-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-47-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-47-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-48-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-48-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-49-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-49-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-50-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-50-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-51-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-51-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-52-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-52-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-53-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-53-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-8-54-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-54-1" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-54-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-54-3" style="white-space:nowrap; text-align:center;"> -24  </td><td class="td11" id="TBL-8-54-4" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-54-5" style="white-space:nowrap; text-align:center;"> -20  </td><td class="td11" id="TBL-8-54-6" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-54-7" style="white-space:nowrap; text-align:center;">  -4   </td><td class="td11" id="TBL-8-54-8" style="white-space:nowrap; text-align:center;"> -3  </td><td class="td11" id="TBL-8-54-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-54-10" style="white-space:nowrap; text-align:center;"> </td>
</tr><tr id="TBL-8-55-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-55-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-56-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-56-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-57-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-57-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-58-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-58-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-59-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-59-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-60-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-60-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-61-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-61-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-62-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-62-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-8-63-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-63-1" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-63-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-63-3" style="white-space:nowrap; text-align:center;"> -23  </td><td class="td11" id="TBL-8-63-4" style="white-space:nowrap; text-align:center;"> -22  </td><td class="td11" id="TBL-8-63-5" style="white-space:nowrap; text-align:center;"> -21  </td><td class="td11" id="TBL-8-63-6" style="white-space:nowrap; text-align:center;"> -22  </td><td class="td11" id="TBL-8-63-7" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-63-8" style="white-space:nowrap; text-align:center;"> -2  </td><td class="td11" id="TBL-8-63-9" style="white-space:nowrap; text-align:center;"> -1  </td><td class="td11" id="TBL-8-63-10" style="white-space:nowrap; text-align:center;"> End  </td>
</tr><tr id="TBL-8-64-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-64-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-65-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-65-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-66-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-66-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-67-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-67-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-68-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-68-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-69-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-69-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-70-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-70-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-71-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-71-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-8-72-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-72-1" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-72-2" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-72-3" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-72-4" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-72-5" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-72-6" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-72-7" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-72-8" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-72-9" style="white-space:nowrap; text-align:center;"> </td><td class="td11" id="TBL-8-72-10" style="white-space:nowrap; text-align:center;"> </td>
</tr><tr id="TBL-8-73-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-73-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-74-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-74-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-75-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-75-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-76-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-76-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-77-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-77-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-78-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-78-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-79-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-79-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td><hr/></td><td></td></tr><tr id="TBL-8-80-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-80-1" style="white-space:nowrap; text-align:center;">
</td></tr><tr class="cline"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><hr/></td><td></td></tr><tr id="TBL-8-81-" style="vertical-align:baseline;"><td class="td11" id="TBL-8-81-1" style="white-space:nowrap; text-align:center;"> </td>
</tr></table></div>
</p></div>
<!--l. 1879--><p class="noindent">The grid layout represents the partial transition model <span class="cmsy-10"><img alt="P" class="10x-x-50" src="cmsy10-50.png"/></span><sub><span class="cmmi-7">ss</span><span class="cmsy-7">′</span></sub><sup><span class="cmmi-7">a</span></sup>, and numbers represent the immediate reward, <span class="cmsy-10"><img alt="R" class="10x-x-52" src="cmsy10-52.png"/></span><sub><span class="cmmi-7">ss</span><span class="cmsy-7">′</span></sub><sup><span class="cmmi-7">a</span></sup> from
each state <span class="cmmi-10">s</span>, which is -1 for all <span class="cmmi-10">a </span>and <span class="cmmi-10">s</span><span class="cmsy-10">′ </span>in this case.
<!--l. 1883--><p class="noindent">
<h4 class="subsectionHead"><span class="titlemark">5.6   </span> <a id="x1-310005.6"></a>Agent categories</h4>
<!--l. 1885--><p class="noindent">An agent is <span class="ecbx-1000">model free </span>when the behavior of the environment is not known. The agent needs a policy or a value
function to operate and there is no model. On the other hand, it is <span class="ecbx-1000">model based </span>when the environment is known by
means of a model. In this case, a policy and a value function might be optional, since it is possible that the agent can
operate just knowing the model.
                                                                                            
                                                                                            
<!--l. 1892--><p class="noindent">Model free agents are simpler, while model based agents are more sample efficient.
<!--l. 1895--><p class="noindent">Another categorization is the following:
     <ul class="itemize1">
<li class="itemize">
<!--l. 1897--><p class="noindent">Value based: there is no policy, it is implicit in the value function.
     </p></li>
<li class="itemize">
<!--l. 1898--><p class="noindent">Policy based: there is no value function, the model operates only by means of the policy.
     </p></li>
<li class="itemize">
<!--l. 1900--><p class="noindent">Actor critic: they have both a policy and a value function.</p></li></ul>
<!--l. 1903--><p class="noindent">
<h4 class="subsectionHead"><span class="titlemark">5.7   </span> <a id="x1-320005.7"></a>Subproblems of RL</h4>
<!--l. 1905--><p class="noindent"><span class="ecbx-1000">Prediction </span>consists in evaluating the future, for a given policy, i.e., what are the values in each state?
<!--l. 1908--><p class="noindent"><span class="ecbx-1000">Control </span>refers to the problem of optimising the future to find the best policy, i.e., which actions to take?
<!--l. 1911--><p class="noindent">These two problems are strongly related, because the best actions to take will be decided using our predictions about
the future:
<div class="math-display">
<img alt="π (s) = argmax v (s).
 *          π  π
" class="math-display" src="summary201x.png"/></div>
<!--l. 1917--><p class="noindent">Two fundamental problems in RL are:
     <ul class="itemize1">
<li class="itemize">
<!--l. 1919--><p class="noindent">Learning: the environment is initially unknown and the agent interacts with it to learn.
     </p></li>
<li class="itemize">
<!--l. 1921--><p class="noindent">Planning/search: a model of the environment is given or learnt, and the agent plans in this model.</p></li></ul>
<!--l. 1924--><p class="noindent">In order to learn, we need to define all components of the problem as functions:
     <ul class="itemize1">
<li class="itemize">
<!--l. 1927--><p class="noindent">Policy: <span class="cmmi-10">π </span><span class="cmr-10">: </span><span class="cmmi-10">S </span><span class="cmsy-10">→ </span><span class="cmmi-10">A </span>(or probabilities over <span class="cmmi-10">A</span>).
                                                                                            
                                                                                            
     </p></li>
<li class="itemize">
<!--l. 1928--><p class="noindent">Value functions: <span class="cmmi-10">v </span><span class="cmr-10">: </span><span class="cmmi-10">S </span><span class="cmsy-10">→ </span><span class="msbm-10">ℝ</span>.
     </p></li>
<li class="itemize">
<!--l. 1929--><p class="noindent">Models: <span class="cmmi-10">p </span><span class="cmr-10">: </span><span class="cmmi-10">S </span><span class="cmsy-10">→ </span><span class="cmmi-10">S </span>or <span class="cmmi-10">r </span><span class="cmr-10">: </span><span class="cmmi-10">S </span><span class="cmsy-10">→ </span><span class="msbm-10">ℝ</span>.
     </p></li>
<li class="itemize">
<!--l. 1930--><p class="noindent">State update: <span class="cmmi-10">u </span><span class="cmr-10">: </span><span class="cmmi-10">S </span><span class="cmsy-10">× </span><span class="cmmi-10">O </span><span class="cmsy-10">→ </span><span class="cmmi-10">S</span>.</p></li></ul>
<!--l. 1932--><p class="noindent">Then, we can use, for example, neural networks and deep learning techniques to learn. But we do need to be careful,
because in RL it is usual to violate assumptios made in supervised learning, such as having i.i.d. samples, or
stationarity.
<!--l. 1937--><p class="noindent">
<h3 class="sectionHead"><span class="titlemark">6   </span> <a id="x1-330006"></a>Markov Decision Processes</h3>
<!--l. 1939--><p class="noindent">We saw the notion of MDP, and now we formalize it:
<div class="tcolorbox tcolorbox" id="tcolobox-15">
<div class="tcolorbox-title">
</div>
<div class="tcolorbox-content"><div class="newtheorem">
<!--l. 1942--><p class="noindent"><span class="head">
<a id="x1-33001r1"></a>
<span class="ecbx-1000">Definition 6.1.</span> </span>A <span class="ecbx-1000">Markov Decision Process (MDP) </span>is a tuple <img align="middle" alt="(S,A,p,γ)" class="left" src="summary202x.png"/> where:
     <ul class="itemize1">
<li class="itemize">
<!--l. 1946--><p class="noindent"><span class="cmmi-10">S </span>is the set of all possible states with the Markov Property.
     </p></li>
<li class="itemize">
<!--l. 1947--><p class="noindent"><span class="cmmi-10">A </span>is the set of all possible actions.
     </p></li>
<li class="itemize">
<!--l. 1948--><p class="noindent"><span class="cmmi-10">p</span><img align="middle" alt="(r,s′|s,a)" class="left" src="summary203x.png"/> is the joint probability of a reward, <span class="cmmi-10">r</span>, and next state, <span class="cmmi-10">s</span><span class="cmsy-10">′</span>, given a state <span class="cmmi-10">s </span>and an action
     <span class="cmmi-10">a</span>.
     </p></li>
<li class="itemize">
<!--l. 1950--><p class="noindent"><span class="cmmi-10">γ </span><span class="cmsy-10">∈</span><img align="middle" alt="[0,1]" class="left" src="summary204x.png"/> is a discount factor that trades off later rewards to earlier ones.</p></li></ul>
</p></div>
<!--l. 1953--><p class="noindent">
</p></div>
</div>
<div class="newtheorem">
<!--l. 1956--><p class="noindent"><span class="head">
<a id="x1-33002r1"></a>
<span class="ecti-1000">Remark </span>6.1<span class="ecti-1000">.</span> </span><span class="cmmi-10">p </span>defines the dynamics of the problem.
<!--l. 1959--><p class="noindent">Sometimes, it is useful to marginalise out the state transitions or expected rewards:
<div class="math-display">
<img alt="          ∑
p (s′|s,a) =   p (s′,r|s,a),
           r
" class="math-display" src="summary205x.png"/></div>
<!--l. 1964--><p class="noindent">to obtain the probability of arriving to a certain state.
<!--l. 1966--><p class="noindent">Also, the expected reward:
<div class="math-display">
<img alt="          ∑   ∑
E [R |s,a] =   r    p(r,s′|s,a).
           r   s′
" class="math-display" src="summary206x.png"/></div>
</p></p></p></p></div>
<!--l. 1970--><p class="noindent">
There is an alternative equivalent definition, which introduces the notion of the expected reward into the concept, and
takes it out of the probabity function:
<div class="tcolorbox tcolorbox" id="tcolobox-16">
<div class="tcolorbox-title">
</div>
<div class="tcolorbox-content"><div class="newtheorem">
<!--l. 1976--><p class="noindent"><span class="head">
<a id="x1-33003r2"></a>
<span class="ecbx-1000">Definition 6.2.</span> </span>A MDP is a tuple <img align="middle" alt="(S,A, p,r,γ)" class="left" src="summary207x.png"/> where:
     <ul class="itemize1">
<li class="itemize">
<!--l. 1979--><p class="noindent"><span class="cmmi-10">S </span>is the set of all possible states with the Markov Property.
     </p></li>
<li class="itemize">
<!--l. 1980--><p class="noindent"><span class="cmmi-10">A </span>is the set of all possible actions.
     </p></li>
<li class="itemize">
<!--l. 1981--><p class="noindent"><span class="cmmi-10">p</span><img align="middle" alt="(s′|s,a)" class="left" src="summary208x.png"/> is the probability of transitioning to <span class="cmmi-10">s</span><span class="cmsy-10">′</span>, fiven a state <span class="cmmi-10">s </span>and an action <span class="cmmi-10">a</span>.
     </p></li>
<li class="itemize">
<!--l. 1983--><p class="noindent"><span class="cmmi-10">r </span><span class="cmr-10">: </span><span class="cmmi-10">S </span><span class="cmsy-10">× </span><span class="cmmi-10">A </span><span class="cmsy-10">→ </span><span class="msbm-10">ℝ </span>is the expected reward, achieved on a transition starting in <img align="middle" alt="(s,a)" class="left" src="summary209x.png"/>,
<div class="math-display">
<img alt="r = E [R|s,a].
" class="math-display" src="summary210x.png"/></div>
</p></li>
<li class="itemize">
<!--l. 1988--><p class="noindent"><span class="cmmi-10">γ </span><span class="cmsy-10">∈</span><img align="middle" alt="[0,1]" class="left" src="summary211x.png"/> is a discount factor that trades off later rewards to earlier ones.</p></li></ul>
</p></div>
<!--l. 1991--><p class="noindent">
</p></div>
</div>
<!--l. 1994--><p class="noindent">Now, we have to clarify what is the Markov Property:
<div class="tcolorbox tcolorbox" id="tcolobox-17">
<div class="tcolorbox-title">
</div>
<div class="tcolorbox-content"><div class="newtheorem">
<!--l. 1997--><p class="noindent"><span class="head">
<a id="x1-33004r3"></a>
<span class="ecbx-1000">Definition 6.3.</span> </span>Consider a sequence of random variables, <img align="middle" alt="{St}" class="left" src="summary212x.png"/><sub><span class="cmmi-7">t</span><span class="cmsy-7">∈</span><span class="msbm-7">ℕ</span></sub>, indexed by time and taken from a
set of states <span class="cmmi-10">S</span>. Consider also the set of actions <span class="cmmi-10">A </span>and rewards in <span class="msbm-10">ℝ</span>.
<!--l. 2002--><p class="noindent">A state <span class="cmmi-10">s </span>has the <span class="ecbx-1000">Markov Property </span>when, for all <span class="cmmi-10">s</span><span class="cmsy-10">′∈ </span><span class="cmmi-10">S</span>,
<div class="math-display">
<img alt="p(St+1 = s′|St = s) = p (St+1 = s′|ht- 1,St = s),
" class="math-display" src="summary213x.png"/></div>
<!--l. 2006--><p class="noindent">for all possible histories <span class="cmmi-10">h</span><sub><span class="cmmi-7">t</span><span class="cmsy-7">-</span><span class="cmr-7">1</span></sub> <span class="cmr-10">= </span><img align="middle" alt="{S1,...,St-1,A1,...,At-1,R1, ...,Rt-1}" class="left" src="summary214x.png"/>.
</p></p></p></div>
<!--l. 2007--><p class="noindent">
</p></div>
</div>
<!--l. 2010--><p class="noindent">Therefore, in an MDP, the current state captures all relevant information from the history, it is a sufficient statistic of
the past. So, once the state is known, the history may be thrown away.
<div class="newtheorem">
<!--l. 2013--><p class="noindent"><span class="head">
<a id="x1-33005r1"></a>
<span class="ecbx-1000">Exercise 6.1.</span> </span>In an MDP, which of the following statemest are true?
     <ol class="enumerate1">
<li class="enumerate" id="x1-33007x1">
<!--l. 2016--><p class="noindent"><span class="cmmi-10">p</span><img align="middle" alt="        ′
(St+1 = s |St = s,At = a)" class="left" src="summary215x.png"/> <span class="cmr-10">=   </span><span class="cmmi-10">p</span><img align="middle" alt="        ′
(St+1 = s |S1,...,St-1,A1,...,At,St = s)" class="left" src="summary216x.png"/>:  false,  the  RHS  does  not
     condition on <span class="cmmi-10">A</span><sub><span class="cmmi-7">t</span></sub> <span class="cmr-10">= </span><span class="cmmi-10">a</span>.
     </p></li>
<li class="enumerate" id="x1-33009x2">
<!--l. 2018--><p class="noindent"><span class="cmmi-10">p</span><img align="middle" alt="(S   = s′|S  = s,A  = a)
  t+1     t      t" class="left" src="summary217x.png"/> <span class="cmr-10">= </span><span class="cmmi-10">p</span><img align="middle" alt="(S    = s′|S ,...,S   ,S = s,A = a)
  t+1      1    t-1  t     t" class="left" src="summary218x.png"/>: true.
     </p></li>
<li class="enumerate" id="x1-33011x3">
<!--l. 2020--><p class="noindent"><span class="cmmi-10">p</span><img align="middle" alt="(St+1 = s′|St = s,At = a)" class="left" src="summary219x.png"/> <span class="cmr-10">=  </span><span class="cmmi-10">p</span><img align="middle" alt="(St+1 = s′|S1,...,St-1,St = s)" class="left" src="summary220x.png"/>:  false,  the  RHS  does  not  condition  on
     <span class="cmmi-10">A</span><sub><span class="cmmi-7">t</span></sub> <span class="cmr-10">= </span><span class="cmmi-10">a</span>.
     </p></li>
<li class="enumerate" id="x1-33013x4">
<!--l. 2022--><p class="noindent"><span class="cmmi-10">p</span><img align="middle" alt="(Rt+1 = r,St+1 = s′|St = s)" class="left" src="summary221x.png"/> <span class="cmr-10">= </span><span class="cmmi-10">p</span><img align="middle" alt="(Rt+1 = r,St+1 = s′|S1,...,St-1,St = s)" class="left" src="summary222x.png"/>: true.</p></li></ol>
</p></div>
<!--l. 2025--><p class="noindent">
It is also worth noting that most MDPs are discounted, and there are several reasons for these:
     <ul class="itemize1">
<li class="itemize">
<!--l. 2029--><p class="noindent">Problem specification: immediate rewards may actually be more valuable. For instance, animal/human
     behavior shows preference for immediate reward.
     </p></li>
<li class="itemize">
<!--l. 2032--><p class="noindent">Solution side: it is mathematically convenient to discount rewards, because it allows for easier proofs of
     convergence, and avoids infinite returns in cyclic Markov processes.</p></li></ul>
<!--l. 2036--><p class="noindent">As we outlined previously, the <span class="ecbx-1000">goal of an RL agent </span>is to find a behavior policy that maximises the expected return
<span class="cmmi-10">G</span><sub><span class="cmmi-7">t</span></sub>. Recall our definition for value funtion
<div class="math-display">
<img alt="vπ (s) = E[Gt|St = s,π].
" class="math-display" src="summary223x.png"/></div>
<!--l. 2042--><p class="noindent">Similarly, we can define the <span class="ecbx-1000">state-action values</span>, as
<div class="math-display">
<img alt="qπ (s,a) = E [Gt |St = s,At = a,π],
" class="math-display" src="summary224x.png"/></div>
<!--l. 2046--><p class="noindent">and there is the following connection between them:
<div class="math-display">
<img alt="       ∑
vπ(s) =   π (a|s)qπ(s,a) = E [qπ(St,At)|St = s,π],∀s ∈ S.
        a
" class="math-display" src="summary225x.png"/></div>
<!--l. 2051--><p class="noindent">Also, we can define the maximum possible value functions:
                                                                                            
                                                                                            
<div class="tcolorbox tcolorbox" id="tcolobox-18">
<div class="tcolorbox-title">
</div>
<div class="tcolorbox-content"><div class="newtheorem">
<!--l. 2054--><p class="noindent"><span class="head">
<a id="x1-33014r4"></a>
<span class="ecbx-1000">Definition 6.4.</span> </span>The <span class="ecbx-1000">optimal state value function</span>, <span class="cmmi-10">v</span><sup><span class="cmsy-7">*</span></sup><img align="middle" alt="(s)" class="left" src="summary226x.png"/>, is the maximum value function over all
policies,
<div class="math-display">
<img alt=" *
v (s) = maπx vπ(s).
" class="math-display" src="summary227x.png"/></div>
<!--l. 2060--><p class="noindent">The <span class="ecbx-1000">optimal state-action value function</span>, <span class="cmmi-10">q</span><sup><span class="cmsy-7">*</span></sup><img align="middle" alt="(s,a)" class="left" src="summary228x.png"/>, is the amximum state-action value function over
all policies,
<div class="math-display">
<img alt=" *
q (s,a) = maπx qπ (s,a).
" class="math-display" src="summary229x.png"/></div>
</p></p></div>
<!--l. 2065--><p class="noindent">
</p></div>
</div>
<!--l. 2068--><p class="noindent">The optimal value function specifies the best possible performance in the MDP. We can consider the MDP to be solved
when we know the optimal value function.
<!--l. 2072--><p class="noindent">In addition, value functions allow us to define a partial ordering over policies, having
<div class="math-display">
<img alt="π ≥ π′ ⇐ ⇒ vπ(s) ≥ vπ′ (s),∀s ∈ S.
" class="math-display" src="summary230x.png"/></div>
<!--l. 2078--><p class="noindent">With this partial ordering, the following theorem state that optimal policies exist for every MDP:
<div class="tcolorbox tcolorbox" id="tcolobox-19">
<div class="tcolorbox-title">
</div>
<div class="tcolorbox-content"><div class="newtheorem">
<!--l. 2082--><p class="noindent"><span class="head">
<a id="x1-33015r1"></a>
<span class="ecbx-1000">Theorem 6.1.</span> </span><span class="ecbi-1000">Optimal Policies Theorem</span>
<!--l. 2085--><p class="noindent"><span class="ecti-1000">For any MDP:</span>
<ul class="itemize1">
<li class="itemize">
<!--l. 2087--><p class="noindent"><span class="ecti-1000">There exists an optimal policy </span><span class="cmmi-10">π</span><sup><span class="cmsy-7">*</span></sup> <span class="ecti-1000">that is better than or equal to all other policies,</span>
<div class="math-display">
<img alt="π* ≥ π,∀π.
" class="math-display" src="summary231x.png"/></div>
</p></li>
<li class="itemize">
<!--l. 2092--><p class="noindent"><span class="ecti-1000">All optimal policies achieve the optimal value function,</span>
<div class="math-display">
<img alt=" π*      *
v  (s) = v (s).
" class="math-display" src="summary232x.png"/></div>
</p></li>
<li class="itemize">
<!--l. 2096--><p class="noindent"><span class="ecti-1000">All optimal policies achieve the optimal state-action value function,</span>
<div class="math-display">
<img alt=" π*        *
q  (s,a) = q (s,a).
" class="math-display" src="summary233x.png"/></div>
</p></li></ul>
</p></p></div>
<!--l. 2101--><p class="noindent">
</p></div>
</div>
<!--l. 2104--><p class="noindent">To find an optimal policy, we can maximise over <span class="cmmi-10">q</span><sup><span class="cmsy-7">*</span></sup><img align="middle" alt="(s,a)" class="left" src="summary234x.png"/>:
<div class="math-display">
<img alt="          {##
π *(s,a) = #1# if a = argmaxa∈A q*(s,a) .
           0  otherwise
" class="math-display" src="summary235x.png"/></div>
<!--l. 2112--><p class="noindent">That is, the optimal policy is to take action <span class="cmmi-10">a </span>in state <span class="cmmi-10">s </span>if <span class="cmmi-10">a </span>is the action that gives the highes state-action value given
state <span class="cmmi-10">s</span>.
<div class="newtheorem">
<!--l. 2115--><p class="noindent"><span class="head">
<a id="x1-33016r2"></a>
<span class="ecti-1000">Remark </span>6.2<span class="ecti-1000">.</span> </span>There is always a deterministic optimal policy for any MDP, and we know <span class="cmmi-10">q</span><sup><span class="cmsy-7">*</span></sup><img align="middle" alt="(s,a)" class="left" src="summary236x.png"/>, we know the
optimal policy immediately.
<!--l. 2119--><p class="noindent">Also, there can be multiple optimal policies, and if multiple actions maximize <span class="cmmi-10">q</span><sub><span class="cmsy-7">*</span></sub><img align="middle" alt="(s,⋅)" class="left" src="summary237x.png"/>, we can pick any of them.
</p></p></div>
<!--l. 2121--><p class="noindent">
Now, recall the Bellman Equations we saw previously. The following theorem explains how to express the value
functions by means of these equations:
<div class="tcolorbox tcolorbox" id="tcolobox-20">
<div class="tcolorbox-title">
</div>
<div class="tcolorbox-content"><div class="newtheorem">
<!--l. 2127--><p class="noindent"><span class="head">
<a id="x1-33017r2"></a>
<span class="ecbx-1000">Theorem 6.2.</span> </span><span class="ecbi-1000">Bellman Expectation Equations</span>
<!--l. 2130--><p class="noindent"><span class="ecti-1000">Given an MDP, </span><span class="cmmi-10">M </span><span class="cmr-10">= </span><img align="middle" alt="(S,A, p,r,γ)" class="left" src="summary238x.png"/><span class="ecti-1000">, for any policy </span><span class="cmmi-10">π</span><span class="ecti-1000">, the value functions obey the followin expectation</span>
<span class="ecti-1000">equations:</span>
<div class="math-display">
<img alt="                [                         ]
       ∑                  ∑     ′        ′
vπ(s) =   π(s,a) r(s,a)+ γ  ′ p (s |a,s)vπ(s ) ,
        a                  s
" class="math-display" src="summary239x.png"/></div>
<!--l. 2135--><p class="noindent"><span class="ecti-1000">and</span>
<div class="math-display">
<img alt="qπ(s,a) = r(s,a)+ γ∑ p(s′|a,s) ∑  π(a′|s′) qπ (s′,a′).
                   s′        a′∈A
" class="math-display" src="summary240x.png"/></div>
</p></p></p></div>
<!--l. 2139--><p class="noindent">
</p></div>
</div>
<!--l. 2142--><p class="noindent">
<h3 class="likesectionHead"><a id="x1-340006"></a>References</h3>
<!--l. 1--><p class="noindent">
<div class="thebibliography">
<p class="bibitem"><span class="biblabel">
 [1]<span class="bibsp">   </span></span><a id="XDupuis2023"></a>Tom Dupuis. Machine learning. Lecture Notes.
</p>
</div>
</p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></body></html>
