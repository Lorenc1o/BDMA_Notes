<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="LyX 2.3.6" name="GENERATOR"/>
<meta content="text/html;charset=utf-8" http-equiv="Content-type"/>
<title>BDMA - Machine Learning</title>
<style type="text/css">
/* LyX Provided Styles */
div.bibtexentry { margin-left: 2em; text-indent: -2em; }
span.bibtexlabel:before{ content: "["; }
span.bibtexlabel:after{ content: "] "; }

/* Layout-provided Styles */
h1.title {
font-family: sans-serif;
font-weight: bold;
font-size: x-large;
margin-bottom: 1ex;
text-align: center;

}
div.date {
font-size: x-large;
margin-top: 0.9ex;
margin-bottom: 0.5ex;
text-align: center;

}
div.author {
font-size: x-large;
margin-top: 1.3ex;
margin-bottom: 0.7ex;
text-align: center;

}
div.standard {
	text-indent: 2em;
	margin-bottom: 2ex;
}
div.address {
margin-bottom: 1.5ex;
text-align: left;

}
h1.part {
font-family: sans-serif;
font-weight: bold;
font-size: x-large;
margin-top: 2ex;
margin-bottom: 1.5ex;
text-align: left;

}
h2.section {
font-family: sans-serif;
font-weight: bold;
font-size: x-large;
margin-top: 1.3ex;
margin-bottom: 0.7ex;
text-align: left;

}
ul.itemize {
margin-top: 0.7ex;
margin-bottom: 0.7ex;
margin-left: 3ex;
text-align: left;

}
h3.subsection {
font-family: sans-serif;
font-weight: bold;
font-size: large;
margin-top: 0.9ex;
margin-bottom: 0.5ex;
text-align: left;

}
div.definition {
font-style: normal;
font-variant: normal;
font-size: medium;
margin-top: 0.7ex;
margin-bottom: 0.7ex;
text-align: left;

}
span.definition_label {
font-weight: bold;
font-style: normal;
font-variant: normal;
font-size: medium;

}
div.example {
font-style: normal;
font-variant: normal;
font-size: medium;
margin-top: 0.7ex;
margin-bottom: 0.7ex;
text-align: left;

}
span.example_label {
font-weight: bold;
font-style: normal;
font-variant: normal;
font-size: medium;

}
div.plain_layout {
text-align: left;

}
h4.subsubsection {
font-family: sans-serif;
font-weight: bold;
font-size: medium;
margin-top: 0.7ex;
margin-bottom: 0.4ex;
text-align: left;

}
h5.paragraph {
font-family: sans-serif;
font-weight: bold;
font-size: medium;
margin-top: 0.4ex;
text-align: left;

}
div.theorem {
font-style: italic;
font-variant: normal;
font-size: medium;
margin-top: 0.7ex;
margin-bottom: 0.7ex;
text-align: left;

}
span.theorem_label {
font-weight: bold;
font-style: normal;
font-variant: normal;
font-size: medium;

}
ol.enumerate {
margin-top: 0.7ex;
margin-bottom: 0.7ex;
margin-left: 3ex;
text-align: left;

}
div.remark {
font-style: normal;
font-variant: normal;
font-size: medium;
margin-top: 0.7ex;
margin-bottom: 0.7ex;
text-align: left;

}
span.remark_label {
font-weight: normal;
font-style: italic;
font-variant: normal;
font-size: medium;

}
div.conjecture {
font-style: italic;
font-variant: normal;
font-size: medium;
margin-top: 0.7ex;
margin-bottom: 0.7ex;
text-align: left;

}
span.conjecture_label {
font-weight: bold;
font-style: normal;
font-variant: normal;
font-size: medium;

}
div.exercise {
font-style: normal;
font-variant: normal;
font-size: medium;
margin-top: 0.7ex;
margin-bottom: 0.7ex;
text-align: left;

}
span.exercise_label {
font-weight: bold;
font-style: normal;
font-variant: normal;
font-size: medium;

}
div.claim {
font-style: normal;
font-variant: normal;
font-size: medium;
margin-top: 0.7ex;
margin-bottom: 0.7ex;
text-align: left;

}
span.claim_label {
font-weight: normal;
font-style: italic;
font-variant: normal;
font-size: medium;

}
ol.lyxlist {
	list-style-type: none;
}
li.labeling_item {
	text-indent: -5em;
	margin-left: 5em;
}
span.lyxlist {
	margin-right: 1em;
}
div.toc {
  margin: 2em 0em;
  border-style: solid;
  border-width: 2px 0px;
  padding: 1em 0em;
}
h2.tochead { font-size: x-large; font-weight: bold; }
div.lyxtoc-0 {
  margin: 2em 0em 0em 0em;
  font-size: xx-large;
  font-weight: bold;
}
div.lyxtoc-1 {
  margin: 1em 0em 0em 0em;
  font-size: x-large;
  font-weight: bold;
}
div.lyxtoc-2 {
  margin: 0em 0.1em 0em 1em;
  font-size: large;
  font-weight: normal;
}
div.lyxtoc-3 { margin: 0em 0.1em 0em 0.5em; font-size: medium; }
div.lyxtoc-4 { margin: 0em 0.1em 0em 0.5em; }
div.lyxtoc-5 { margin: 0em 0.1em 0em 0.5em; }
div.lyxtoc-6 { margin: 0em 0.1em 0em 0.5em; }
a.tocentry {
  text-decoration: none;
  color: black;
}
a.tocentry:visited { color: black; }
table {
	border-collapse: collapse;
	display: inline-block;
}
td {
	border: 1px solid black;
	padding: 0.5ex;
}
div.float {
	border: 2px solid black;
	text-align: center;
}
div.float-caption {
	text-align: center;
	border: 2px solid black;
	padding: 1ex;
	margin: 1ex;
}
span.flex_url {
font-family: monospace;
}
div.float-listings {
	border: 2px solid black;
	padding: 1ex;
	margin: 1ex;
}
div.listings-caption {
	text-align: center;
	border: 2px solid black;
	padding: 1ex;
	margin: 1ex;
	}


</style>
</head>
<body dir="auto">
<h1 class="title" id="magicparlabel-1">BDMA - Machine Learning</h1>
<div class="date" id="magicparlabel-2">Fall 2023</div>
<div class="author" id="magicparlabel-3">Jose Antonio Lorencio Abril</div>
<div class="standard" id="magicparlabel-4" style="text-align: center;"><img alt="image: 0_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___cision-Modeling_LectureNotes_source_CS-logo.png" src="0_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___cision-Modeling_LectureNotes_source_CS-logo.png" style="width:70%;"/>
</div>
<div class="standard" id="magicparlabel-5" style="text-align: right;">Professor: Tom Dupuis</div>
<div class="standard" id="magicparlabel-6" style="text-align: right;">Student e-mail: jose-antonio.lorencio-abril@student-cs.fr</div>
<div class="standard" id="magicparlabel-7"><br/>
</div>
<div class="address" id="magicparlabel-8">This is a summary of the course <em>Machine Learning</em> taught at the Université Paris Saclay - CentraleSupélec by Professor Tom Dupuis in the academic year 23/24. Most of the content of this document is adapted from the course notes by Dupuis, [<a href="#LyXCite-Dupuis2023"><span class="bib-label">1</span></a>], so I won't be citing it all the time. Other references will be provided when used.</div>
<div class="toc"><h2 class="tochead section">Table of Contents</h2></div>
<div class="lyxtoc-0"><a class="tocentry" href="#magicparlabel-11">Part I Deep Learning</a>
<div class="lyxtoc-1"><div class="lyxtoc-2"><a class="tocentry" href="#magicparlabel-12">1 Introduction</a>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-27">1.1 AI History</a>
</div>
</div>
<div class="lyxtoc-2"><a class="tocentry" href="#magicparlabel-52">2 Machine Learning Basics</a>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-54">2.1 Linear Algebra Basics</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-73">2.2 Probability Basics</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-316">2.3 Machine Learning Basics</a>
</div>
</div>
<div class="lyxtoc-2"><a class="tocentry" href="#magicparlabel-349">3 Deep Neural Networks</a>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-350">3.1 Perceptron</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-358">3.2 Multi-layer perceptron</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-624">3.3 Cost Functions</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-648">3.4 Why deep NN?</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-678">3.5 Gradient-based Learning</a>
</div>
</div>
<div class="lyxtoc-2"><a class="tocentry" href="#magicparlabel-714">4 Deep Neural Networks: Optimization and Regularization</a>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-715">4.1 Optimization</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-783">4.2 Initialization and Normalization</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-805">4.3 Regularization</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-834">4.4 Vanishing Gradients</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-853">4.5 Double Descent</a>
</div>
</div>
<div class="lyxtoc-2"><a class="tocentry" href="#magicparlabel-866">5 Convolutional Neural Networks</a>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-867">5.1 Introduction</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-884">5.2 Convolutions</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-939">5.3 Padding, Stride, Pooling</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-966">5.4 CNNs</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-1202">5.5 Data and Transfer</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-1218">5.6 Self-Supervised Learning (SSL)</a>
</div>
</div>
<div class="lyxtoc-2"><a class="tocentry" href="#magicparlabel-1244">6 Recurrent Neural Networks</a>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-1245">6.1 Introduction</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-1250">6.2 Recurrent Neural Networks</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-1295">6.3 Recurrent Neural Networks Training</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-1301">6.4 Long-Short Term Memory (LSTM) and Gated Recurrent Units (GRU)</a>
</div>
</div>
<div class="lyxtoc-2"><a class="tocentry" href="#magicparlabel-1323">7 Deep Generative Modelling</a>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-1331">7.1 Variational Auto-Encoders (VAE)</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-1374">7.2 Generative Adversarial Networks</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-1479">7.3 Other Approaches</a>
</div>
</div>
<div class="lyxtoc-2"><a class="tocentry" href="#magicparlabel-1485">8 Denoising Diffusion Models</a>
</div>
</div>
</div>
<div class="lyxtoc-0"><a class="tocentry" href="#magicparlabel-1503">Part II Reinforcement Learning</a>
<div class="lyxtoc-1"><div class="lyxtoc-2"><a class="tocentry" href="#magicparlabel-1504">9 Introduction</a>
</div>
<div class="lyxtoc-2"><a class="tocentry" href="#magicparlabel-1530">10 Definition and components</a>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-1550">10.1 Maximising the value by taking actions</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-1555">10.2 Markov Decision Processes</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-1577">10.3 Policies</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-1579">10.4 Value Functions</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-1585">10.5 Model</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-3208">10.6 Agent categories</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-3215">10.7 Subproblems of RL</a>
</div>
</div>
<div class="lyxtoc-2"><a class="tocentry" href="#magicparlabel-3228">11 Markov Decision Processes</a>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-3310">11.1 Solving Reinforcement Learning Problems with Bellman Equations</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-3337">11.2 Dynamic Programming</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-4655">11.3 Extensions to Dynamic Programming</a>
</div>
</div>
<div class="lyxtoc-2"><a class="tocentry" href="#magicparlabel-4677">12 Model-Free Prediction</a>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-4678">12.1 Monte Carlo Algorithms</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-4759">12.2 Temporal Difference Learning</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-4835">12.3 Comparing MC and TD</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-4845">12.4 Batch Monte Carlo and Temporal Difference</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-4859">12.5 Multi-Step Temporal Difference</a>
</div>
</div>
<div class="lyxtoc-2"><a class="tocentry" href="#magicparlabel-4864">13 Model-Free Control</a>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-4866">13.1 Monte-Carlo Control</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-4893">13.2 Temporal-Difference Learning for Control</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-4923">13.3 Off-Policy Temporal-Difference and Q-Learning</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-4948">13.4 Overestimation in Q-Learning</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-4960">13.5 Importance of Sampling Corrections</a>
</div>
</div>
<div class="lyxtoc-2"><a class="tocentry" href="#magicparlabel-4965">14 Value approximation &amp; Deep Reinforcement Learning</a>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-4966">14.1 Approximate Model-Free Prediction</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-4976">14.2 Deep Reinforcement Learning</a>
</div>
<div class="lyxtoc-3"><a class="tocentry" href="#magicparlabel-5025">14.3 Deep Learning aware Reinforcement Learning</a>
</div>
</div>
</div>
</div>
<div class="standard" id="magicparlabel-10"><br/>
</div>
<h1 class="part" id="magicparlabel-11"><span class="part_label">Part I</span> Deep Learning</h1>
<h2 class="section" id="magicparlabel-12"><span class="section_label">1</span> Introduction</h2>
<div class="standard" id="magicparlabel-13"><b>Artificial Intelligence</b> is a wide concept, encompassing different aspects and fields. We can understand the term AI as the multidisciplinary field of study that aims at recreating human intelligence using artificial means. This is a bit abstract, and, in fact, there is no single definition for what this means. Intelligence is not fully understood, and thus it is hard to assess whether an artificial invention has achieved intelligence, further than intuitively thinking so.</div>
<div class="standard" id="magicparlabel-14">For instance, AI involves a whole variety of fields:</div>
<ul class="itemize" id="magicparlabel-15"><li class="itemize_item">Perception</li>
<li class="itemize_item">Knowledge</li>
<li class="itemize_item">Cognitive System</li>
<li class="itemize_item">Planning</li>
<li class="itemize_item">Robotics</li>
<li class="itemize_item">Machine Learning (Neural Networks)</li>
<li class="itemize_item">Natural Language Processing</li>
</ul>
<div class="standard" id="magicparlabel-22">Leveraging all of these, people try to recreate or even surpass human performance in different tasks. For example, a computer program that can play chess better than any human could ever possibly play, such as Stockfish, or a system that is able to understand our messages and reply, based on the knowledge that it has learnt in the past, such as ChatGPT and similar tools. Other examples are self-driving cars, auto-controlled robots, etc.</div>
<div class="standard" id="magicparlabel-23">Therefore, AI is a very wide term, which merges many different scientific fields. <b>Machine Learning</b>, on the other side, is a narrower term, which deals with the study of the techniques that we can use to make a computer learn to perform some task. It takes concepts from Statistics, Optimization Theory, Computer Science, Algorithms, etc. A relevant subclass of Machine Learning, which has come to be one of the most prominent fields of research in the recent years, is <b>Neural Networks </b>or <b>Deep Learning</b>, which consists on an ML technique based on the human brain. Many amazing use cases that we see everywhere, like Siri (Apple assistant), Cortana (Windows assistant), Amazon recommender system, Dall-E (OpenAI image generation system), etc. Not only this, but the trend is growing, and the interest in DL is continuously increasing.</div>
<div class="standard" id="magicparlabel-24">This is partly also due to the increase in computing resources, and the continuous optimization that different techniques are constantly experiencing. For instance, for a model trained on one trillion data points, in 2021 the training process required around 16500x less compute than a model trained in 2012.</div>
<div class="standard" id="magicparlabel-25">But not everything is sweet and roses when using DL. Since these systems are being involved in decision making processes, there are some questions that arise, like whose responsibility is it when a model fails? Moreover, data is needed to train the models, so it is relevant to address how datasets should be collected, and to respect the privacy of the people that produce data. In addition, the recent technologies that are able to generate new content and to modify real content, make it a new issue that AI can create false information, mistrust, and even violence or paranoia.</div>
<div class="standard" id="magicparlabel-26">Nonetheless, let's not focus on the negative, there are lots of nice applications of DL, and it is a key component to deal with data, achieving higher performance than traditional ML techniques for huge amount of data.</div>
<h3 class="subsection" id="magicparlabel-27"><span class="subsection_label">1.1</span> AI History</h3>
<div class="standard" id="magicparlabel-28">In 1950, Alan turing aimed to answer the question '<em>Can machines think?</em>' through a test, which came to be named the <b>Turing Test</b>, and consists in a 3 players game. First, a similar game is the following: 2 talkers, a man and a female, and 1 interrogator. The interrogator asks questions to the talkers, with the aim of determining who is the man and who is the female. The man tries to trick the interrogator, while the woman tries to help him to identify her.</div>
<div class="standard" id="magicparlabel-29">Then, the Turing Test consists in replacing the man by an artificial machine. Turing thought that a machine that could trick a human interrogator, should be considered intelligent.</div>
<div class="standard" id="magicparlabel-30">Later, in 1956, in the Dartmouth Workshop organized by IBM, the term <b>Artificial Intelligence</b> was first used to describe <em>every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it</em>.</div>
<div class="standard" id="magicparlabel-31">From this year on, there was a focus on researching about <b>Symbolic AI</b>, specially in three areas of research:</div>
<ul class="itemize" id="magicparlabel-32"><li class="itemize_item">Reasoning as search: a different set of actions leads to a certain goal, so we can try to find the best choice of action to obtain the best possible outcome.</li>
<li class="itemize_item">Natural Language: different tools were developed, following grammar and language rules.</li>
<li class="itemize_item">Micro world: small block based worlds, that the system can identify and move.</li>
</ul>
<div class="standard" id="magicparlabel-35">In 1958, the <b>Perceptron</b> was conceived, giving birth to what is called the connectionism, an approach to AI based on the human brain, and a big hype that encouraged funding to support AI research. At this era, scientists experience a bit of lack of perspective, thinking that the power of AI was much higher than it was. For instance, H. A. Simon stated in 1965 that '<em>machines will be capable, within twenty years, of doing any work a man can do.</em>' We can relate to our time, with the huge hype that AI is experiencing, as well as the many apocaliptic theories that some people are making. Maybe we are again overestimating the power of AI.</div>
<div class="standard" id="magicparlabel-36">The time from 1974 to 1980 is seen as the first winter of AI, in which research was slowed down and funding was reduced. This was due to several problems found at the time:</div>
<ul class="itemize" id="magicparlabel-37"><li class="itemize_item">There were few computational resources.</li>
<li class="itemize_item">The models at the time were not scalable.</li>
<li class="itemize_item">The Moravec's paradox: it is comparatively easy to make computers exhibit adult level performance on intelligence test or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility.</li>
<li class="itemize_item">Marvin Minsky made some devastating critics to connectionism, compared to symbolic, rule-based models:

<ul class="itemize" id="magicparlabel-41"><li class="itemize_item">Limited capacity: Minsky showed that single-layer perceptrons (a simple kind of neural network) could not solve certain classes of problems, like the XOR problem. While it was later shown that multi-layer perceptrons could solve these problems, Minsky's work resulted in a shift away from neural networks for a time.</li>
<li class="itemize_item">Lack of clear symbols: Minsky believed that human cognition operates at a higher level with symbols and structures (like frames and scripts), rather than just distributed patterns of activation. He often argued that connectionist models lacked a clear way to represent these symbolic structures.</li>
<li class="itemize_item">Generalization and Abstraction: Minsky was concerned that connectionist models struggled with generalizing beyond specific training examples or abstracting high-level concepts from raw data.</li>
<li class="itemize_item">Inefficiency: Minsky pointed out that many problems which seemed simple for symbolic models could be extremely computationally intensive for connectionist models.</li>
<li class="itemize_item">Lack of explanation: Connectionist models, especially when they become complex, can be seen as "black boxes", making it difficult to interpret how they arrive at specific conclusions.</li>
<li class="itemize_item">Over-reliance on learning: Minsky believed that not all knowledge comes from learning from scratch, and some of it might be innate or structured in advance. He felt connectionism put too much emphasis on learning from raw data.</li>
</ul>
</li></ul>
<div class="standard" id="magicparlabel-47">In 1980, there was a boom in expert knowledge systems that made AI recover interest. An <b>expert system</b> solves specific tasks following an ensemble of rules based on knowledge facilitated by experts. A remarkable use case was the XCON sorting system, developed for the Digital Equipment Corporation, which helped them save 40M$ per year. In addition, connectionism also came again on scene, thanks to the development of <b>backpropagation</b> applied to neurons, by Geoffrey Hinton. All these achievement made funding to come back to the field.</div>
<div class="standard" id="magicparlabel-48">Nonetheless, there came a second winter of AI, from 1987 to 1994, mainly because several companies were disappointed and AI was seen as a technology that couldn't solve wide varieties of tasks. The funding was withdrawn from the field and a lot AI companies went bankrupt.</div>
<div class="standard" id="magicparlabel-49">Luckily, from 1995 there started a new return of AI in the industry. The Moore's Law states that speed and memory of computer doubles every two years, and so computing power and memory was rapidly increasing, making the use of AI systems more feasible each year. During this time, many new concepts were introduced, such as <b>intelligent agents</b> as systems that perceive their environment and take actions which maximize their chances of success; or different <b>probabilistic reasoning tools</b> such as Bayesian networks, hidden Markov models, information theory, SVM,... In addition, AI researchers started to reframe their work in terms of mathematics, computer science, physics, etc., making the field more attractive for funding. A remarkable milestone during this time was the victory of Deep Blue against Garry Kasparov.</div>
<div class="standard" id="magicparlabel-50">The last era of AI comes from 2011 to today, with the advent and popularization of <b>Deep Learning</b> (DL), which are deep graph processing layers mimicking human neurons interactions. This happened thanks to the advances of hardware technologies, that have enabled the enormous computing requirements needed for DL. The huge hype comes from the spectacular results shown by this kind of systems in a huge variety of tasks, such as computer vision, natural language processing, anomaly detection,...</div>
<div class="standard" id="magicparlabel-51">In summary, we can see how the history of AI has been a succession of hype and dissapointment cycles, with many actors involved and the industry as a very important part of the process.</div>
<h2 class="section" id="magicparlabel-52"><span class="section_label">2</span> Machine Learning Basics</h2>
<div class="standard" id="magicparlabel-53">In this section, we review some notation, and basic knowledge of Linear Algebra, Probability and Machine Learning.</div>
<h3 class="subsection" id="magicparlabel-54"><span class="subsection_label">2.1</span> Linear Algebra Basics</h3>
<div class="standard" id="magicparlabel-55">A <b>scalar</b> is a number, either real and usually denoted <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>x</mi><mo> ∈ </mo>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
</mrow></math>, or natural and denoted <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>n</mi><mo> ∈ </mo>
<mstyle mathvariant="double-struck"><mi>N</mi>
</mstyle>
</mrow>
</mrow></math>. A <b>vector</b> is an array of numbers, usually real, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>x</mi><mo> ∈ </mo>
<msup>
<mrow>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msup><mo>,</mo>
</mrow>
</mrow></math> or<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>x</mi><mo>=</mo><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">[</mo>
<mtable>
<mtr>
<mtd>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mtd>
</mtr>
<mtr>
<mtd>
<mi>⋮
      </mi>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msub>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">]</mo><mn>.</mn>
</mrow>
</mrow></math> A <b>matrix</b> is a 2-dimensional array of numbers, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>A</mi><mo> ∈ </mo>
<msup>
<mrow>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>n</mi><mo> × </mo><mi>m</mi>
</mrow>
</mrow>
</msup>
</mrow>
</mrow></math>, or<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>A</mi><mo>=</mo><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">[</mo>
<mtable>
<mtr>
<mtd>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mn>11</mn>
</mrow>
</msub>
</mtd>
<mtd>
<mi>…
      </mi>
</mtd>
<mtd>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow>
<mrow><mn>1</mn><mi>n</mi>
</mrow>
</mrow>
</msub>
</mtd>
</mtr>
<mtr>
<mtd>
<mi>⋮
      </mi>
</mtd>
<mtd>
<mi>⋱
      </mi>
</mtd>
<mtd>
<mi>⋮
      </mi>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow>
<mrow><mi>m</mi><mn>1</mn>
</mrow>
</mrow>
</msub>
</mtd>
<mtd>
<mi>…
      </mi>
</mtd>
<mtd>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow>
<mrow><mi>m</mi><mi>n</mi>
</mrow>
</mrow>
</msub>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">]</mo><mn>.</mn>
</mrow>
</mrow></math> A <b>tensor</b> is an <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>n</mi>
</mrow></math>-dimensional array of numbers, for example <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>A</mi><mo> ∈ </mo>
<msup>
<mrow>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>m</mi><mo> × </mo><mi>k</mi><mo> × </mo><mi>p</mi>
</mrow>
</mrow>
</msup>
</mrow>
</mrow></math> is a 3-dimensional tensor.</div>
<div class="standard" id="magicparlabel-56">Usually, we will be working with matrices, which can be operated in different ways:</div>
<ul class="itemize" id="magicparlabel-57"><li class="itemize_item">Transposition: <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msup>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msup>
</mrow></math> is the transposed of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>A</mi>
</mrow></math>, defined as <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msup>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msup>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mrow>
<mrow><mi>i</mi><mi>j</mi>
</mrow>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow>
<mrow><mi>j</mi><mo>,</mo><mi>i</mi>
</mrow>
</mrow>
</msub>
</mrow>
</mrow></math>.</li>
<li class="itemize_item">Multiplication: Let <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>A</mi><mo> ∈ </mo>
<msup>
<mrow>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>m</mi><mo> × </mo><mi>k</mi>
</mrow>
</mrow>
</msup><mo>,</mo><mi>B</mi><mo> ∈ </mo>
<msup>
<mrow>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>k</mi><mo> × </mo><mi>n</mi>
</mrow>
</mrow>
</msup>
</mrow>
</mrow></math>, their multiplication, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>C</mi><mo> ∈ </mo>
<msup>
<mrow>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>m</mi><mo> × </mo><mi>n</mi>
</mrow>
</mrow>
</msup>
</mrow>
</mrow></math> is defined as <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>C</mi><mo>=</mo><mi>A</mi><mo> ⋅ </mo><mi>B</mi><mo>=</mo><mi>A</mi><mi>B</mi><mo>=</mo>
<msub>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>C</mi>
</mrow>
<mrow>
<mrow><mi>i</mi><mi>j</mi>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mrow>
<mrow><mi>i</mi><mo> ≤ </mo><mi>m</mi><mo>,</mo><mi>j</mi><mo> ≤ </mo><mi>n</mi>
</mrow>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow><mi>k</mi>
</mrow>
</msub>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow>
<mrow><mi>i</mi><mi>k</mi>
</mrow>
</mrow>
</msub>
<msub>
<mrow><mi>B</mi>
</mrow>
<mrow>
<mrow><mi>k</mi><mi>j</mi>
</mrow>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mrow>
<mrow><mi>i</mi><mo> ≤ </mo><mi>m</mi><mo>,</mo><mi>j</mi><mo> ≤ </mo><mi>n</mi>
</mrow>
</mrow>
</msub><mn>.</mn>
</mrow>
</mrow></math> Note that the following holds for every matrix <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>A</mi><mo>,</mo><mi>B</mi>
</mrow>
</mrow></math>:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>A</mi><mi>B</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msup><mo>=</mo>
<msup>
<mrow><mi>B</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msup>
<msup>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msup><mn>.</mn>
</mrow>
</mrow></math></li>
<li class="itemize_item">Point-wise operations: if we have two matrices of the same size, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>A</mi><mo>,</mo><mi>B</mi><mo> ∈ </mo>
<msup>
<mrow>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>m</mi><mo> × </mo><mi>n</mi>
</mrow>
</mrow>
</msup>
</mrow>
</mrow></math>, we can use apply scalar operator point-wise to each pair of elements in the same position in the two matrices. For example, the sum or the substraction of matrices.</li>
</ul>
<div class="standard" id="magicparlabel-60">There are also special matrices:</div>
<ul class="itemize" id="magicparlabel-61"><li class="itemize_item">Identity matrix: the identity matrix is a square matrix that preserves any vector it is multiplied with. For vectors of size <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>n</mi>
</mrow></math>, the identity matrix <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>I</mi>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msub>
</mrow></math> verifies<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>I</mi>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msub><mi>x</mi><mo>=</mo><mi>x</mi><mo>,</mo><mi> ∀ </mi><mi>x</mi><mo> ∈ </mo>
<msup>
<mrow>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msup><mn>.</mn>
</mrow>
</mrow></math></li>
<li class="itemize_item">Inverse matrix: the inverse of a square matrix, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>A</mi><mo> ∈ </mo>
<msup>
<mrow>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>n</mi><mo> × </mo><mi>n</mi>
</mrow>
</mrow>
</msup>
</mrow>
</mrow></math>, when it exists, is defined as the only matrix <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msup>
<mrow><mi>A</mi>
</mrow>
<mrow>
<mrow><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msup>
</mrow></math> such that<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>A</mi>
</mrow>
<mrow>
<mrow><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msup><mi>A</mi><mo>=</mo><mi>A</mi>
<msup>
<mrow><mi>A</mi>
</mrow>
<mrow>
<mrow><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msup><mo>=</mo>
<msub>
<mrow><mi>I</mi>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msub><mn>.</mn>
</mrow>
</mrow></math></li>
</ul>
<div class="standard" id="magicparlabel-63">Another important concept is that of the norm, which is basically measuring how far a point is from the origin of the space and can be used to measure distances:</div>
<div class="standard" id="magicparlabel-64"><div class="flex_color_box"><div class="definition" id="magicparlabel-68"><div class="definition_item"><span class="definition_label">Definition 2.1.</span>
A <b>norm</b> is a function <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>f</mi>
</mrow></math> that measures the size of vectors, and must have the following properties:</div>
<ul class="itemize" id="magicparlabel-69"><li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mn>0</mn><mo> ⟺ </mo><mi>x</mi><mo>=</mo><mn>0</mn><mo>,</mo>
</mrow>
</mrow></math></li>
<li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>+</mo><mi>y</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ≤ </mo><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>y</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> and</li>
<li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> ∀ </mi><mi> α </mi><mo> ∈ </mo>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle><mo>,</mo><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi> α </mi><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">|</mo>
<mi> α </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">|</mo>
<mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></li>
</ul>
</div>
</div></div>
<div class="standard" id="magicparlabel-72">A very important family of norms is the <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msup>
<mrow><mi>L</mi>
</mrow>
<mrow><mi>p</mi>
</mrow>
</msup>
</mrow></math> norm, defined as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">∥</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">∥</mo>
</mrow>
<mrow><mi>p</mi>
</mrow>
</msub><mo>=</mo>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">|</mo>
</mrow>
<mrow><mi>p</mi>
</mrow>
</msup>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mrow>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mi>p</mi>
</mrow>
</mfrac>
</mrow>
</msup><mn>.</mn>
</mrow>
</mrow></math> The <b>Euclidean norm</b> is the <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msup>
<mrow><mi>L</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msup>
</mrow></math> norm, noted <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">∥</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">∥</mo>
</mrow></math> and equivalent to computing <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msqrt>
<mrow>
<msup>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msup><mi>x</mi>
</mrow>
</msqrt>
</mrow></math>. In Machine Learning, it is not uncommon to find the use of the squared Euclidean norm, since it maintains the ordinals and is easier to operate with. The <b>Manhattan norm</b> is the <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msup>
<mrow><mi>L</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msup>
</mrow></math> norm, and it is used when the difference between zero and nonzero elements is important. Finally, the <b>Max norm</b> is the <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msup>
<mrow><mi>L</mi>
</mrow>
<mrow><mi> ∞ </mi>
</mrow>
</msup>
</mrow></math>, or <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">∥</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">∥</mo>
</mrow>
<mrow><mi> ∞ </mi>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">|</mo>
</mrow>
</mrow></math>.</div>
<h3 class="subsection" id="magicparlabel-73"><span class="subsection_label">2.2</span> Probability Basics</h3>
<div class="standard" id="magicparlabel-74">A <b>random variable</b>, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>X</mi>
</mrow></math>, is a variable that can take different values, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>x</mi>
</mrow></math>, randomly. They can be <b>discrete</b>, like the number drawn from a dice, or <b>continuous</b>, like the humidity in the air.</div>
<div class="standard" id="magicparlabel-75">A probability distribution, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>p</mi>
</mrow></math>, is a <b>Probability Mass Function (PMF)</b> for discrete variables, and a <b>Probability Density Function (PDF)</b> for continuous random variables. It must satisfy:</div>
<ul class="itemize" id="magicparlabel-76"><li class="itemize_item">The domain of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>p</mi>
</mrow></math> describe all possible states of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>X</mi>
</mrow></math>.</li>
<li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> ∀ </mi><mi>x</mi><mo> ∈ </mo><mi>X</mi><mo>,</mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ≥ </mo><mn>0</mn>
</mrow>
</mrow></math>.</li>
<li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msubsup>
<mrow><mo> ∫ </mo>
</mrow>
<mrow>
<mrow><mi>x</mi><mo> ∈ </mo><mi>X</mi>
</mrow>
</mrow>
<mrow>
<mrow><mi>x</mi><mo> ∈ </mo><mi>X</mi>
</mrow>
</mrow>
</msubsup>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow><mo> ⁢ </mo>
<mrow><mo> ⅆ </mo><mi>x</mi>
</mrow><mo>=</mo><mn>1</mn>
</mrow>
</mrow></math>.</li>
</ul>
<div class="standard" id="magicparlabel-79">It is usual to have two (or more) random variables, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>X</mi>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>Y</mi>
</mrow></math>, and to be interested in the probability distribution of their combination, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>,</mo><mi>y</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>. In this context, we define the <b>marginal probability</b> of the variable <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>X</mi>
</mrow></math> as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>X</mi><mo>=</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msubsup>
<mrow><mo> ∫ </mo>
</mrow>
<mrow>
<mrow><mi>y</mi><mo> ∈ </mo><mi>Y</mi>
</mrow>
</mrow>
<mrow>
<mrow><mi>y</mi><mo> ∈ </mo><mi>Y</mi>
</mrow>
</mrow>
</msubsup>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>,</mo><mi>y</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow><mo> ⁢ </mo>
<mrow><mo> ⅆ </mo><mi>y</mi>
</mrow><mo>,</mo><mi> ∀ </mi><mi>x</mi><mo> ∈ </mo><mi>X</mi><mn>.</mn>
</mrow>
</mrow></math> The <b>conditional probability</b> of the variable <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>Y</mi>
</mrow></math> conditioned to <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>X</mi><mo>=</mo><mi>x</mi>
</mrow>
</mrow></math> is<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>Y</mi><mo>=</mo><mi>y</mi><mo>|</mo><mi>X</mi><mo>=</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mfrac>
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>Y</mi><mo>=</mo><mi>y</mi><mo>,</mo><mi>X</mi><mo>=</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
<mrow>
<mrow><mi>P</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>X</mi><mo>=</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</mfrac><mn>.</mn>
</mrow>
</mrow></math> Finally, there is the <b>chain rule of conditional probabilities</b>, in which we start with <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>n</mi>
</mrow></math> random variables, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>X</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo><mn>...</mn><mo>,</mo>
<msub>
<mrow><mi>X</mi>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msub>
</mrow>
</mrow></math>, and it follows:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>X</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo><mn>...</mn><mo>,</mo>
<msub>
<mrow><mi>X</mi>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>X</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msubsup>
<mrow><mo> ∏ </mo>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>=</mo><mn>2</mn>
</mrow>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msubsup><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>X</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>X</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo><mn>...</mn><mo>,</mo>
<msub>
<mrow><mi>X</mi>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<div class="example" id="magicparlabel-80"><div class="example_item"><span class="example_label">Example 2.1.</span>
For example, let's say <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>X</mi><mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">}</mo>
</mrow>
</mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>Y</mi><mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mrow><mn>1</mn><mo>,</mo><mn>2</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">}</mo>
</mrow>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>Z</mi><mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mrow><mn>1</mn><mo>,</mo><mn>2</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">}</mo>
</mrow>
</mrow></math> with the following probabilities:</div>
<div class="standard" id="magicparlabel-81" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-144"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>X</mi>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-147"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>Y</mi>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-150"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>Z</mi>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-153"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-156">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-159">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-162">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-165"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>6</mn>
</mrow>
</mfrac>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-168">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-171">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-174">2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-177"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>6</mn>
</mrow>
</mfrac>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-180">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-183">2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-186">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-189"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>12</mn>
</mrow>
</mfrac>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-192">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-195">2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-198">2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-201"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>24</mn>
</mrow>
</mfrac>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-204">2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-207">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-210">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-213"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>24</mn>
</mrow>
</mfrac>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-216">2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-219">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-222">2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-225"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>20</mn>
</mrow>
</mfrac>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-228">2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-231">2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-234">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-237"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>20</mn>
</mrow>
</mfrac>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-240">2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-243">2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-246">2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-249"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>20</mn>
</mrow>
</mfrac>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-252">3</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-255">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-258">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-261"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>6</mn>
</mrow>
</mfrac>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-264">3</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-267">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-270">2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-273"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>12</mn>
</mrow>
</mfrac>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-276">3</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-279">2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-282">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-285"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>20</mn>
</mrow>
</mfrac>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-288">3</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-291">2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-294">2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-297"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>20</mn>
</mrow>
</mfrac>
</mrow></math></div>
</td>
</tr></tbody>
</table>
</div>
<div class="example_item">Then, the marginal probabilities for the variable <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>X</mi>
</mrow></math> are<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>P</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>X</mi><mo>=</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>6</mn>
</mrow>
</mfrac><mo>+</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>6</mn>
</mrow>
</mfrac><mo>+</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>12</mn>
</mrow>
</mfrac><mo>+</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>24</mn>
</mrow>
</mfrac><mo>=</mo>
<mfrac>
<mrow><mn>11</mn>
</mrow>
<mrow><mn>24</mn>
</mrow>
</mfrac><mo>,</mo>
</mrow>
</mrow></math><math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>P</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>X</mi><mo>=</mo><mn>2</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>24</mn>
</mrow>
</mfrac><mo>+</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>20</mn>
</mrow>
</mfrac><mo>+</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>20</mn>
</mrow>
</mfrac><mo>+</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>20</mn>
</mrow>
</mfrac><mo>=</mo>
<mfrac>
<mrow><mn>23</mn>
</mrow>
<mrow><mn>120</mn>
</mrow>
</mfrac><mo>,</mo>
</mrow>
</mrow></math><math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>P</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>X</mi><mo>=</mo><mn>3</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>6</mn>
</mrow>
</mfrac><mo>+</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>12</mn>
</mrow>
</mfrac><mo>+</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>20</mn>
</mrow>
</mfrac><mo>+</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>20</mn>
</mrow>
</mfrac><mo>=</mo>
<mfrac>
<mrow><mn>21</mn>
</mrow>
<mrow><mn>60</mn>
</mrow>
</mfrac><mo>=</mo>
<mfrac>
<mrow><mn>7</mn>
</mrow>
<mrow><mn>20</mn>
</mrow>
</mfrac><mn>.</mn>
</mrow>
</mrow></math></div>
<div class="example_item">The conditional probability for the event <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mrow><mi>Y</mi><mo>=</mo><mn>1</mn><mo>|</mo><mi>X</mi><mo>=</mo><mn>3</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">}</mo>
</mrow></math> is:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>P</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>Y</mi><mo>=</mo><mn>1</mn><mo>|</mo><mi>X</mi><mo>=</mo><mn>3</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mfrac>
<mrow>
<mrow><mi>P</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>Y</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>X</mi><mo>=</mo><mn>3</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
<mrow>
<mrow><mi>P</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>X</mi><mo>=</mo><mn>3</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</mfrac><mo>=</mo>
<mfrac>
<mrow>
<mrow>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>6</mn>
</mrow>
</mfrac><mo>+</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>12</mn>
</mrow>
</mfrac>
</mrow>
</mrow>
<mrow>
<mfrac>
<mrow><mn>7</mn>
</mrow>
<mrow><mn>20</mn>
</mrow>
</mfrac>
</mrow>
</mfrac><mo>=</mo>
<mfrac>
<mrow>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>4</mn>
</mrow>
</mfrac>
</mrow>
<mrow>
<mfrac>
<mrow><mn>7</mn>
</mrow>
<mrow><mn>20</mn>
</mrow>
</mfrac>
</mrow>
</mfrac><mo>=</mo>
<mfrac>
<mrow><mn>5</mn>
</mrow>
<mrow><mn>7</mn>
</mrow>
</mfrac><mn>.</mn>
</mrow>
</mrow></math></div>
<div class="example_item">The conditional probability for the event <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mrow><mi>Z</mi><mo>=</mo><mn>1</mn><mo>|</mo><mi>X</mi><mo>=</mo><mn>3</mn><mo>,</mo><mi>Y</mi><mo>=</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">}</mo>
</mrow></math> is:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>P</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>Z</mi><mo>=</mo><mn>1</mn><mo>|</mo><mi>X</mi><mo>=</mo><mn>3</mn><mo>,</mo><mi>Y</mi><mo>=</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mfrac>
<mrow>
<mrow><mi>P</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>X</mi><mo>=</mo><mn>3</mn><mo>,</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>Z</mi><mo>=</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
<mrow>
<mrow><mi>P</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>X</mi><mo>=</mo><mn>3</mn><mo>,</mo><mi>Y</mi><mo>=</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</mfrac><mo>=</mo>
<mfrac>
<mrow>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>6</mn>
</mrow>
</mfrac>
</mrow>
<mrow>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>4</mn>
</mrow>
</mfrac>
</mrow>
</mfrac><mo>=</mo>
<mfrac>
<mrow><mn>2</mn>
</mrow>
<mrow><mn>3</mn>
</mrow>
</mfrac><mn>.</mn>
</mrow>
</mrow></math></div>
<div class="example_item">The probability of the event <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mrow><mi>X</mi><mo>=</mo><mn>3</mn><mo>,</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>Z</mi><mo>=</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">}</mo>
</mrow></math> could be computed from the conditional probabilities as follows, in case we only knew these:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow><mi>P</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>X</mi><mo>=</mo><mn>3</mn><mo>,</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>Z</mi><mo>=</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
</mrow>
</mtd>
<mtd>
<mrow><mi>P</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>X</mi><mo>=</mo><mn>3</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ⋅ </mo><mi>P</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>Y</mi><mo>=</mo><mn>1</mn><mo>|</mo><mi>X</mi><mo>=</mo><mn>3</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ⋅ </mo><mi>P</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>Z</mi><mo>=</mo><mn>1</mn><mo>|</mo><mi>X</mi><mo>=</mo><mn>3</mn><mo>,</mo><mi>Y</mi><mo>=</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mo>=</mo>
</mtd>
<mtd>
<mrow>
<mfrac>
<mrow><mn>7</mn>
</mrow>
<mrow><mn>20</mn>
</mrow>
</mfrac><mo> ⋅ </mo>
<mfrac>
<mrow><mn>5</mn>
</mrow>
<mrow><mn>7</mn>
</mrow>
</mfrac><mo> ⋅ </mo>
<mfrac>
<mrow><mn>2</mn>
</mrow>
<mrow><mn>3</mn>
</mrow>
</mfrac><mo>=</mo>
<mfrac>
<mrow><mn>10</mn>
</mrow>
<mrow><mn>60</mn>
</mrow>
</mfrac><mo>=</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>6</mn>
</mrow>
</mfrac><mn>.</mn>
</mrow>
</mtd>
</mtr>
</mtable></math></div>
</div>
<div class="standard" id="magicparlabel-302">When there are several variables, it is possible that the value of one of them is dependant, somehow, on the values that the other variables take; or that it is not:</div>
<div class="standard" id="magicparlabel-303"><div class="flex_color_box"><div class="definition" id="magicparlabel-307"><div class="definition_item"><span class="definition_label">Definition 2.2.</span>
Two random variables <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>X</mi>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>Y</mi>
</mrow></math> are <b>independant</b>, denoted <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>X</mi><mo> ⊥ </mo><mi>Y</mi>
</mrow>
</mrow></math>, if <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> ∀ </mi><mi>x</mi><mo> ∈ </mo><mi>X</mi><mo>,</mo><mi>y</mi><mo> ∈ </mo><mi>Y</mi><mo>,</mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>X</mi><mo>=</mo><mi>x</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mi>y</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>X</mi><mo>=</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ⋅ </mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>Y</mi><mo>=</mo><mi>y</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math> </div>
<div class="definition_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>X</mi>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>Y</mi>
</mrow></math> are <b>conditionally independent</b> given the random variable <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>Z</mi>
</mrow></math>, written <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>X</mi>
<msub>
<mrow><mo> ⊥ </mo>
</mrow>
<mrow><mi>Z</mi>
</mrow>
</msub><mi>Y</mi>
</mrow>
</mrow></math> if <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> ∀ </mi><mi>x</mi><mo> ∈ </mo><mi>X</mi><mo>,</mo><mi>y</mi><mo> ∈ </mo><mi>Y</mi><mo>,</mo><mi>z</mi><mo> ∈ </mo><mi>Z</mi>
</mrow>
</mrow></math>,<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>X</mi><mo>=</mo><mi>x</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>|</mo><mi>Z</mi><mo>=</mo><mi>z</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>X</mi><mo>=</mo><mi>x</mi><mo>|</mo><mi>Z</mi><mo>=</mo><mi>z</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ⋅ </mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>Y</mi><mo>=</mo><mi>y</mi><mo>|</mo><mi>Z</mi><mo>=</mo><mi>z</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
</div>
</div></div>
<div class="standard" id="magicparlabel-309">In Statistics and Machine Learning, there are some measures that summarize information about random variables, and that hold great importance.</div>
<div class="standard" id="magicparlabel-310"><div class="flex_color_box"><div class="definition" id="magicparlabel-314"><div class="definition_item"><span class="definition_label">Definition 2.3.</span>
The <b>expectation</b> of a function <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>x</mi><mo> ∼ </mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> is the average value of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>f</mi>
</mrow></math> over <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>x</mi>
</mrow></math>:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>x</mi><mo> ∼ </mo><mi>p</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>=</mo>
<msubsup>
<mrow><mo> ∫ </mo>
</mrow>
<mrow>
<mrow><mi>x</mi><mo> ∈ </mo><mi>X</mi>
</mrow>
</mrow>
<mrow>
<mrow><mi>x</mi><mo> ∈ </mo><mi>X</mi>
</mrow>
</mrow>
</msubsup>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow><mo> ⁢ </mo>
<mrow><mo> ⅆ </mo><mi>x</mi>
</mrow><mn>.</mn>
</mrow>
</mrow></math> The <b>variance</b> of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> measures how the values of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>f</mi>
</mrow></math> varies from its average:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>V</mi><mi>a</mi><mi>r</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>=</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>-</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msup>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>,</mo>
</mrow>
</mrow></math> and the <b>standard deviation </b>is the square root of the variance.</div>
<div class="definition_item">The <b>covariance</b> of two random variables provides informaiton about how much two values are linearly related. More generally, if we apply two functions <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>x</mi><mo> ∼ </mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>g</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>y</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>y</mi><mo> ∼ </mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>y</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, the covariance between them is:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>C</mi><mi>o</mi><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo><mi>g</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>y</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>=</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>-</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>g</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>y</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>-</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>g</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>y</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
</div>
</div></div>
<h3 class="subsection" id="magicparlabel-316"><span class="subsection_label">2.3</span> Machine Learning Basics</h3>
<div class="standard" id="magicparlabel-317">To finalize with this review chapter, we are going to remember some basic concepts of Machine Learning.</div>
<div class="standard" id="magicparlabel-318">First, let's give a definition of the concept:</div>
<div class="standard" id="magicparlabel-319"><div class="flex_color_box"><div class="definition" id="magicparlabel-323"><div class="definition_item"><span class="definition_label">Definition 2.4.</span>
A computer program is said to <b>learn</b> from experience <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>E</mi>
</mrow></math> with respect to some class of tasks <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>T</mi>
</mrow></math> and performance measure <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>P</mi>
</mrow></math>, if its performance at tasks in <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>T</mi>
</mrow></math>, as measured by <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>P</mi>
</mrow></math>, improves with experience <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>E</mi>
</mrow></math>.</div>
</div>
</div></div>
<ul class="itemize" id="magicparlabel-324"><li class="itemize_item">The <b>task</b> <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>T</mi>
</mrow></math> can be classification, regression, translation, generation, anomaly detection,...</li>
<li class="itemize_item">The <b>performance measure <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>P</mi>
</mrow></math> </b>is specific to the tasks involved, and can be accuracy for classification, for example. It is measured on a <b>test set</b>.</li>
<li class="itemize_item">The <b>experience</b> <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>E</mi>
</mrow></math> is divided into two main categories:

<ul class="itemize" id="magicparlabel-327"><li class="itemize_item"><b>Supervised learning</b>: a dataset of points associated with a label or a target determines the expected outcome of each event.</li>
<li class="itemize_item"><b>Unsupervised learning</b>: a dataset of points without labels or targets, in which the desirable outcome needs to be define in some different way.</li>
</ul>
</li></ul>
<div class="standard" id="magicparlabel-329">Mathematically, we can formalize this as having a dataset of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>m</mi>
</mrow></math> points and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>k</mi>
</mrow></math> features, which can be represented as a matrix <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>X</mi><mo> ∈ </mo>
<msup>
<mrow>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>m</mi><mo> × </mo><mi>k</mi>
</mrow>
</mrow>
</msup>
</mrow>
</mrow></math>. In the case of supervised learning, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>X</mi>
</mrow></math> is associated with a vector of labels, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>y</mi>
</mrow></math>, and we aim to learn a joint distribution, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>X</mi><mo>,</mo><mi>y</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> to infer<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>Y</mi><mo>=</mo><mi>y</mi><mo>|</mo><mi>X</mi><mo>=</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mfrac>
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>,</mo><mi>y</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
<mrow>
<mrow>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>y</mi><mo>'</mo>
</mrow>
</mrow>
</msub><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>,</mo><mi>y</mi><mo>'</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</mfrac><mn>.</mn>
</mrow>
</mrow></math> The goal is then to find a function <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mover>
<mrow><mi>f</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow></math> that associates each <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>x</mi>
</mrow></math> to the best approximation of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>y</mi>
</mrow></math>, and that is capable of generalizing to unseen data. Usually, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mover>
<mrow><mi>f</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow></math> is parameterized by a set of parameters, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> θ </mi>
</mrow></math>, which are learnt during training.</div>
<div class="standard" id="magicparlabel-330">The main challenge of an ML model is <b>generalization</b> to unseen data estimated on test data after the training on training data. <b>Overfitting</b> occurs when the gap between training error and test error is too large, while <b>underfitting</b> occurs when the training error is too large. The <b>capacity</b> of a model is the range of functions that it is able to leanr and control how likely the model can overfit or underfit. This is visualized in Figure <a href="#fig_Appropriate_capacity__overfittin">1</a>.</div>
<div class="float-figure"><div class="plain_layout" id="magicparlabel-335" style="text-align: center;"><img alt="image: 1_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___achine-Learning_LectureNotes_source_pegado1.png" src="1_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___achine-Learning_LectureNotes_source_pegado1.png" style="width:70%;"/>
</div>
<div class="plain_layout" id="magicparlabel-336"><span class="float-caption-Standard float-caption float-caption-standard">Figure 1:  <a id="fig_Appropriate_capacity__overfittin"></a>
Appropriate capacity, overfitting and underfitting visualization.</span></div>
</div>
<div class="standard" id="magicparlabel-341">When we want to train a model, we will define the parameters that characterize it, and then we need to obtain the best possible of the parameters, according to the data. For this, we use estimators:</div>
<div class="standard" id="magicparlabel-342"><div class="flex_color_box"><div class="definition" id="magicparlabel-346"><div class="definition_item"><span class="definition_label">Definition 2.5.</span>
Given an unknown parameter <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> θ </mi>
</mrow></math>, we estimate it through an <b>estimator</b>, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mover>
<mrow><mi> θ </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow></math>. A <b>point estimator</b> is a function of the data, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>X</mi>
</mrow></math>,<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mover>
<mrow><mi> θ </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover><mo>=</mo><mi>g</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>X</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math> The <b>bias</b> of an estimator is<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>b</mi><mi>i</mi><mi>a</mi><mi>s</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mover>
<mrow><mi> θ </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mover>
<mrow><mi> θ </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>-</mo><mi> θ </mi><mn>.</mn>
</mrow>
</mrow></math> An estimator is <b>unbiased </b>if <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>b</mi><mi>i</mi><mi>a</mi><mi>s</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mover>
<mrow><mi> θ </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mn>0</mn>
</mrow>
</mrow></math>.</div>
<div class="definition_item">The <b>variance</b> of an estimator is <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>V</mi><mi>a</mi><mi>r</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mover>
<mrow><mi> θ </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>.</div>
</div>
</div></div>
<div class="standard" id="magicparlabel-348">There are different ways to construct estimators, but one that is frequently used and that has solid mathematical foundations is the <b>maximum likelihood estimator</b>. Consider a dataset <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>X</mi><mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo><mn>...</mn><mo>,</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">}</mo>
</mrow>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>;</mo><mi> θ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> a parametric family of probability distribution that maps for each <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>x</mi>
</mrow></math> the probability <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow>
<mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>. This is, for each <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> θ </mi>
</mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>;</mo><mi> θ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> is a probability density function. The maximum likelihood estimator is then<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi> θ </mi>
</mrow>
<mrow>
<mrow><mi>M</mi><mi>L</mi>
</mrow>
</mrow>
</msub><mo>=</mo>
</mrow>
</mtd>
<mtd>
<mrow><mo> arg </mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow>
<mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>X</mi><mo>;</mo><mi> θ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mo>=</mo>
</mtd>
<mtd>
<mrow><mo> arg </mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub>
<msubsup>
<mrow><mo> ∏ </mo>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>=</mo><mn>1</mn>
</mrow>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msubsup>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow>
<mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>;</mo><mi> θ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mtd>
</mtr>
</mtable></math> considering that all instances of data are independent and identically distributed (iid). It is also a common practice to use the maximum <b>log</b>-likelihood instead, removing the product and avoiding floating point issues, since when the dataset is large, the product will rapidly go to 0. In addition, the logarithm does not modify the ordinals of the function. Therefore, we can use:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi> θ </mi>
</mrow>
<mrow>
<mrow><mi>M</mi><mi>L</mi>
</mrow>
</mrow>
</msub><mo>=</mo><mo> arg </mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub>
<msubsup>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>=</mo><mn>1</mn>
</mrow>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msubsup><mo> log </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow>
<mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>;</mo><mi> θ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<h2 class="section" id="magicparlabel-349"><span class="section_label">3</span> Deep Neural Networks</h2>
<h3 class="subsection" id="magicparlabel-350"><span class="subsection_label">3.1</span> Perceptron</h3>
<div class="standard" id="magicparlabel-351">A deeper explanation of the perceptron can be read in my notes from another course, <span class="flex_url">https://lorenc1o.github.io/BDMA_Notes/universities/UPC/Machine_Learning_summary.pdf</span>.</div>
<div class="standard" id="magicparlabel-356">A perceptron is an algorithm for supervised learning of binary classifiers. That is, we have a dataset <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>X</mi><mo> ∈ </mo>
<msup>
<mrow>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>n</mi><mo> × </mo><mi>m</mi>
</mrow>
</mrow>
</msup>
</mrow>
</mrow></math> associated with a vector of labels <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>y</mi><mo> ∈ </mo>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mrow><mn>0</mn><mo>,</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">}</mo>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msup>
</mrow>
</mrow></math>. Then, the perceptron learns a function <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mover>
<mrow><mi>f</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow></math> parametrized by a vector of weights <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>w</mi><mo> ∈ </mo>
<msup>
<mrow>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
<mrow><mi>m</mi>
</mrow>
</msup>
</mrow>
</mrow></math> and a bias <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>b</mi>
</mrow></math>, such that:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mover>
<mrow><mi>f</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mtable>
<mtr>
<mtd><mn>1</mn>
</mtd>
<mtd>
<mrow><mi>i</mi><mi>f</mi><mspace width="10px"></mspace><mi>w</mi><mo> ⋅ </mo><mi>x</mi><mo>+</mo><mi>b</mi><mo>&gt;</mo><mn>0</mn>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mn>0</mn>
</mtd>
<mtd><mo> ∼ </mo>
</mtd>
</mtr>
</mtable><mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-357">Therefore, it is a linear classifier, which divides the input space into two regions separated by a hyperplane. This means that a perceptron cannot separate non-linear data.</div>
<h3 class="subsection" id="magicparlabel-358"><span class="subsection_label">3.2</span> Multi-layer perceptron</h3>
<div class="standard" id="magicparlabel-359">A deeper explanation of the MLP can be read in my notes from another course, <span class="flex_url">https://lorenc1o.github.io/BDMA_Notes/universities/UPC/Machine_Learning_summary.pdf</span>.</div>
<div class="standard" id="magicparlabel-364">When we say 'Deep' neural network, we refer to a series of stacked perceptrons. However, just like this, the model is still linear. This is why activation functions are introduced. An <b>activation function</b> is a function that is applied to the output of a perceptron, to make it non linear.</div>
<div class="standard" id="magicparlabel-365">For example, ReLU is a piecewise-linear function defined as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>z</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mtable>
<mtr>
<mtd><mi>z</mi>
</mtd>
<mtd>
<mrow><mi>i</mi><mi>f</mi><mspace width="10px"></mspace><mi>z</mi><mo> ≥ </mo><mn>0</mn>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mn>0</mn>
</mtd>
<mtd><mo> ∼ </mo>
</mtd>
</mtr>
</mtable><mo>=</mo><mo> max </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mrow><mi>z</mi><mo>,</mo><mn>0</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">}</mo>
<mn>.</mn>
</mrow>
</mrow></math> This function preserves much of the good oprimization properties of a linear function, i.e., it is differentiable (apart from one point), and its derivative is constant.</div>
<div class="example" id="magicparlabel-366"><div class="example_item"><span class="example_label">Example 3.1.</span>
Learn the XOR function with a 2-layer MLP.</div>
<div class="example_item">The XOR function is represented with the table:</div>
<div class="standard" id="magicparlabel-368" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-393"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-396"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-399"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>y</mi>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-402">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-405">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-408">0</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-411">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-414">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-417">1</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-420">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-423">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-426">1</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-429">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-432">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-435">0</div>
</td>
</tr></tbody>
</table>
</div>
<div class="example_item">We want to use a 2-layer MLP to learn this function:</div>
<div class="standard" id="magicparlabel-437" style="text-align: center;"><img alt="image: 2_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___ine-Learning_LectureNotes_source_xor_drawio.png" src="2_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___ine-Learning_LectureNotes_source_xor_drawio.png" style="width:60%;"/>
</div>
<div class="example_item">In <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>h</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow></math>, it will be<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>11</mn>
</mrow>
</msub>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>12</mn>
</mrow>
</msub>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo>
</mrow>
</mrow></math> and in <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>h</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow></math><math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>11</mn>
</mrow>
</msub>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>12</mn>
</mrow>
</msub>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mn>.</mn>
</mrow>
</mrow></math> This can be represented as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>h</mi><mo>=</mo>
<msubsup>
<mrow><mi>W</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msubsup><mi>X</mi><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub><mn>.</mn>
</mrow>
</mrow></math> Then, we apply ReLU<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> max </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>0</mn><mo>,</mo>
<msubsup>
<mrow><mi>W</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msubsup><mi>X</mi><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> and finally the output layer<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>y</mi><mo>=</mo>
<msubsup>
<mrow><mi>W</mi>
</mrow>
<mrow><mi>y</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msubsup><mo> max </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>0</mn><mo>,</mo>
<msubsup>
<mrow><mi>W</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msubsup><mi>X</mi><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mi>y</mi>
</mrow>
</msub><mn>.</mn>
</mrow>
</mrow></math></div>
<div class="example_item">Let's see the different inputs:</div>
<div class="standard" id="magicparlabel-440" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-471"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-474"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-477"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>h</mi>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-480"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>y</mi>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-483">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-486">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-489"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>11</mn>
</mrow>
</msub>
</mtd>
<mtd>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>12</mn>
</mrow>
</msub>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>21</mn>
</mrow>
</msub>
</mtd>
<mtd>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>22</mn>
</mrow>
</msub>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd><mn>0</mn>
</mtd>
</mtr>
<mtr>
<mtd><mn>0</mn>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo>+</mo><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo>=</mo><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-492"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd>
<msubsup>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
<mrow><mi>y</mi>
</mrow>
</msubsup>
</mtd>
<mtd>
<msubsup>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
<mrow><mi>y</mi>
</mrow>
</msubsup>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd>
<mrow><mo> max </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>0</mn><mo>,</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow><mo> max </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>0</mn><mo>,</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mi>y</mi>
</mrow>
</msub><mo> ≤ </mo><mn>0</mn>
</mrow>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-495">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-498">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-501"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>12</mn>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>22</mn>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-504"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd>
<msubsup>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
<mrow><mi>y</mi>
</mrow>
</msubsup>
</mtd>
<mtd>
<msubsup>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
<mrow><mi>y</mi>
</mrow>
</msubsup>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd>
<mrow><mo> max </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>0</mn><mo>,</mo>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>12</mn>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow><mo> max </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>0</mn><mo>,</mo>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>22</mn>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mi>y</mi>
</mrow>
</msub><mo>&gt;</mo><mn>0</mn>
</mrow>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-507">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-510">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-513"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>11</mn>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>21</mn>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-516"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd>
<msubsup>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
<mrow><mi>y</mi>
</mrow>
</msubsup>
</mtd>
<mtd>
<msubsup>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
<mrow><mi>y</mi>
</mrow>
</msubsup>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd>
<mrow><mo> max </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>0</mn><mo>,</mo>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>11</mn>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow><mo> max </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>0</mn><mo>,</mo>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>21</mn>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mi>y</mi>
</mrow>
</msub><mo>&gt;</mo><mn>0</mn>
</mrow>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-519">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-522">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-525"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>11</mn>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>12</mn>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>21</mn>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>22</mn>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-528"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd>
<msubsup>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
<mrow><mi>y</mi>
</mrow>
</msubsup>
</mtd>
<mtd>
<msubsup>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
<mrow><mi>y</mi>
</mrow>
</msubsup>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd>
<mrow><mo> max </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>0</mn><mo>,</mo>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>11</mn>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>12</mn>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow><mo> max </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>0</mn><mo>,</mo>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>21</mn>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow><mn>22</mn>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mi>y</mi>
</mrow>
</msub><mo> ≤ </mo><mn>0</mn>
</mrow>
</mrow></math></div>
</td>
</tr></tbody>
</table>
</div>
<div class="example_item">A solution is:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>W</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub><mo>=</mo><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd><mn>1</mn>
</mtd>
<mtd><mn>1</mn>
</mtd>
</mtr>
<mtr>
<mtd><mn>1</mn>
</mtd>
<mtd><mn>1</mn>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo>,</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub><mo>=</mo><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd><mn>0</mn>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow><mo>-</mo><mn>1</mn>
</mrow>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo>,</mo>
<msub>
<mrow><mi>W</mi>
</mrow>
<mrow><mi>y</mi>
</mrow>
</msub><mo>=</mo><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd><mn>1</mn>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow><mo>-</mo><mn>2</mn>
</mrow>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo>,</mo><mi>b</mi><mo>=</mo><mn>0.</mn>
</mrow>
</mrow></math> Let's check:</div>
<div class="standard" id="magicparlabel-530" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-561"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-564"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-567"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>h</mi>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-570"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>y</mi>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-573">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-576">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-579"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd><mn>0</mn>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow><mo>-</mo><mn>1</mn>
</mrow>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-582"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd><mn>1</mn>
</mtd>
<mtd>
<mrow><mo>-</mo><mn>2</mn>
</mrow>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd><mn>0</mn>
</mtd>
</mtr>
<mtr>
<mtd><mn>0</mn>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo>=</mo><mn>0</mn><mo> ≤ </mo><mn>0</mn>
</mrow>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-585">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-588">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-591"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd><mn>1</mn>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow><mn>1</mn><mo>-</mo><mn>1</mn>
</mrow>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-594"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd><mn>1</mn>
</mtd>
<mtd>
<mrow><mo>-</mo><mn>2</mn>
</mrow>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd><mn>1</mn>
</mtd>
</mtr>
<mtr>
<mtd><mn>0</mn>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo>=</mo><mn>1</mn><mo>&gt;</mo><mn>0</mn>
</mrow>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-597">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-600">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-603"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd><mn>1</mn>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow><mn>1</mn><mo>-</mo><mn>1</mn>
</mrow>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-606"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd><mn>1</mn>
</mtd>
<mtd>
<mrow><mo>-</mo><mn>2</mn>
</mrow>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd><mn>1</mn>
</mtd>
</mtr>
<mtr>
<mtd><mn>0</mn>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo>=</mo><mn>1</mn><mo>&gt;</mo><mn>0</mn>
</mrow>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-609">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-612">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-615"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd>
<mrow><mn>1</mn><mo>+</mo><mn>1</mn>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow><mn>1</mn><mo>+</mo><mn>1</mn><mo>-</mo><mn>1</mn>
</mrow>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-618"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd><mn>1</mn>
</mtd>
<mtd>
<mrow><mo>-</mo><mn>2</mn>
</mrow>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd><mn>2</mn>
</mtd>
</mtr>
<mtr>
<mtd><mn>1</mn>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mo>=</mo><mn>0</mn><mo> ≤ </mo><mn>0</mn>
</mrow>
</mrow></math></div>
</td>
</tr></tbody>
</table>
</div>
<div class="example_item">So, it works! Note that this solution is not unique!</div>
<div class="example_item">What happens is actually that the solution for the XOR problem is not linearly separable:</div>
<div class="standard" id="magicparlabel-621" style="text-align: center;"><img alt="image: 3_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___hine-Learning_LectureNotes_source_xor_space.png" src="3_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___hine-Learning_LectureNotes_source_xor_space.png" style="width:60%;"/>
</div>
<div class="example_item">But, the hidden layer transforms this space, making the problem linearly separable, and therefore solvable in the last layer:</div>
<div class="standard" id="magicparlabel-623" style="text-align: center;"><img alt="image: 4_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___earning_LectureNotes_source_xor_space_trans.png" src="4_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___earning_LectureNotes_source_xor_space_trans.png" style="width:60%;"/>
</div>
</div>
<h3 class="subsection" id="magicparlabel-624"><span class="subsection_label">3.3</span> Cost Functions</h3>
<div class="standard" id="magicparlabel-625">The cost function is important when working with neural networks, because our goal is to ultimately train the model to solve some problem, and the cost functions will be the function that our model will aim at minimizing, thus guiding the training process.</div>
<div class="standard" id="magicparlabel-626">Usually, we will need to choose a cost function <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mstyle mathvariant="script"><mi>L</mi>
</mstyle>
</mrow></math> that is suitable for our problem. Then, we will minimize <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mstyle mathvariant="script"><mi>L</mi>
</mstyle>
</mrow></math> with stochastic gradient descent, by:</div>
<ul class="itemize" id="magicparlabel-627"><li class="itemize_item">Training on a training dataset.</li>
<li class="itemize_item">Estimating error on an evaluation dataset.</li>
<li class="itemize_item">Computing the gradients using backpropagation.</li>
</ul>
<div class="standard" id="magicparlabel-630">In this process, we will aim to find good local minima, instead of global minimum. This is related to overfitting (learning only the training data, losing generalization capabilities), and to the empirical fact that deep neural network have surprisingly good local and non-global optima.</div>
<h4 class="subsubsection" id="magicparlabel-631"><span class="subsubsection_label">3.3.1</span> Choice of cost function</h4>
<div class="standard" id="magicparlabel-632">In the general case, we use the maximum likelihood principle, taking the output types of the network into account. This means that we assume our dataset <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo><mn>...</mn><mo>,</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">}</mo>
</mrow></math> to be independently and identically distributed (i.i.d.) from an unknown distribution, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow>
<mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>. We choose a parametric model family <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow>
<mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>;</mo><mi> θ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> represented as a neural network, which we use to estimate an approximation of the true distribution. For this, we utilize the <b>maximum likelihood estimator</b>, defined as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi> θ </mi>
</mrow>
<mrow>
<mrow><mi>M</mi><mi>L</mi>
</mrow>
</mrow>
</msub><mo>=</mo><mo> arg </mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub>
<msubsup>
<mrow><mo> ∏ </mo>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>=</mo><mn>1</mn>
</mrow>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msubsup>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow>
<mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>;</mo><mi> θ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math> Usually, to avoid floating point errors, the log-likelihood is used instead:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi> θ </mi>
</mrow>
<mrow>
<mrow><mi>M</mi><mi>L</mi>
</mrow>
</mrow>
</msub><mo>=</mo><mo> arg </mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub>
<msubsup>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>=</mo><mn>1</mn>
</mrow>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msubsup><mo> log </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow>
<mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>;</mo><mi> θ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-633">In the maximum likelihood estimation framework, we might apply activation functions to the output layer to get a desired structure for our distribution. This choice will also influence the mathematical form of the cost function. For example, we can use linear units for regression or for Gaussian distributions, sigmoid units for binary classification or softmax units for multi-class classification.</div>
<h5 class="paragraph" id="magicparlabel-634"><span class="paragraph_label"></span> Linear units for regression</h5>
<div class="standard" id="magicparlabel-635">A <b>linear output layer</b> is such that, given the features <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>h</mi>
</mrow></math>, the output is<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mover>
<mrow><mi>y</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover><mo>=</mo>
<msup>
<mrow><mi>W</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msup><mi>h</mi><mo>+</mo><mi>b</mi><mo>,</mo>
</mrow>
</mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>W</mi>
</mrow></math> is the weights vector and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>b</mi>
</mrow></math> the bias.</div>
<div class="standard" id="magicparlabel-636">We can use this to predict real or vector valued variables, such as prices, biometrics,...</div>
<h5 class="paragraph" id="magicparlabel-637"><span class="paragraph_label"></span> Linear unit for Gaussian distribution</h5>
<div class="standard" id="magicparlabel-638">A <b>Gaussian output unit</b> is such that, given features <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>h</mi>
</mrow></math>, a linear layer produces a vector <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mover>
<mrow><mi>y</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow></math> representing the mean and the covariance matrix of a conditional Gaussian distribution:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>y</mi><mo>|</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="script"><mi>N</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>y</mi><mo>;</mo>
<mover>
<mrow><mi>y</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover><mo>,</mo><mi>I</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-639">Covariance is usually not modelled or simplified to be diagonal (in which case we need to ensure that the output is non-negative).</div>
<h5 class="paragraph" id="magicparlabel-640"><span class="paragraph_label"></span> Binary classification</h5>
<div class="standard" id="magicparlabel-641">In this case, the objective is to predict a binary variable, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>y</mi>
</mrow></math>: the neural network must predict <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>P</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>y</mi><mo>=</mo><mn>1</mn><mo>|</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>. Thus, we must ensure that the output is a probability, in the interval <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mn>0</mn><mo>,</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow></math>. For this, we can take<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>P</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>y</mi><mo>=</mo><mn>1</mn><mo>|</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo> max </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mrow><mn>0</mn><mo>,</mo><mo> min </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mrow><mn>1</mn><mo>,</mo>
<msup>
<mrow><mi>W</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msup><mi>h</mi><mo>+</mo><mi>b</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">}</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">}</mo>
<mn>.</mn>
</mrow>
</mrow></math> The problem with this approach is that if <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>W</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msup><mi>h</mi><mo>+</mo><mi>b</mi><mo> ∉ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mn>0</mn><mo>,</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
</mrow></math> then the gradient is 0 and the training will stop. To solve this issue, we can use a <b>sigmoid unit</b>, which is <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mover>
<mrow><mi>y</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover><mo>=</mo><mi> σ </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msup>
<mrow><mi>W</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msup><mi>h</mi><mo>+</mo><mi>b</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow>
<mrow><mn>1</mn><mo>+</mo>
<msup>
<mrow><mi>e</mi>
</mrow>
<mrow>
<mrow><mo>-</mo>
<msup>
<mrow><mi>W</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msup><mi>h</mi><mo>+</mo><mi>b</mi>
</mrow>
</mrow>
</msup>
</mrow>
</mrow>
</mfrac><mn>.</mn>
</mrow>
</mrow></math></div>
<h5 class="paragraph" id="magicparlabel-642"><span class="paragraph_label"></span> Softmax unit for multi-class classification</h5>
<div class="standard" id="magicparlabel-643">Now our objective is to classify the input data into one among <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>N</mi><mo>&gt;</mo><mn>2</mn>
</mrow>
</mrow></math> classes. We want to predict <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mover>
<mrow><mi>y</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow></math> with <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mover>
<mrow>
<msub>
<mrow><mi>y</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
</mrow><mo stretchy="true">ˆ</mo>
</mover><mi>P</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>y</mi><mo>=</mo><mi>i</mi><mo>|</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, subject to <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mover>
<mrow>
<msub>
<mrow><mi>y</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
</mrow><mo stretchy="true">ˆ</mo>
</mover><mo> ∈ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mn>0</mn><mo>,</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>,</mo><mi> ∀ </mi><mi>i</mi>
</mrow>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
<msub>
<mrow><mi>y</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>=</mo><mn>1</mn>
</mrow>
</mrow></math>.</div>
<div class="standard" id="magicparlabel-644">In the output layer we can have <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>N</mi>
</mrow></math> perceptrons, each of them computing <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>z</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>=</mo><mo> log </mo><mi>P</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>y</mi><mo>=</mo><mi>i</mi><mo>|</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, i.e., the <b>logits</b>. With this, we can apply the <b>softmax output unit</b> to all of them, obtaining our vector of probabilities, as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi>
<msub>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>z</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>=</mo>
<mfrac>
<mrow>
<msup>
<mrow><mi>e</mi>
</mrow>
<mrow>
<msub>
<mrow><mi>z</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
</mrow>
</msup>
</mrow>
<mrow>
<mrow>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow><mi>j</mi>
</mrow>
</msub>
<msup>
<mrow><mi>e</mi>
</mrow>
<mrow>
<msub>
<mrow><mi>z</mi>
</mrow>
<mrow><mi>j</mi>
</mrow>
</msub>
</mrow>
</msup>
</mrow>
</mrow>
</mfrac><mn>.</mn>
</mrow>
</mrow></math></div>
<h4 class="subsubsection" id="magicparlabel-645"><span class="subsubsection_label">3.3.2</span> Cross-entropy</h4>
<div class="standard" id="magicparlabel-646">In classification problems we want to estimate the probability of different outcomes. Let the estimated probability of outcome <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>i</mi>
</mrow></math> be <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow>
<mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>=</mo><mi>i</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> with to-be-optimized parameters <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> θ </mi>
</mrow></math> and let the frequency of outcome <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>i</mi>
</mrow></math> in the training set be <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>=</mo><mi>i</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>. Given <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>N</mi>
</mrow></math> conditionally independent samples in the training set, then the likelihood of the parameters <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> θ </mi>
</mrow></math> of the model <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow>
<mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>=</mo><mi>i</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> on the training set is:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mstyle mathvariant="script"><mi>L</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mo> ∏ </mo>
</mrow>
<mrow>
<mrow><mi>i</mi><mo> ∈ </mo><mi>X</mi>
</mrow>
</mrow>
</msub>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow>
<mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi>
</mrow>
</mrow>
</msub>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>=</mo><mi>i</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mrow>
<mrow><mi>N</mi><mo> ⋅ </mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>=</mo><mi>i</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</msup><mn>.</mn>
</mrow>
</mrow></math> Therefore, the log-likelihood, divided by <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>N</mi>
</mrow></math>, is<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mi>N</mi>
</mrow>
</mfrac><mo> log </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<mstyle mathvariant="script"><mi>L</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mi>N</mi>
</mrow>
</mfrac><mi>N</mi><mo> ⋅ </mo>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>i</mi><mo> ∈ </mo><mi>X</mi>
</mrow>
</mrow>
</msub><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>=</mo><mi>i</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> log </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow>
<mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>=</mo><mi>i</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>i</mi><mo> ∈ </mo><mi>X</mi>
</mrow>
</mrow>
</msub><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>=</mo><mi>i</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> log </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow>
<mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>=</mo><mi>i</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-647">Cross-entropy minimization is frequently used in optimization and rare-event probability estimation. When comparing a distribution <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>q</mi>
</mrow></math> against a fixed reference distribution <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>p</mi>
</mrow></math>, cross-entropy and KL divergence are identical up to an additive constant (since <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>p</mi>
</mrow></math> is fixed): According to the Gibbs' inequality, both take on their minimal values when <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo>=</mo><mi>q</mi>
</mrow>
</mrow></math>, which is <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mn>0</mn>
</mrow></math> for KL divergence, and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>H</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>p</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> for cross-entropy. In the engineering literature, the principle of minimizing KL divergence (Kullback's "Principle of Minimum Discrimination Information") is often called the Principle of Minimum Cross-Entropy (MCE), or Minxent.</div>
<h3 class="subsection" id="magicparlabel-648"><span class="subsection_label">3.4</span> Why deep NN?</h3>
<div class="standard" id="magicparlabel-649">Depth is the longest data path data can take from input to output. For a deep feed forward NN, depth is the number of hidden layers plus the output layer. State-of-the-art architectures used in practice have dozens to hundreds of layers.</div>
<div class="standard" id="magicparlabel-650"><div class="flex_color_box"><div class="theorem" id="magicparlabel-654"><div class="theorem_item"><span class="theorem_label">Theorem 3.1.</span>
Universal Approximation Theorem</div>
<div class="theorem_item">Let <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> ϕ </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow></mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> be a nonconstant, bounded, and monotonically increasing continuous function. Let <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>I</mi>
</mrow>
<mrow>
<msub>
<mrow><mi>m</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
</msub>
</mrow></math> denote the <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>m</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>-</mo>
</mrow>
</mrow></math>dimensional unit hypercube, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mn>0</mn><mo>,</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
<mrow>
<msub>
<mrow><mi>m</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
</msup>
</mrow></math>. The space of continuous functions on <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>I</mi>
</mrow>
<mrow>
<msub>
<mrow><mi>m</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
</msub>
</mrow></math> is denoted by <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>C</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>I</mi>
</mrow>
<mrow>
<msub>
<mrow><mi>m</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>.</div>
<div class="theorem_item">Then, given any function <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>f</mi><mo> ∈ </mo><mi>C</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>I</mi>
</mrow>
<mrow>
<msub>
<mrow><mi>m</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> ϵ </mi><mo>&gt;</mo><mn>0</mn>
</mrow>
</mrow></math>, there exists an integer <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>m</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow></math> and sets of real constants <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi> α </mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
</mrow>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow>
<mrow><mi>i</mi><mi>j</mi>
</mrow>
</mrow>
</msub><mo> ∈ </mo>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
</mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>...</mn><mo>,</mo>
<msub>
<mrow><mi>m</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>...</mn><mo>,</mo>
<msub>
<mrow><mi>m</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
</mrow></math> such that we may define<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>F</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msubsup>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>=</mo><mn>1</mn>
</mrow>
</mrow>
<mrow>
<msub>
<mrow><mi>m</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
</msubsup>
<msub>
<mrow><mi> α </mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo> ⋅ </mo><mi> ϕ </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msubsup>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>j</mi><mo>=</mo><mn>1</mn>
</mrow>
</mrow>
<mrow>
<msub>
<mrow><mi>m</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
</msubsup>
<msub>
<mrow><mi>w</mi>
</mrow>
<mrow>
<mrow><mi>i</mi><mi>j</mi>
</mrow>
</mrow>
</msub><mo> ⋅ </mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>j</mi>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> as an approximate realization of the function <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>f</mi>
</mrow></math>, that is<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">|</mo>
<mrow><mi>F</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>-</mo><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">|</mo>
<mo>&lt;</mo><mi> ϵ </mi><mo>,</mo><mi> ∀ </mi><mi>x</mi><mo> ∈ </mo>
<msub>
<mrow><mi>I</mi>
</mrow>
<mrow><mi>m</mi>
</mrow>
</msub><mn>.</mn>
</mrow>
</mrow></math></div>
</div>
</div></div>
<div class="standard" id="magicparlabel-657">This theorem is very relevant, because it says that for any mapping function <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>f</mi>
</mrow></math> in supervised learning, there exists a MLP with <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>m</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow></math> neurons in the hidden layer which is able to approximate it with a desired precision.</div>
<div class="standard" id="magicparlabel-658">However, it only proves the existence of a shallow (just one hidden layer) MLP with <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>m</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow></math> neurons in the hidden layer that can approximate the function, but it does not tell how to find this number.</div>
<div class="standard" id="magicparlabel-659">As a rule of thumb for the generalization error, it is<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> ϵ </mi><mo>=</mo>
<mfrac>
<mrow>
<mrow><mi>V</mi>
<msub>
<mrow><mi>C</mi>
</mrow>
<mrow>
<mrow><mi>d</mi><mi>i</mi><mi>m</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>M</mi><mi>L</mi><mi>P</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
<mrow><mi>N</mi>
</mrow>
</mfrac><mo>,</mo>
</mrow>
</mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>V</mi>
<msub>
<mrow><mi>C</mi>
</mrow>
<mrow>
<mrow><mi>d</mi><mi>i</mi><mi>m</mi>
</mrow>
</mrow>
</msub>
</mrow>
</mrow></math> is the Vapnik-Chervonenkis dimension, a measure of the capacity of a model. It refers to the largest set of points that the model can shatter. It is not easy to compute, but a rough upper bound for a FFNN is <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>O</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>W</mi><mo> log </mo><mi>W</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, with <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>W</mi>
</mrow></math> being the total number of weight in the network.</div>
<div class="standard" id="magicparlabel-660">Also, this theorem hints us that having more neurons in the hidden layers will give us better training error, but worse generalization error: overfitting.</div>
<div class="standard" id="magicparlabel-661">However, for most functions <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>m</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow></math> is very high, and becomes quickly computationally intractable: so we need to go deeper.</div>
<div class="standard" id="magicparlabel-662"><div class="flex_color_box"><div class="theorem" id="magicparlabel-666"><div class="theorem_item"><span class="theorem_label">Theorem 3.2.</span>
No Free Lunch Theorem</div>
<div class="theorem_item">Multiple informal formulations:</div>
<ul class="itemize" id="magicparlabel-668"><li class="itemize_item"><i>For every learning algorithm A and B, there are as many problems where A has a better generalization error than problems where B has a better one.</i></li>
<li class="itemize_item"><i>All learning algorithms ahve the same generalization error if we average over all learning problems.</i></li>
<li class="itemize_item"><i>There is no universally better learning algorithm.</i></li>
</ul>
</div>
</div></div>
<div class="standard" id="magicparlabel-671"><div class="flex_color_box"><div class="plain_layout" id="magicparlabel-675"><b>Depth Property</b></div>
<div class="plain_layout" id="magicparlabel-676">The number of polygonal regions generated by a MLP with a ReLU function, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>d</mi>
</mrow></math> inputs, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>n</mi>
</mrow></math> neurons per hidden layer and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>l</mi>
</mrow></math> layers is<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>O</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mfrac linethickness="0"><mi>n</mi><mi>d</mi></mfrac><mo fence="true" form="postfix" stretchy="true">)</mo>
</mrow>
<mrow>
<mrow><mi>d</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>l</mi><mo>-</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</msup>
<msup>
<mrow><mi>n</mi>
</mrow>
<mrow><mi>d</mi>
</mrow>
</msup>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
</div></div>
<div class="standard" id="magicparlabel-677">This number grows exponentially with depth. This means that adding depth basically allows for more transformations of the input space.</div>
<h3 class="subsection" id="magicparlabel-678"><span class="subsection_label">3.5</span> Gradient-based Learning</h3>
<div class="standard" id="magicparlabel-679">The <b>gradient</b> of a function <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>f</mi><mo>:</mo>
<msup>
<mrow>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msup><mo> → </mo>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
</mrow></math>, is <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> ∇ </mi><mi>f</mi><mo>:</mo>
<msup>
<mrow>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msup><mo> → </mo>
<msup>
<mrow>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msup>
</mrow>
</mrow></math>, defined at the point <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo><mn>...</mn><mo>,</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> ∇ </mi><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>p</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo fence="true" form="prefix" lspace="thinmathspace" stretchy="true" symmetric="true">(</mo>
<mtable>
<mtr>
<mtd>
<mrow>
<mfrac>
<mrow>
<mrow><mi> ∂ </mi><mi>f</mi>
</mrow>
</mrow>
<mrow>
<mrow><mi> ∂ </mi>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
</mrow>
</mfrac><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>p</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<mi>⋮
      </mi>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow>
<mfrac>
<mrow>
<mrow><mi> ∂ </mi><mi>f</mi>
</mrow>
</mrow>
<mrow>
<mrow><mi> ∂ </mi>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msub>
</mrow>
</mrow>
</mfrac><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>p</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
</mtable><mo fence="true" form="postfix" lspace="thinmathspace" stretchy="true" symmetric="true">)</mo><mn>.</mn>
</mrow>
</mrow></math> This is, it's the local derivative or slope of each dimension at a certain point.</div>
<div class="standard" id="magicparlabel-680">Going in the opposite direction of the gradient is a naïve but practical guess of the direction of the local minimum. This is the base for the gradient descent method.</div>
<div class="standard" id="magicparlabel-681"><div class="flex_color_box"><div class="plain_layout" id="magicparlabel-685"><b>Gradient-descent method </b>(Cauchy, 1847)</div>
<div class="plain_layout" id="magicparlabel-686">A parametric function <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> can be iteratively minimized by following the opposite direction of the gradient:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi> θ </mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi> θ </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>-</mo><mi> ϵ </mi>
<msub>
<mrow><mi> ∇ </mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> ϵ </mi><mo>&gt;</mo><mn>0</mn>
</mrow>
</mrow></math> is the <b>learning rate</b>.</div>
<div class="plain_layout" id="magicparlabel-687">We stop iterating when the gradient is near to 0.</div>
</div></div>
<div class="standard" id="magicparlabel-688">Notice that this is useless if we have a close form for the gradient! In that case it is easier to just minimize it. This is useful when this is not the case, which always happens for neural networks.</div>
<div class="standard" id="magicparlabel-689">In addition, there are variations to the method, for example, we can vary <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> ϵ </mi>
</mrow></math> during training.</div>
<div class="standard" id="magicparlabel-690"><div class="flex_color_box"><div class="plain_layout" id="magicparlabel-694"><b>Stochastic gradient descent</b></div>
<div class="plain_layout" id="magicparlabel-695">Given a cost function <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, parameters of the network are updated with<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> θ </mi><mo> ← </mo><mi> θ </mi><mo>-</mo><mi> ϵ </mi>
<msub>
<mrow><mi> ∇ </mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<div class="plain_layout" id="magicparlabel-696">For the negative log-likelihood (MLE), the function is:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mi>m</mi>
</mrow>
</mfrac>
<msubsup>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>=</mo><mn>1</mn>
</mrow>
</mrow>
<mrow><mi>m</mi>
</mrow>
</msubsup><mi>L</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msup>
<mrow><mi>x</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>i</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup><mo>,</mo>
<msup>
<mrow><mi>y</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>i</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup><mo>,</mo><mi> θ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> so the estimated gradient is<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi> ∇ </mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mi>m</mi>
</mrow>
</mfrac>
<msubsup>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>=</mo><mn>1</mn>
</mrow>
</mrow>
<mrow><mi>m</mi>
</mrow>
</msubsup>
<msub>
<mrow><mi> ∇ </mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mi>L</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msup>
<mrow><mi>x</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>i</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup><mo>,</mo>
<msup>
<mrow><mi>y</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>i</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup><mo>,</mo><mi> θ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
</div></div>
<div class="standard" id="magicparlabel-697">The problem with this approach is that to take a single step of gradient descent, we must compute the loss over the whole dataset everytime, making the method not scalable at all. This is called <b>batch gradient descent</b>.</div>
<div class="standard" id="magicparlabel-698">One solution is to compute the gradient with 1 sample only at each step, which is very noisy and inefficient, but works. This is the <b>stochastic gradient descent</b>.</div>
<div class="standard" id="magicparlabel-699">In the middle ground, we find the <b>mini-batch gradient descent</b>, which divides the dataset into subsets, and updates the parameters after processing each of these subsets. A batch is a collection is a collection of samples used at each iteration for performing SDG in DL. A bigger batch provides a better gradient estimation, and therefore a faster learning, but also implies more device memory and slower descent.</div>
<div class="standard" id="magicparlabel-700">Therefore, there is a tradeoff between money and performance at companies. In practice, a batch is set between 1 to 256 on one GPU.</div>
<div class="standard" id="magicparlabel-701">But there is an even <b>greater problem</b>, the computation of the gradient is computationally very costly. To go around this problem, <b>back-propagation</b> was invented, as an efficient technique for gradient computation. </div>
<h4 class="subsubsection" id="magicparlabel-702"><span class="subsubsection_label">3.5.1</span> Back-propagation</h4>
<div class="standard" id="magicparlabel-703">The back-propagation algorithm is based on the chain rule for the derivative of composite functions: if we have <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>y</mi><mo>=</mo><mi>g</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>z</mi><mo>=</mo><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>y</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>g</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>f</mi><mo> ○ </mo><mi>g</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, then<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mfrac>
<mrow>
<mrow><mi>d</mi><mi>f</mi>
</mrow>
</mrow>
<mrow>
<mrow><mi>d</mi><mi>x</mi>
</mrow>
</mrow>
</mfrac><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi>f</mi><mo>'</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>g</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mi>g</mi><mo>'</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> or, abusing notation, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mfrac>
<mrow>
<mrow><mi>d</mi><mi>z</mi>
</mrow>
</mrow>
<mrow>
<mrow><mi>d</mi><mi>x</mi>
</mrow>
</mrow>
</mfrac><mo>=</mo>
<mfrac>
<mrow>
<mrow><mi>d</mi><mi>z</mi>
</mrow>
</mrow>
<mrow>
<mrow><mi>d</mi><mi>y</mi>
</mrow>
</mrow>
</mfrac>
<mfrac>
<mrow>
<mrow><mi>d</mi><mi>y</mi>
</mrow>
</mrow>
<mrow>
<mrow><mi>d</mi><mi>x</mi>
</mrow>
</mrow>
</mfrac><mn>.</mn>
</mrow>
</mrow></math> This is generalized to multivariate funtions as follows: let <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>x</mi><mo> ∈ </mo>
<msup>
<mrow>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
<mrow><mi>m</mi>
</mrow>
</msup><mo>,</mo><mi>y</mi><mo> ∈ </mo>
<msup>
<mrow>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msup><mo>,</mo><mi>g</mi><mo>:</mo>
<msup>
<mrow>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
<mrow><mi>m</mi>
</mrow>
</msup><mo> → </mo>
<msup>
<mrow>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msup>
</mrow>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>f</mi><mo>:</mo>
<msup>
<mrow>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
<mrow><mi>n</mi>
</mrow>
</msup><mo> → </mo>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
</mrow></math>. If <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>z</mi><mo>=</mo><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>y</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>g</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, then<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mfrac>
<mrow>
<mrow><mi> ∂ </mi><mi>z</mi>
</mrow>
</mrow>
<mrow>
<mrow><mi> ∂ </mi>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
</mrow>
</mrow>
</mfrac><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow><mi>j</mi>
</mrow>
</msub>
<mfrac>
<mrow>
<mrow><mi> ∂ </mi><mi>z</mi>
</mrow>
</mrow>
<mrow>
<mrow><mi> ∂ </mi>
<msub>
<mrow><mi>y</mi>
</mrow>
<mrow><mi>j</mi>
</mrow>
</msub>
</mrow>
</mrow>
</mfrac>
<mfrac>
<mrow>
<mrow><mi> ∂ </mi>
<msub>
<mrow><mi>y</mi>
</mrow>
<mrow><mi>j</mi>
</mrow>
</msub>
</mrow>
</mrow>
<mrow>
<mrow><mi> ∂ </mi>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
</mrow>
</mrow>
</mfrac><mo>,</mo>
</mrow>
</mrow></math> or, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi> ∇ </mi>
</mrow>
<mrow><mi>x</mi>
</mrow>
</msub><mi>z</mi><mo>=</mo>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mfrac>
<mrow>
<mrow><mi> ∂ </mi><mi>y</mi>
</mrow>
</mrow>
<mrow>
<mrow><mi> ∂ </mi><mi>x</mi>
</mrow>
</mrow>
</mfrac>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msup>
<msub>
<mrow><mi> ∇ </mi>
</mrow>
<mrow><mi>y</mi>
</mrow>
</msub><mi>z</mi><mo>,</mo>
</mrow>
</mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mfrac>
<mrow>
<mrow><mi> ∂ </mi><mi>y</mi>
</mrow>
</mrow>
<mrow>
<mrow><mi> ∂ </mi><mi>x</mi>
</mrow>
</mrow>
</mfrac>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow></math> is the Jacobian of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>g</mi>
</mrow></math>.</div>
<div class="standard" id="magicparlabel-704">Now, back-propagation is a recursive application of the chain rule, starting from the cost function. The algorithm works as follows:</div>
<ol class="enumerate" id="magicparlabel-705"><li class="enumerate_item"><b>Forward pass</b>: a feedforward network takes as input <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>x</mi>
</mrow></math> and produces the output <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mover>
<mrow><mi>y</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow></math>. The information flows from layer to layer.</li>
<li class="enumerate_item"><b>Cost function</b>: compute the error between expected output and actual output.</li>
<li class="enumerate_item"><b>Back-propagate</b>: evaluate the individual gradient of each parameter and propagates them backwards to update them. For this, we use the concept of <b>local derivative</b>: the derivative of connected nodes are computed locally on the edges of the graph. For non-connected nodes, we multiply the edges connected between the nodes, and we sum over all incoming edges.</li>
</ol>
<div class="standard" id="magicparlabel-708">If we do this in a forward way, summing over all paths becomes intractable pretty quickly, while when doing it in a backwards way, it allows to obtain the derivative of the output with respect to every node directly in one pass. This leads to massive parallelization.</div>
<div class="standard" id="magicparlabel-709">I did a more detailed explanation, with visualizations in my previous notes, <span class="flex_url">https://lorenc1o.github.io/BDMA_Notes/universities/UPC/Machine_Learning_summary.pdf</span>.</div>
<h2 class="section" id="magicparlabel-714"><span class="section_label">4</span> Deep Neural Networks: Optimization and Regularization</h2>
<h3 class="subsection" id="magicparlabel-715"><span class="subsection_label">4.1</span> Optimization</h3>
<div class="standard" id="magicparlabel-716">Recall that neural networks learn by optimizing a cost function, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>J</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>. This optimization is, in practice, performed by gradient descent, so we must be able to compute <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi> ∇ </mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mi>J</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>. The problem is that this is computationally very costly. However, we have seen how back-propagation is an efficient gradient computation technique.</div>
<div class="standard" id="magicparlabel-717">Now, learning is related to what we want to optimize, since we are interested in the performance on the test set, which is not always possible to ensure. Therefore, what is done is minimizing a cost function, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>J</mi>
</mrow></math>, hoping that it will improve the performance in the test set. But this relationship is also what makes learning and optimizing two different things! In pure optimization, our objective is minimizing <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>J</mi>
</mrow></math>, while in learning, the objective is the ability to <b>generalize</b>, or perform well on the test set. Optimizing <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>J</mi>
</mrow></math> on the training set does not ensure a good generalization, and sometimes worse results regarding the pure optimization problem in the training set, can yield better results in the learning problem (think about the overfitting problem).</div>
<div class="standard" id="magicparlabel-718">Therefore, optimization is a crucial part of learning, and gradient-descent is a “cheap” optimization technique. However, it is not exempt of problems:</div>
<ul class="itemize" id="magicparlabel-719"><li class="itemize_item">Local minima and saddle points: small gradients can stop or greatly slow down the method.</li>
<li class="itemize_item">Partial estimation of gradients slows down descent, but can be beneficial for generalization. This refers to computing the gradient in batches, instead of in the full dataset (as we saw).</li>
</ul>
<div class="standard" id="magicparlabel-721">In addition, there are problems that are specific to the kind of functions that arise when working with neural networks:</div>
<ul class="itemize" id="magicparlabel-722"><li class="itemize_item">Bad convergence: due to the existence of many local optima.</li>
<li class="itemize_item">Long training time: the speed of stochastic gradient descent depends on initialization.</li>
<li class="itemize_item">Overfitting: deep neural networks have a lot of free parameters. They can sometimes learn by heart the whole training set, losing the ability to generalize.</li>
<li class="itemize_item">Vanishing gradients: in the backpropagation scheme, the first layers of the network may not receive sufficiently large gradients early in training.</li>
</ul>
<div class="standard" id="magicparlabel-726">There are different techniques to address these problems. Let's see some of them.</div>
<h4 class="subsubsection" id="magicparlabel-727"><span class="subsubsection_label">4.1.1</span> Solving Bad Convergence</h4>
<div class="standard" id="magicparlabel-728">It's important to stabilize and improve the convergence of gradient descent on DNNs, and for this we need good optimization techniques.</div>
<h5 class="paragraph" id="magicparlabel-729"><span class="paragraph_label"></span> Gradient Clipping</h5>
<div class="standard" id="magicparlabel-730">Gradient Clipping proposes to clip the gradient norm to a threshold <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>v</mi>
</mrow></math>, so:</div>
<ol class="enumerate" id="magicparlabel-731"><li class="enumerate_item">Compute the gradient, say <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>g</mi>
</mrow></math>.</li>
<li class="enumerate_item">If <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">∥</mo>
<mi>g</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">∥</mo>
<mo>&gt;</mo><mi>t</mi><mi>h</mi>
</mrow>
</mrow></math>, then <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>g</mi><mo> ← </mo>
<mfrac>
<mrow>
<mrow><mi>g</mi><mo> ⋅ </mo><mi>t</mi><mi>h</mi>
</mrow>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">∥</mo>
<mi>g</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">∥</mo>
</mrow>
</mfrac><mo>,</mo>
</mrow>
</mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>t</mi><mi>h</mi>
</mrow>
</mrow></math> is a threshold to the maximum admissible gradient norm.</li>
</ol>
<div class="standard" id="magicparlabel-733">This way, we keep the direction of the gradient, while preventing <em>overshooting</em>, i.e., taking too large steps. Although this introduces a bias in the optimization process, it works well in practice.</div>
<div class="standard" id="magicparlabel-734">A visualization is the following:</div>
<div class="standard" id="magicparlabel-735" style="text-align: center;"><img alt="image: 5_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___chine-Learning_LectureNotes_source_pegado11.png" src="5_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___chine-Learning_LectureNotes_source_pegado11.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-736">Here, we can observe how if we don't clip, the gradient is too large and we miss the minimum at the right, while clipping enables us to get ther easily.</div>
<h5 class="paragraph" id="magicparlabel-737"><span class="paragraph_label"></span> Gradient with Momentum</h5>
<div class="standard" id="magicparlabel-738">Momentum represents an acceleration method for stochastic gradient descent. The idea is to smooth gradient steps with some momentum or inertia, using previous gradient steps as a “<em>memory</em>” of the direction.</div>
<div class="standard" id="magicparlabel-739">Let's call the gradient at step <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>t</mi>
</mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, then we define the velocity, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi> α </mi>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>-</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>1</mn><mo>-</mo><mi> α </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msup>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mn>0</mn><mo> ≤ </mo><mi> α </mi><mo>&lt;</mo><mn>1</mn>
</mrow>
</mrow></math> is a parameter controlling how much of the gradient we use for the parameter update, usually set as 0.9. If it is set to 0, we obtain the vanilla SGD.</div>
<div class="standard" id="magicparlabel-740">The updates are done as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi> θ </mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msup><mo>=</mo>
<msup>
<mrow><mi> θ </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup><mo>-</mo><mi> η </mi>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> η </mi>
</mrow></math> is the learning rate.</div>
<div class="standard" id="magicparlabel-741">The momentum approach is shown below, observing a slight speedup.</div>
<div class="standard" id="magicparlabel-742" style="text-align: center;"><img alt="image: 6_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___chine-Learning_LectureNotes_source_pegado15.png" src="6_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___chine-Learning_LectureNotes_source_pegado15.png" style="width:60%;"/>
</div>
<h5 class="paragraph" id="magicparlabel-743"><span class="paragraph_label"></span> Nesterov Momentum</h5>
<div class="standard" id="magicparlabel-744">This represents a variant of gradient descent with momentum. It tries to address the problem of the momentum approach, which is that is tends to oscillate around the minimum. Nesterov corrects these oscillations by estimating the gradient after the momentum update as:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi> α </mi>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>-</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>1</mn><mo>-</mo><mi> α </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mi>G</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi> θ </mi><mo>-</mo><mi> α </mi>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-745"><img alt="image: 7_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___chine-Learning_LectureNotes_source_pegado16.png" src="7_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___chine-Learning_LectureNotes_source_pegado16.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-746">We can observe how the oscillations are less prominent. If the function was different and momentum oscillates around the minimum, Nesterov would help with this.</div>
<h5 class="paragraph" id="magicparlabel-747"><span class="paragraph_label"></span> Dynamic Learning Rate</h5>
<div class="standard" id="magicparlabel-748">Stochastic gradient descent estimates the gradient with only one sample, and iterates over the whole dataset, as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> θ </mi><mo> ← </mo><mi> θ </mi><mo>-</mo><mi> η </mi><mi>G</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi> θ </mi><mo>;</mo>
<msup>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msup><mo>,</mo>
<msup>
<mrow><mi>y</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msup>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math> It is common to use minibatches instead:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> θ </mi><mo> ← </mo><mi> θ </mi><mo>-</mo><mi> η </mi><mi>G</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi> θ </mi><mo>;</mo>
<msup>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow>
<msub>
<mrow><mi>i</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>:</mo>
<msub>
<mrow><mi>i</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>+</mo><mi>B</mi>
</mrow>
</mrow>
</msup><mo>,</mo>
<msup>
<mrow><mi>y</mi>
</mrow>
<mrow>
<mrow>
<msub>
<mrow><mi>i</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>:</mo>
<msub>
<mrow><mi>i</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>+</mo><mi>B</mi>
</mrow>
</mrow>
</msup>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi> θ </mi><mo>-</mo><mi> η </mi>
<msub>
<mrow><mi> ∇ </mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub>
<msubsup>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>=</mo>
<msub>
<mrow><mi>i</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
</mrow>
<mrow>
<mrow>
<msub>
<mrow><mi>i</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>+</mo><mi>B</mi>
</mrow>
</mrow>
</msubsup><mi>L</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi> θ </mi><mo>;</mo>
<msup>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msup><mo>,</mo>
<msup>
<mrow><mi>y</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msup>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math> The learning rate is very important, and in fact it should decrease during training, as we get closer to the optimum. Some reasons to make this decrease are:</div>
<ul class="itemize" id="magicparlabel-749"><li class="itemize_item">True gradients becomes small when <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> θ </mi>
</mrow></math> is close to the minimum.</li>
<li class="itemize_item">With SGD, estimating the gradient with samples introduces noise, and these gradients don't necessarily decrease.</li>
<li class="itemize_item">A sufficient condition for convergence of SGD is:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msubsup>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>k</mi><mo>=</mo><mn>1</mn>
</mrow>
</mrow>
<mrow><mi> ∞ </mi>
</mrow>
</msubsup>
<msub>
<mrow><mi> η </mi>
</mrow>
<mrow><mi>k</mi>
</mrow>
</msub><mo>=</mo><mi> ∞ </mi><mo>,</mo><mspace width="10px"></mspace><mi>a</mi><mi>n</mi><mi>d</mi><mspace width="10px"></mspace>
<msubsup>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>k</mi><mo>=</mo><mn>1</mn>
</mrow>
</mrow>
<mrow><mi> ∞ </mi>
</mrow>
</msubsup>
<msubsup>
<mrow><mi> η </mi>
</mrow>
<mrow><mi>k</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msubsup><mo>&lt;</mo><mi> ∞ </mi><mn>.</mn>
</mrow>
</mrow></math></li>
</ul>
<div class="standard" id="magicparlabel-752">In practice, what we do is apply a linear decay until some point <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> τ </mi>
</mrow></math>, and keep it constant after this point:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi> η </mi>
</mrow>
<mrow><mi>k</mi>
</mrow>
</msub><mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>1</mn><mo>-</mo>
<mfrac>
<mrow><mi>k</mi>
</mrow>
<mrow><mi> τ </mi>
</mrow>
</mfrac>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mi> η </mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>+</mo>
<mfrac>
<mrow><mi>k</mi>
</mrow>
<mrow><mi> τ </mi>
</mrow>
</mfrac>
<msub>
<mrow><mi> η </mi>
</mrow>
<mrow><mi> τ </mi>
</mrow>
</msub><mo>,</mo>
</mrow>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi> η </mi>
</mrow>
<mrow><mi> τ </mi>
</mrow>
</msub>
</mrow></math> after <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> τ </mi>
</mrow></math> iterations.</div>
<div class="standard" id="magicparlabel-753">Now, the question is, how to choose <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi> η </mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi> η </mi>
</mrow>
<mrow><mi> τ </mi>
</mrow>
</msub>
</mrow>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> τ </mi>
</mrow></math>? There are several options:</div>
<ul class="itemize" id="magicparlabel-754"><li class="itemize_item">With trail and error (using train/validation sets).</li>
<li class="itemize_item">It's better to monitor the loss function during training.</li>
<li class="itemize_item">A practical choice:

<ul class="itemize" id="magicparlabel-757"><li class="itemize_item">Choose <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> τ </mi>
</mrow></math> so that the whole training dataset is seen around 100 times.</li>
<li class="itemize_item">Choose <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi> η </mi>
</mrow>
<mrow><mi> τ </mi>
</mrow>
</msub>
</mrow></math> around 1% of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi> η </mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow></math>.</li>
<li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi> η </mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow></math> comes with experience:

<ul class="itemize" id="magicparlabel-760"><li class="itemize_item">Too big: big variations in loss</li>
<li class="itemize_item">Too small: learning is slow, we can get stuck in a plateau</li>
</ul>
</li><li class="itemize_item">Recipe:

<ul class="itemize" id="magicparlabel-763"><li class="itemize_item">Try multiple <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi> η </mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow></math> over 100 iterations</li>
<li class="itemize_item">Pick <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi> η </mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow></math> slightly higher than the best</li>
</ul>
</li></ul>
</li></ul>
<h5 class="paragraph" id="magicparlabel-765"><span class="paragraph_label"></span> Cyclical Learning Rate</h5>
<div class="standard" id="magicparlabel-766">Optimization difficulties comes more from plateaus than from bad local optima, and increasing <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> η </mi>
</mrow></math> allows to go across these plateaus. For this, what is done is varying the learning rate in a cyclical manner, from a minimum bound, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi> η </mi>
</mrow>
<mrow>
<mrow><mi>m</mi><mi>i</mi><mi>n</mi>
</mrow>
</mrow>
</msub>
</mrow></math> to a maximum bound, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi> η </mi>
</mrow>
<mrow>
<mrow><mi>m</mi><mi>a</mi><mi>x</mi>
</mrow>
</mrow>
</msub>
</mrow></math>.</div>
<h5 class="paragraph" id="magicparlabel-767"><span class="paragraph_label"></span> Super Convergence with One-Cycle Policy</h5>
<div class="standard" id="magicparlabel-768">Only once cycle works as:</div>
<ul class="itemize" id="magicparlabel-769"><li class="itemize_item">Start with a small learning rate to begin convergence.</li>
<li class="itemize_item">Increases and then stabilizes to high value, to cross big plateaus.</li>
<li class="itemize_item">Decreases to a small value, to optimize local minima.</li>
</ul>
<div class="standard" id="magicparlabel-772" style="text-align: center;"><img alt="image: 8_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___chine-Learning_LectureNotes_source_pegado17.png" src="8_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___chine-Learning_LectureNotes_source_pegado17.png"/>
</div>
<h5 class="paragraph" id="magicparlabel-773"><span class="paragraph_label"></span> SGD with Adaptive Learning Rates</h5>
<div class="standard" id="magicparlabel-774">Preconditioning:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi> θ </mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msup><mo> ← </mo>
<msup>
<mrow><mi> θ </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup><mo>-</mo>
<msub>
<mrow><mi> η </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
<msubsup>
<mrow><mi>P</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
<mrow>
<mrow><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msubsup><mi>G</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msup>
<mrow><mi> θ </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>P</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow></math> can be defined in different ways. Let's see AdaGrad and RMSProp.</div>
<div class="standard" id="magicparlabel-775"><b>AdaGrad</b>: parameters with largest partial derivatives should have a rapid decrease.<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>P</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msubsup>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>j</mi><mo>=</mo><mn>0</mn>
</mrow>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msubsup><mi>G</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msup>
<mrow><mi> θ </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mi>G</mi>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msup>
<mrow><mi> θ </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msup>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
<mrow>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>2</mn>
</mrow>
</mfrac>
</mrow>
</msup><mn>.</mn>
</mrow>
</mrow></math> More precisely:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mtable>
<mtr>
<mtd>
<mrow>
<msup>
<mrow><mi>g</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup><mo> ← </mo><mi>G</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msup>
<mrow><mi> θ </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
<mtd><mrow></mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow>
<msup>
<mrow><mi>r</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup><mo> ← </mo>
<msup>
<mrow><mi>r</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msup><mo>+</mo>
<msup>
<mrow><mi>g</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup><mo> ⊙ </mo>
<msup>
<mrow><mi>g</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup>
</mrow>
</mtd>
<mtd><mrow></mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow>
<msup>
<mrow><mi> θ </mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msup><mo> ← </mo>
<msup>
<mrow><mi> θ </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup><mo>-</mo>
<mfrac>
<mrow><mi> λ </mi>
</mrow>
<mrow>
<mrow><mi> δ </mi><mo>+</mo>
<msqrt>
<msup>
<mrow><mi>r</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup>
</msqrt>
</mrow>
</mrow>
</mfrac><mo> ⊙ </mo>
<msup>
<mrow><mi>g</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup>
</mrow>
</mtd>
<mtd><mrow></mrow>
</mtd>
</mtr>
</mtable>
</mrow></math></div>
<div class="standard" id="magicparlabel-776">AdaGrad converges quickly on convex problems. However, keeping all the history with momentum can be detrimentary, and smoothing the gradient destroys information. </div>
<div class="standard" id="magicparlabel-777">RMSProp: introduces momentum when computing the precondicioner. The idea is to adapt the learning rate to the curvature of the loss function, putting the brakes on when the function is steep and accelerating when the loss function is flat.<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>P</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi> α </mi>
<msub>
<mrow><mi>P</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo>
<msubsup>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>j</mi><mo>=</mo><mn>0</mn>
</mrow>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msubsup><mi>G</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msup>
<mrow><mi> θ </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mi>G</mi>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msup>
<mrow><mi> θ </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msup>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
<mrow>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>2</mn>
</mrow>
</mfrac>
</mrow>
</msup><mn>.</mn>
</mrow>
</mrow></math></div>
<h5 class="paragraph" id="magicparlabel-778"><span class="paragraph_label"></span> More precisely:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
<mtd>
<mrow><mo>=</mo><mi> α </mi>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>1</mn><mo>-</mo><mi> α </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mi>G</mi>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msup>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<msup>
<mrow><mi> θ </mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msup>
</mtd>
<mtd>
<mrow><mo>=</mo>
<msup>
<mrow><mi> θ </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup><mo>-</mo>
<mfrac>
<mrow><mi> η </mi>
</mrow>
<mrow>
<mrow><mi> ε </mi><mo>+</mo>
<msqrt>
<mrow>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msqrt>
</mrow>
</mrow>
</mfrac><mi>G</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mtd>
</mtr>
</mtable></math> Adam</h5>
<div class="standard" id="magicparlabel-779">Adam (Adaptative Moment Estimation) builds on RMSProp, but also uses a moving average of the gradients. It works as:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow>
<msup>
<mrow><mi>m</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
<mtd>
<mrow><mo>=</mo>
<msub>
<mrow><mi> β </mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
<msup>
<mrow><mi>m</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>1</mn><mo>-</mo>
<msub>
<mrow><mi> β </mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mi>G</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
<mtd>
<mrow><mo>=</mo>
<msub>
<mrow><mi> β </mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>1</mn><mo>-</mo>
<msub>
<mrow><mi> β </mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mi>G</mi>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msup>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<msup>
<mrow><mi> θ </mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msup>
</mtd>
<mtd>
<mrow><mo>=</mo>
<msup>
<mrow><mi> θ </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msup><mo>-</mo><mi> η </mi>
<mfrac>
<mrow>
<mrow><mi>m</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
<mrow>
<mrow><mi> ε </mi><mo>+</mo>
<msqrt>
<mrow><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msqrt>
</mrow>
</mrow>
</mfrac><mn>.</mn>
</mrow>
</mtd>
</mtr>
</mtable></math> In practice, Adam is the most used optimizer. However, there are more efficient algorithm, like LARS (Layerwise Adaptive Rate Scaling) or LAMB (LARS+Adam).</div>
<h5 class="paragraph" id="magicparlabel-780"><span class="paragraph_label"></span> Comparison</h5>
<ul class="itemize" id="magicparlabel-781"><li class="itemize_item">SGD momentum should allow for better solution, but hyperparameters are harder to find.</li>
<li class="itemize_item">Adam is easier to tune.</li>
</ul>
<h3 class="subsection" id="magicparlabel-783"><span class="subsection_label">4.2</span> Initialization and Normalization</h3>
<div class="standard" id="magicparlabel-784">The parameters of a deep learning model need initial values. The assignation of initial values is called <b>initialization</b>, and can impact the optimization process in several ways:</div>
<ul class="itemize" id="magicparlabel-785"><li class="itemize_item">A bad initialization can make the training process not to converge.</li>
<li class="itemize_item">It can impact the convergence quality, in terms of speed and the value reached.</li>
<li class="itemize_item">Also, it can impact the generalization error.</li>
</ul>
<div class="standard" id="magicparlabel-788">Therefore, a difficult question arises: initial parameters can help optimization, but can detriment the generalization error.</div>
<div class="standard" id="magicparlabel-789"><div class="flex_color_box"><div class="plain_layout" id="magicparlabel-793"><b>Principle: Break Symmetries</b></div>
<div class="plain_layout" id="magicparlabel-794">Two identical parameters, connected to the same input, should be initialized differently, to incentivize different learning.</div>
</div></div>
<div class="standard" id="magicparlabel-795">One option could be to initialize everything to 0, but this is not a good choice, because it disentivizes learning. Instead, there are different initialization schemes, like random initialization.</div>
<h4 class="subsubsection" id="magicparlabel-796"><span class="subsubsection_label">4.2.1</span> Random Initialization</h4>
<div class="standard" id="magicparlabel-797">Random initialization uses a probability distribution to initialize the weights. For example, <b>Xavier initialization</b> uses an uniform distribution as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>W</mi>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>,</mo><mi>j</mi>
</mrow>
</mrow>
</msub><mo> ∼ </mo><mi>U</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mo>-</mo>
<mfrac>
<mrow>
<msqrt><mn>6</mn>
</msqrt>
</mrow>
<mrow>
<msqrt>
<mrow>
<msub>
<mrow><mi>n</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>n</mi>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow>
</msqrt>
</mrow>
</mfrac><mo>,</mo>
<mfrac>
<mrow>
<msqrt><mn>6</mn>
</msqrt>
</mrow>
<mrow>
<msqrt>
<mrow>
<msub>
<mrow><mi>n</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>n</mi>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow>
</msqrt>
</mrow>
</mfrac>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>n</mi>
</mrow>
<mrow><mi>j</mi>
</mrow>
</msub>
</mrow></math> is the number of neurons in layer <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>...</mn><mo>,</mo><mi>N</mi>
</mrow>
</mrow></math>. The input layer is initialized as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>W</mi>
</mrow>
<mrow>
<mrow><mn>0</mn><mo>,</mo><mi>j</mi>
</mrow>
</mrow>
</msub><mo> ∼ </mo>
<mstyle mathvariant="script"><mi>N</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>0</mn><mo>,</mo>
<msqrt>
<mfrac>
<mrow><mn>2</mn>
</mrow>
<mrow>
<msub>
<mrow><mi>n</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
</mfrac>
</msqrt>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<h4 class="subsubsection" id="magicparlabel-798"><span class="subsubsection_label">4.2.2</span> Input normalization</h4>
<div class="standard" id="magicparlabel-799">Gradient descent is sensitive to strong variations in the input, and, ideally, the surface of the loss function should ahve a uniform curvature in all directions, similar to a sphere. This can be incentivized by input normalization, i.e., normalizing all input parameters so that they all lie in a similar value range.</div>
<div class="standard" id="magicparlabel-800">Moreover, there is the concept of <b>batch normalization</b>, or adaptive reparametrization. This consists in normalizing the input of each layer of a neural network. It is motivated by the following:</div>
<ul class="itemize" id="magicparlabel-801"><li class="itemize_item">Deep neural networks are compositions of functions, whose parameters are iteratively updated during training.</li>
<li class="itemize_item">The updates are done simultaneously to all layers, and unexpected effects can come into play, since each layer is updated assuming all other layers remain constant.</li>
<li class="itemize_item">Therefore, the updates to other layers can add high order effects that can lead to the problem of <b>gradient explosion</b>, a situation in which the gradient keeps growing indefinitely.</li>
</ul>
<div class="standard" id="magicparlabel-804">In this case, the idea is to normalize the distribution of each input feature in each layer, across each minibatch, to a normal, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mstyle mathvariant="script"><mi>N</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>0</mn><mo>,</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd><mi> μ </mi>
</mtd>
<mtd>
<mrow><mo> ← </mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mi>m</mi>
</mrow>
</mfrac>
<msubsup>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>=</mo><mn>1</mn>
</mrow>
</mrow>
<mrow><mi>m</mi>
</mrow>
</msubsup>
<msup>
<mrow>
<mover>
<mrow><mi>x</mi>
</mrow><mo stretchy="true">¯</mo>
</mover>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msup><mo>,</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<msup>
<mrow><mi> σ </mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msup>
</mtd>
<mtd>
<mrow><mo> ← </mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mi>m</mi>
</mrow>
</mfrac>
<msubsup>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>=</mo><mn>1</mn>
</mrow>
</mrow>
<mrow><mi>m</mi>
</mrow>
</msubsup>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msup>
<mrow>
<mover>
<mrow><mi>x</mi>
</mrow><mo stretchy="true">¯</mo>
</mover>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msup><mo>-</mo><mi> μ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msup><mo>,</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<msup>
<mrow>
<mover>
<mrow><mi>x</mi>
</mrow><mo stretchy="true">¯</mo>
</mover>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msup>
</mtd>
<mtd>
<mrow><mo> ← </mo>
<mfrac>
<mrow>
<mrow>
<msup>
<mrow>
<mover>
<mrow><mi>x</mi>
</mrow><mo stretchy="true">¯</mo>
</mover>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msup><mo>-</mo><mi> μ </mi>
</mrow>
</mrow>
<mrow>
<msqrt>
<mrow>
<msup>
<mrow><mi> σ </mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msup><mo>-</mo><mi> ϵ </mi>
</mrow>
</msqrt>
</mrow>
</mfrac><mn>.</mn>
</mrow>
</mtd>
</mtr>
</mtable></math> Remember that <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> ϵ </mi>
</mrow></math> is the approximation of the generalization error, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> ϵ </mi><mo>=</mo>
<mfrac>
<mrow>
<mrow><mi>V</mi>
<msub>
<mrow><mi>C</mi>
</mrow>
<mrow>
<mrow><mi>d</mi><mi>i</mi><mi>m</mi>
</mrow>
</mrow>
</msub>
</mrow>
</mrow>
<mrow><mi>N</mi>
</mrow>
</mfrac>
</mrow>
</mrow></math>.</div>
<h3 class="subsection" id="magicparlabel-805"><span class="subsection_label">4.3</span> Regularization</h3>
<div class="standard" id="magicparlabel-806">The loss function can be small on the training data, but large on the dataset. This is the overfitting problem, that we have seen before. Deep NN can tend to overfit, since more depth implies more free parameters, and therefore a higher VC dimension, which can increase <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> ϵ </mi>
</mrow></math> according the rule of thumb for its approximation. </div>
<div class="standard" id="magicparlabel-807">A way to go around this is to put constraints on the weights, to reduce the VC dimension. This can be done through regularization.</div>
<h4 class="subsubsection" id="magicparlabel-808"><span class="subsubsection_label">4.3.1</span> <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow>
<mstyle mathvariant="script"><mi>L</mi>
</mstyle>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow></math> Regularization (or Ridge)</h4>
<div class="standard" id="magicparlabel-809"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow>
<mstyle mathvariant="script"><mi>L</mi>
</mstyle>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow></math> regularization keeps the <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow>
<mstyle mathvariant="script"><mi>L</mi>
</mstyle>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow></math> norm of the free patameters, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">∥</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">∥</mo>
</mrow></math>, as small as possible, during learning.</div>
<div class="standard" id="magicparlabel-810">The intuition is that each neuron will use all its inputs with small weights, instead of specializing on a small part with high weights.</div>
<div class="standard" id="magicparlabel-811">To accomplish this, we have to minimize two things at the same time: the training loss and a penanlty term representing the norm of the weights:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mstyle mathvariant="script"><mi>J</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow><mi>D</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">∥</mo>
<mrow><mi>t</mi><mo>-</mo><mi>y</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">∥</mo>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msup>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>+</mo><mi> λ </mi>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">∥</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">∥</mo>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msup><mo>,</mo>
</mrow>
</mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> λ </mi>
</mrow></math> is the <b>regularization parameter</b>, controlling the strength of the regularization:</div>
<ul class="itemize" id="magicparlabel-812"><li class="itemize_item">If <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> λ </mi>
</mrow></math> is small, there is only a small regularization, allowing higher weights.</li>
<li class="itemize_item">If <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> λ </mi>
</mrow></math> is high, the weights will be kept very small, but they may not minimize the training loss.</li>
</ul>
<div class="standard" id="magicparlabel-814">The gradient of this new loss function is<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi> ∇ </mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub>
<mstyle mathvariant="script"><mi>J</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo>-</mo><mn>2</mn><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>t</mi><mo>-</mo><mi>y</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mi> ∇ </mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mi>y</mi><mo>+</mo><mn>2</mn><mi> λ </mi><mi> θ </mi><mo>,</mo>
</mrow>
</mrow></math> and so the parameter updates become<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> Δ </mo><mi> θ </mi><mo>=</mo><mi> η </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>t</mi><mo>-</mo><mi>y</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mi> ∇ </mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mi>y</mi><mo>-</mo><mi> η </mi><mi> λ </mi><mi> θ </mi><mn>.</mn>
</mrow>
</mrow></math> The <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow>
<mstyle mathvariant="script"><mi>L</mi>
</mstyle>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow></math> regularization leads to weights decay: even if there is no output error, the weight will converge to 0, forcing the weights to constatly learn, and disincentivizing the specialization on particular examples (overfitting), enhancing generalization.</div>
<h4 class="subsubsection" id="magicparlabel-815"><span class="subsubsection_label">4.3.2</span> <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow>
<mstyle mathvariant="script"><mi>L</mi>
</mstyle>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow></math> regularization (or Lasso)</h4>
<div class="standard" id="magicparlabel-816">In this case, we penalize the absolute value of the weights, instead of their euclidean norm:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mstyle mathvariant="script"><mi>J</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow><mi>D</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">∥</mo>
<mrow><mi>t</mi><mo>-</mo><mi>y</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">∥</mo>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msup>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>+</mo><mi> λ </mi>
<msub>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">∥</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">∥</mo>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mn>.</mn>
</mrow>
</mrow></math> This method leads to very sparse representations, where a lot of neurons may be inactive, and only a few represent the input.</div>
<h4 class="subsubsection" id="magicparlabel-817"><span class="subsubsection_label">4.3.3</span> Early Stopping</h4>
<div class="standard" id="magicparlabel-818">During training, a common behavior is the following:</div>
<div class="standard" id="magicparlabel-819" style="text-align: center;"><img alt="image: 9_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___chine-Learning_LectureNotes_source_pegado18.png" src="9_home_runner_work_BDMA_Notes_BDMA_Notes_Paris-___chine-Learning_LectureNotes_source_pegado18.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-820">It's usual that the training error decreases constantly, while the validation error decreases, and the increases again. If we manage to find the optimal point in this process, we can stop earlier the training process, before the validation error gets larger.</div>
<div class="standard" id="magicparlabel-821">This method is equivalent to <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow>
<mstyle mathvariant="script"><mi>L</mi>
</mstyle>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow></math> normalization, both limiting the capacity of the model:</div>
<ul class="itemize" id="magicparlabel-822"><li class="itemize_item">With Ridge regularization, small slope regions contract the dimension of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> θ </mi>
</mrow></math>, which decays to 0, and high slope regions are not regularized because they help descent.</li>
<li class="itemize_item">With early stopping, parameters with high slope are learned before parameters with low slope.</li>
</ul>
<h4 class="subsubsection" id="magicparlabel-824"><span class="subsubsection_label">4.3.4</span> Dropout</h4>
<div class="standard" id="magicparlabel-825">Dropout considers all the networks can be formed by removing some units from a network. With this in mind, the method consists of:</div>
<ul class="itemize" id="magicparlabel-826"><li class="itemize_item">At each optimization iteration: we apply random binary masks on the units to consider.</li>
</ul>
<div class="standard" id="magicparlabel-827">The probability of dropout, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>p</mi>
</mrow></math>, is a hyperparameter. Therefore, what we do at training time is, at each step, deactivate some neurons, randomly. This incentivizes generalization, since relying on some neurons specializing a lot for some features of the training data is harder if these neurons are not always available. </div>
<div class="standard" id="magicparlabel-828">Note that, at inference time, all neurons are always available.</div>
<h4 class="subsubsection" id="magicparlabel-829"><span class="subsubsection_label">4.3.5</span> Data Augmentation</h4>
<div class="standard" id="magicparlabel-830">The best way to avoid overfitting is collecting more data, but this can be hard, costly, or simply impossible. A simple trick is <b>data augmentation</b>, which consists in creating new varied data from the current data by perturbing it, while not chaning the labels associated to it.</div>
<div class="standard" id="magicparlabel-831">For example:</div>
<div class="standard" id="magicparlabel-832" style="text-align: center;"><img alt="image: 10_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado19.png" src="10_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado19.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-833"><b>Mixup</b> is a particular case of data augmentation, consisting on creating new training new samples with labels by interpolation:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mover>
<mrow><mi>x</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mtd>
<mtd>
<mrow><mo>=</mo><mi> λ </mi>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>+</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>1</mn><mo>-</mo><mi> λ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>j</mi>
</mrow>
</msub><mo>,</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<mover>
<mrow><mi>y</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mtd>
<mtd>
<mrow><mo>=</mo><mi> λ </mi>
<msub>
<mrow><mi>y</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>+</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>1</mn><mo>-</mo><mi> λ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mi>y</mi>
</mrow>
<mrow><mi>j</mi>
</mrow>
</msub><mo>,</mo>
</mrow>
</mtd>
</mtr>
</mtable></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> λ </mi><mo> ∼ </mo><mi>B</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi> α </mi><mo>,</mo><mi> α </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, with <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> α </mi><mo> ∈ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mn>0.1</mn><mo>,</mo><mn>0.4</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
</mrow></math> for classification tasks. This method works for structured data, and can be used to stabilizing GANs (we will see this later).</div>
<h3 class="subsection" id="magicparlabel-834"><span class="subsection_label">4.4</span> Vanishing Gradients</h3>
<div class="standard" id="magicparlabel-835">Contrary to what we could think, adding more layers to a DNN does not necessarily lead to better performance, both on the training and test set. For instance, see the following graph, where we observe the performance of a 20 layers NN (left) and a 56 layers NN (right):</div>
<div class="standard" id="magicparlabel-836" style="text-align: center;"><img alt="image: 11_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado20.png" src="11_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado20.png"/>
</div>
<div class="standard" id="magicparlabel-837">We observe how its performance is worse in all senses. The main reason for this is the <b>vanishing gradient problem</b>. The gradient of the loss function is repeatedly multiplied by a weight matrix <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>W</mi>
</mrow></math> as it travels backwards in a deep NN, as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mfrac>
<mrow>
<mrow><mi> ∂ </mi>
<msub>
<mrow><mi>h</mi>
</mrow>
<mrow><mi>k</mi>
</mrow>
</msub>
</mrow>
</mrow>
<mrow>
<mrow><mi> ∂ </mi>
<msub>
<mrow><mi>h</mi>
</mrow>
<mrow>
<mrow><mi>k</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow>
</mrow>
</mfrac><mo>=</mo><mi>f</mi><mo>'</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msup>
<mrow><mi>W</mi>
</mrow>
<mrow><mi>k</mi>
</mrow>
</msup>
<msub>
<mrow><mi>h</mi>
</mrow>
<mrow>
<mrow><mi>k</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo>
<msup>
<mrow><mi>b</mi>
</mrow>
<mrow><mi>k</mi>
</mrow>
</msup>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msup>
<mrow><mi>W</mi>
</mrow>
<mrow><mi>k</mi>
</mrow>
</msup><mn>.</mn>
</mrow>
</mrow></math> When the gradient arrives to the first layer, the contribution of the weight matrices is comprised between <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msubsup>
<mrow><mi>W</mi>
</mrow>
<mrow>
<mrow><mi>m</mi><mi>i</mi><mi>n</mi>
</mrow>
</mrow>
<mrow><mi>d</mi>
</mrow>
</msubsup><mo>,</mo>
</mrow>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msubsup>
<mrow><mi>W</mi>
</mrow>
<mrow>
<mrow><mi>m</mi><mi>a</mi><mi>x</mi>
</mrow>
</mrow>
<mrow><mi>d</mi>
</mrow>
</msubsup>
</mrow></math>, which are the weight matrix with the highest and lowest norm, and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>d</mi>
</mrow></math> is the depth of the network. We find:</div>
<ul class="itemize" id="magicparlabel-838"><li class="itemize_item">If <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">|</mo>
<msub>
<mrow><mi>W</mi>
</mrow>
<mrow>
<mrow><mi>m</mi><mi>a</mi><mi>x</mi>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">|</mo>
<mo>&lt;</mo><mn>1</mn>
</mrow>
</mrow></math>, then <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msubsup>
<mrow><mi>W</mi>
</mrow>
<mrow>
<mrow><mi>m</mi><mi>a</mi><mi>x</mi>
</mrow>
</mrow>
<mrow><mi>d</mi>
</mrow>
</msubsup>
</mrow></math> is very small for high values of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>d</mi>
</mrow></math>, and the gradient vanishes.</li>
<li class="itemize_item">If <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">|</mo>
<msub>
<mrow><mi>W</mi>
</mrow>
<mrow>
<mrow><mi>m</mi><mi>i</mi><mi>n</mi>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">|</mo>
<mo>&gt;</mo><mn>1</mn>
</mrow>
</mrow></math>, then <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msubsup>
<mrow><mi>W</mi>
</mrow>
<mrow>
<mrow><mi>m</mi><mi>i</mi><mi>n</mi>
</mrow>
</mrow>
<mrow><mi>d</mi>
</mrow>
</msubsup>
</mrow></math> is very high for high values of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>d</mi>
</mrow></math>, and the gradient explodes.</li>
</ul>
<div class="standard" id="magicparlabel-840">We saw that exploding gradients can be solved by gradient clipping. But vanishing gradients are still the current limitation of deep NN. The solutions include the utilization of ReLU activaiton functions, unsupervised pre-training, batch normalization, residual networks, etc.</div>
<h4 class="subsubsection" id="magicparlabel-841"><span class="subsubsection_label">4.4.1</span> Residual network</h4>
<div class="standard" id="magicparlabel-842">A Residual Neural Network, or <b>ResNet</b>, is an advanced type of neural network that is specifically designed to help improve the performance of deep learning models.</div>
<div class="standard" id="magicparlabel-843">ResNet introduces the concept of residual learning. Instead of expecting each stack of layers to directly fit a desired underlying mapping, ResNet layers are designed to fit a residual mapping. The key component of ResNet is the introduction of "skip connections" or "shortcuts" that bypass one or more layers. A skip connection in a ResNet allows the gradient to be directly backpropagated to earlier layers.</div>
<div class="standard" id="magicparlabel-844">These skip connections perform identity mapping, and their outputs are added to the outputs of the stacked layers. This design helps in training deeper networks by mitigating the vanishing gradient problem. With residual blocks, the network learns the additive residual function with respect to the layer inputs, making it easier to optimize and gain accuracy from considerably deeper networks.</div>
<div class="standard" id="magicparlabel-845" style="text-align: center;"><img alt="image: 12_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado21.png" src="12_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado21.png"/>
</div>
<h4 class="subsubsection" id="magicparlabel-846"><span class="subsubsection_label">4.4.2</span> Stochastic Depth</h4>
<div class="standard" id="magicparlabel-847">Stochastic depth is a training technique for deep neural networks, particularly effective for very deep networks like ResNets. It was introduced as a solution to the problem of vanishing gradients and long training times in deep networks.</div>
<div class="standard" id="magicparlabel-848">During training, stochastic depth randomly drops layers in the network. The idea is similar to dropout, where random neurons are turned off during training to prevent overfitting. In stochastic depth, however, it's entire layers that are dropped.</div>
<div class="standard" id="magicparlabel-849">Each training iteration uses a shallower version of the network. The depth of the network varies each time, as different subsets of layers are randomly deactivated.</div>
<div class="standard" id="magicparlabel-850">Notice how stochastic depth is particularly useful for ResNets, where skip connections (or residual connections) are a key feature. When a residual block is dropped, the skip connection effectively takes its place, allowing the signal to still propagate forward.</div>
<div class="standard" id="magicparlabel-851">At test time, all layers are used, but their outputs are scaled appropriately to compensate for the dropout during training. This ensures that the network can benefit from its full depth during inference.</div>
<div class="standard" id="magicparlabel-852" style="text-align: center;"><img alt="image: 13_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado22.png" src="13_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado22.png" style="width:60%;"/>
</div>
<h3 class="subsection" id="magicparlabel-853"><span class="subsection_label">4.5</span> Double Descent</h3>
<div class="standard" id="magicparlabel-854">Double descent is a phenomenon observed in the training of machine learning models, particularly in relation to the model's complexity and its performance on a given task. This concept challenges the traditional understanding of the bias-variance tradeoff and has gained attention in the field of machine learning and statistics.</div>
<div class="standard" id="magicparlabel-855">The double descent curve shows that after the point where the model starts to overfit (as per the traditional U-shaped bias-variance tradeoff curve), increasing the model complexity even further can lead to a decrease in the total error again.</div>
<div class="standard" id="magicparlabel-856">It shows the following phases:</div>
<ul class="itemize" id="magicparlabel-857"><li class="itemize_item"><b>Underparameterized Regime</b>: Where the model has too few parameters and underfits the data. Here, increasing complexity reduces bias and total error. </li>
<li class="itemize_item"><b>Interpolation Threshold</b>: At this point, the model just starts to fit all the training data perfectly (including noise), leading to high variance and total error. </li>
<li class="itemize_item"><b>Overparameterized Regime</b>: Beyond this threshold, as complexity continues to increase, the model enters the second descent where surprisingly, the total error begins to decrease again despite the model being overparameterized.</li>
</ul>
<div class="standard" id="magicparlabel-860">The double descent phenomenon is especially noticeable in scenarios with limited training data. With more data, the peak of the curve (at the interpolation threshold) becomes less pronounced.</div>
<div class="standard" id="magicparlabel-861">Double descent suggests that in some cases, choosing an even more complex model after hitting the overfitting point can improve performance.</div>
<div class="standard" id="magicparlabel-862">For example, observe this phenomenon in the following graphs:</div>
<div class="standard" id="magicparlabel-863" style="text-align: center;"><img alt="image: 14_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado23.png" src="14_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado23.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-864">A similar phenomenon is <b>grokking</b>, which occur when we increase training time. It is remarkable that shallow models don't show it, which is a reason to use deep networks! </div>
<div class="standard" id="magicparlabel-865" style="text-align: center;"><img alt="image: 15_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado24.png" src="15_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado24.png" style="width:60%;"/>
</div>
<h2 class="section" id="magicparlabel-866"><span class="section_label">5</span> Convolutional Neural Networks</h2>
<h3 class="subsection" id="magicparlabel-867"><span class="subsection_label">5.1</span> Introduction</h3>
<div class="standard" id="magicparlabel-868">When working with MLP, we find several limitations:</div>
<ol class="enumerate" id="magicparlabel-869"><li class="enumerate_item">Dealing with data with a high number of features. For example, let imagine we are working with a dataset of images at full HD resolution. The feature size is <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mn>1920</mn><mo> × </mo><mn>1080</mn><mo> × </mo><mn>3</mn><mo>=</mo><mn>6</mn><mspace width="10px"></mspace><mn>220</mn><mspace width="10px"></mspace><mn>800</mn>
</mrow>
</mrow></math>. Learning a single layer to reduce the dimension to 1000 requires around <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mn>6000</mn><mi>K</mi><mo> × </mo><mn>1</mn><mi>K</mi><mo> ∼ </mo><mn>1</mn>
<msup>
<mrow><mn>0</mn>
</mrow>
<mrow><mn>9</mn>
</mrow>
</msup>
</mrow>
</mrow></math> parameters, making the training really difficult.</li>
<li class="enumerate_item">Use the knowledge of the input data modality to shape the network. E.g., for images, the network should be equivariant to translation. This means that a pattern should be detected independently of where it is located in the image.</li>
</ol>
<div class="standard" id="magicparlabel-871"><b>Convolutional Neural Networks (CNNs)</b> appear to mitigate these limitations.</div>
<div class="standard" id="magicparlabel-872"><div class="flex_color_box"><div class="definition" id="magicparlabel-876"><div class="definition_item"><span class="definition_label">Definition 5.1.</span>
A CNN is an NN that uses convolutions in place of general matrix multiplication in, at least, one of their layers.</div>
</div>
</div></div>
<div class="standard" id="magicparlabel-877">CNNs deal with data arranged according to a grid, which can be temporal date, images, videos, etc.</div>
<h4 class="subsubsection" id="magicparlabel-878"><span class="subsubsection_label">5.1.1</span> History of CNNs</h4>
<ul class="itemize" id="magicparlabel-879"><li class="itemize_item">The first work on CNNs was done in 1998, by LeCun et al. </li>
<li class="itemize_item">Theoretical advances to make DNN converge by Hinton et al. in 2006.</li>
<li class="itemize_item">Access to large datasets such as ImageNet thanks to Deng et al. in 2009.</li>
<li class="itemize_item">Advances in hardware technology to scale learning, with better CPUs and the development of GPUs.</li>
<li class="itemize_item">Win of the 2012 ImageNet challenge with AlexNet, using CNNs, by Krizhevsky et al. in 2012, made them famous.</li>
</ul>
<h3 class="subsection" id="magicparlabel-884"><span class="subsection_label">5.2</span> Convolutions</h3>
<div class="standard" id="magicparlabel-885"><div class="flex_color_box"><div class="definition" id="magicparlabel-889"><div class="definition_item"><span class="definition_label">Definition 5.2.</span>
Given two functions, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>f</mi><mo>,</mo><mi>g</mi><mo>:</mo>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle><mo> → </mo>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
</mrow></math>, their <b>convolution</b> is defined as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>f</mi><mo>*</mo><mi>g</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msubsup>
<mrow><mo> ∫ </mo>
</mrow>
<mrow>
<mrow><mo>-</mo><mi> ∞ </mi>
</mrow>
</mrow>
<mrow><mi> ∞ </mi>
</mrow>
</msubsup>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>z</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mi>g</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>-</mo><mi>z</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow><mo> ⁢ </mo>
<mrow><mo> ⅆ </mo><mi>z</mi>
</mrow><mn>.</mn>
</mrow>
</mrow></math> For discrete function, it becomes<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>f</mi><mo>*</mo><mi>g</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>m</mi><mo>,</mo><mi>n</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msubsup>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>=</mo><mo>-</mo><mi> ∞ </mi>
</mrow>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>=</mo><mi> ∞ </mi>
</mrow>
</mrow>
</msubsup><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>i</mi><mo>,</mo><mi>j</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mi>g</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>m</mi><mo>-</mo><mi>i</mi><mo>,</mo><mi>n</mi><mo>-</mo><mi>j</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> where in this case <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>f</mi>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>g</mi>
</mrow></math> are bidimensional.</div>
<div class="definition_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>f</mi>
</mrow></math> is called the <b>input</b> <b>signal</b>.</div>
<div class="definition_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>g</mi>
</mrow></math> is called the <b>kernel</b>, or filter.</div>
<div class="definition_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>f</mi><mo>*</mo><mi>g</mi>
</mrow>
</mrow></math>, the convolution output, is the <b>feature map</b>.</div>
</div>
</div></div>
<div class="remark" id="magicparlabel-893"><div class="remark_item"><span class="remark_label">Remark 5.1.</span>
Properties of convolutions:</div>
<ul class="itemize" id="magicparlabel-894"><li class="itemize_item">Commutativity:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>f</mi><mo>*</mo><mi>g</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>g</mi><mo>*</mo><mi>f</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></li>
<li class="itemize_item">Distributivity:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>f</mi><mo>*</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>g</mi><mo>+</mo><mi>h</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>f</mi><mo>*</mo><mi>g</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>f</mi><mo>*</mo><mi>h</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></li>
<li class="itemize_item">Associativity:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>f</mi><mo>*</mo><mi>g</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>*</mo><mi>h</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>f</mi><mo>*</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>g</mi><mo>*</mo><mi>h</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></li>
</ul>
</div>
<div class="standard" id="magicparlabel-897"><br/>
</div>
<div class="remark" id="magicparlabel-898"><div class="remark_item"><span class="remark_label">Remark 5.2.</span>
Observe that convolutions can be understood as taking a moving average of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>f</mi>
</mrow></math> around each point <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>x</mi>
</mrow></math>, with weights provided by <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>g</mi>
</mrow></math>. If <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mrow><mo> ∫ </mo>
</mrow><mi>g</mi><mo> ⁢ </mo>
<mrow><mo> ⅆ </mo><mi>x</mi>
</mrow><mo>=</mo><mn>1</mn>
</mrow>
</mrow></math>, then it would represent a proper moving average.</div>
</div>
<div class="standard" id="magicparlabel-899">In practice, however, CNNs do not really use convolutions, but <b>cross-correlations</b>, which consist in sliding one signal (or function) over another and measuring the similarity at each position. It's a way to track how much one signal resembles another as you shift one of them over time or space. Their definition is very similar to convolutions, but convolutions invert the kernel function, while cross-correlations do not:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>f</mi><mo> logof </mo><mi>g</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>m</mi><mo>,</mo><mi>n</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msubsup>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>=</mo><mo>-</mo><mi> ∞ </mi>
</mrow>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>=</mo><mi> ∞ </mi>
</mrow>
</mrow>
</msubsup><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>m</mi><mo>,</mo><mi>n</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mi>g</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>m</mi><mo>+</mo><mi>i</mi><mo>,</mo><mi>n</mi><mo>+</mo><mi>i</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-900">While MLPs make an interaction between each input neurons and each output neurons, CNNs have <b>sparse interactions</b>, thanks to the kernels, which have a small number of parameters, significantly reducing the number of parameters of the network.</div>
<div class="standard" id="magicparlabel-901">Moreover, in MLP there is one weight per connection between the input and the output, while CNNs apply the same kernel to different parts of the input, also reducing the number of parameters of the network. This is called the <b>parameter sharing</b> property.</div>
<div class="standard" id="magicparlabel-902"><div class="flex_color_box"><div class="definition" id="magicparlabel-906"><div class="definition_item"><span class="definition_label">Definition 5.3.</span>
A function <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>f</mi>
</mrow></math> is said to be <b>equivariant</b> to a transform <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>g</mi>
</mrow></math>, if for any input <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>x</mi>
</mrow></math>, it holds<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>g</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi>g</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
</div>
</div></div>
<div class="standard" id="magicparlabel-907">Because of paratemer sharing, the CNNs are equivariant to translations. This makes them particulary useful for some modalities, such as image, for which we aim to detect several objects in the same manner.</div>
<div class="standard" id="magicparlabel-908">Images, and some other modalities, like videos, are composed of <b>channels</b>. For example, RGB is a 3-channel way to compose images.</div>
<div class="standard" id="magicparlabel-909">Kernels are convoluted with all channels of the input, as visualized in the following figure.</div>
<div class="float-figure"><div class="plain_layout" id="magicparlabel-914" style="text-align: center;"><img alt="image: 16_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado28.png" src="16_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado28.png" style="width:60%;"/>
</div>
<div class="plain_layout" id="magicparlabel-915"><span class="float-caption-Standard float-caption float-caption-standard">Figure 2:  Convolution of a 3-channel input with a 3-channel kernel.</span></div>
</div>
<div class="standard" id="magicparlabel-920">Convolutions are not equivariant to rotation, zoom, etc. And these might be interesting for our model to be invariant. To make the network invariant to such transformation, we can transform the data with data augmentations including zooming and rotating, and feeding the network with this transformed data.</div>
<div class="standard" id="magicparlabel-921">Notice how convolutions can reduce the input size, whenever the kernel is larger than <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mn>1</mn><mo> × </mo><mn>1</mn>
</mrow>
</mrow></math>. This is due to the necessity to fit the whole image. </div>
<div class="standard" id="magicparlabel-922">The output size of the convolution of an input of size <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>n</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub><mo> × </mo>
<msub>
<mrow><mi>n</mi>
</mrow>
<mrow><mi>w</mi>
</mrow>
</msub>
</mrow>
</mrow></math> with a kernel of size <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>k</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub><mo> × </mo>
<msub>
<mrow><mi>k</mi>
</mrow>
<mrow><mi>w</mi>
</mrow>
</msub>
</mrow>
</mrow></math> is<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>n</mi><mo>-</mo>
<msub>
<mrow><mi>k</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub><mo>+</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> × </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>n</mi>
</mrow>
<mrow><mi>w</mi>
</mrow>
</msub><mo>-</mo>
<msub>
<mrow><mi>k</mi>
</mrow>
<mrow><mi>w</mi>
</mrow>
</msub><mo>+</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<h4 class="subsubsection" id="magicparlabel-923"><span class="subsubsection_label">5.2.1</span> Fixed Kernels Weights</h4>
<div class="standard" id="magicparlabel-924">If we are interested in certain features, and we know how to define a kernel that is good for identifying these features, we can just do this and applying this kernel.</div>
<div class="standard" id="magicparlabel-925">For example, in the following illustration we observe a kernel that is good at finding vertical edges:</div>
<div class="standard" id="magicparlabel-926" style="text-align: center;"><img alt="image: 17_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado29.png" src="17_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado29.png" style="width:60%;"/>
</div>
<h4 class="subsubsection" id="magicparlabel-927"><span class="subsubsection_label">5.2.2</span> Learned Kernel Weights</h4>
<div class="standard" id="magicparlabel-928">Learning a CNN backbone using DL is learning the weights of the kernels via backpropagation. The filters seek to learn the best representation to fit its training objective, via the training loss.</div>
<div class="standard" id="magicparlabel-929">Using different kernels allows to detect various patterns, so usually CNNs contain thousands of kernels and are grouped in hierarchical layers, to learn from low-level features to high-level features.</div>
<div class="standard" id="magicparlabel-930">To compute various features, such as different detections (horizontal, vertical, diagonal, etc.) we need several filters. For an input of size <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>C</mi><mo> × </mo>
<msub>
<mrow><mi>n</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo> × </mo>
<msub>
<mrow><mi>n</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow>
</mrow></math>, where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>C</mi>
</mrow></math> is the amount of channels, and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>F</mi>
</mrow></math> filters of size <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>C</mi><mo> × </mo>
<msub>
<mrow><mi>m</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo> × </mo>
<msub>
<mrow><mi>m</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow>
</mrow></math>, the feature map produced has <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>F</mi>
</mrow></math> channels and is of size<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>F</mi><mo> × </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>n</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>-</mo>
<msub>
<mrow><mi>m</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>+</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> × </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>n</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub><mo>-</mo>
<msub>
<mrow><mi>m</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub><mo>+</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<h4 class="subsubsection" id="magicparlabel-931"><span class="subsubsection_label">5.2.3</span> Receptive fields</h4>
<div class="standard" id="magicparlabel-932">The <b>receptive field</b> is the region of the input space that a particular CNN's feature is looking at.</div>
<h4 class="subsubsection" id="magicparlabel-933"><span class="subsubsection_label">5.2.4</span> <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mn>1</mn><mo> × </mo><mn>1</mn>
</mrow>
</mrow></math> Convolution</h4>
<div class="standard" id="magicparlabel-934">The <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mn>1</mn><mo> × </mo><mn>1</mn>
</mrow>
</mrow></math> convolution is a linear combination of all channels for each pixel, allowing to learn to reduce or increase the number of filters.</div>
<div class="standard" id="magicparlabel-935">In general, if the input is of size <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>F</mi>
</mrow>
<mrow>
<mrow><mi>i</mi><mi>n</mi>
</mrow>
</mrow>
</msub><mo> × </mo><mi>H</mi><mo> × </mo><mi>W</mi>
</mrow>
</mrow></math> and the filter is of size <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>F</mi>
</mrow>
<mrow>
<mrow><mi>o</mi><mi>u</mi><mi>t</mi>
</mrow>
</mrow>
</msub><mo> × </mo><mn>1</mn><mo> × </mo><mn>1</mn>
</mrow>
</mrow></math>, then</div>
<ul class="itemize" id="magicparlabel-936"><li class="itemize_item">If <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>F</mi>
</mrow>
<mrow>
<mrow><mi>o</mi><mi>u</mi><mi>t</mi>
</mrow>
</mrow>
</msub><mo>&lt;</mo>
<msub>
<mrow><mi>F</mi>
</mrow>
<mrow>
<mrow><mi>i</mi><mi>n</mi>
</mrow>
</mrow>
</msub>
</mrow>
</mrow></math>: the dimension is reduced.</li>
<li class="itemize_item">If <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>F</mi>
</mrow>
<mrow>
<mrow><mi>o</mi><mi>u</mi><mi>t</mi>
</mrow>
</mrow>
</msub><mo>&gt;</mo>
<msub>
<mrow><mi>F</mi>
</mrow>
<mrow>
<mrow><mi>i</mi><mi>n</mi>
</mrow>
</mrow>
</msub>
</mrow>
</mrow></math>: the dimension is increased.</li>
</ul>
<div class="standard" id="magicparlabel-938"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mn>1</mn><mo> × </mo><mn>1</mn>
</mrow>
</mrow></math> convolutions can replace fully connected layers. For example, in classification, we could do <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>F</mi>
</mrow>
<mrow>
<mrow><mi>o</mi><mi>u</mi><mi>t</mi>
</mrow>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>N</mi>
</mrow>
<mrow>
<mrow><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi><mi>e</mi><mi>s</mi>
</mrow>
</mrow>
</msub>
</mrow>
</mrow></math>.</div>
<h3 class="subsection" id="magicparlabel-939"><span class="subsection_label">5.3</span> Padding, Stride, Pooling</h3>
<h4 class="subsubsection" id="magicparlabel-940"><span class="subsubsection_label">5.3.1</span> The Sides Problem</h4>
<div class="standard" id="magicparlabel-941">Convolutions are applied on fixed size patches of the input data, and they can only be shifted until there are no pixels. This introduces two potential problems:</div>
<ul class="itemize" id="magicparlabel-942"><li class="itemize_item">The output size is reduced.</li>
<li class="itemize_item">The information in the sides of the image can be lost, because the convolution cannot be applied there.</li>
</ul>
<h4 class="subsubsection" id="magicparlabel-944"><span class="subsubsection_label">5.3.2</span> Padding</h4>
<div class="standard" id="magicparlabel-945">Padding is a solution to the edge problem, consisting on adding data at the sides of the image. There are several strategies, but in general, we just add zeros:</div>
<div class="standard" id="magicparlabel-946" style="text-align: center;"><img alt="image: 18_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado30.png" src="18_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado30.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-947">There are several modes of zero padding. For a padding of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>k</mi>
</mrow></math> pixels:</div>
<ul class="itemize" id="magicparlabel-948"><li class="itemize_item">Valid: no padding.</li>
<li class="itemize_item">Same: add <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mfrac>
<mrow>
<mrow><mi>k</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
<mrow><mn>2</mn>
</mrow>
</mfrac>
</mrow></math> zeros on each side.</li>
<li class="itemize_item">Full: add <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>k</mi>
</mrow></math> zeros on each side.</li>
</ul>
<div class="standard" id="magicparlabel-951">The output size of a convolution of a <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>n</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub><mo> × </mo>
<msub>
<mrow><mi>n</mi>
</mrow>
<mrow><mi>w</mi>
</mrow>
</msub>
</mrow>
</mrow></math> sized input, to which we add <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub>
</mrow></math> rows and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi>w</mi>
</mrow>
</msub>
</mrow></math> columns of padding, and a kernel of size <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>k</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub><mo> × </mo>
<msub>
<mrow><mi>k</mi>
</mrow>
<mrow><mi>w</mi>
</mrow>
</msub>
</mrow>
</mrow></math> becomes<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>n</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub><mo>-</mo>
<msub>
<mrow><mi>k</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub><mo>+</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> × </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>n</mi>
</mrow>
<mrow><mi>w</mi>
</mrow>
</msub><mo>-</mo>
<msub>
<mrow><mi>k</mi>
</mrow>
<mrow><mi>w</mi>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi>w</mi>
</mrow>
</msub><mo>+</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math> Generally, we want to have <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>k</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub><mo>-</mo><mn>1</mn>
</mrow>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi>w</mi>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>k</mi>
</mrow>
<mrow><mi>w</mi>
</mrow>
</msub><mo>-</mo><mn>1</mn>
</mrow>
</mrow></math>, which is achieve by applying same zero padding.</div>
<h4 class="subsubsection" id="magicparlabel-952"><span class="subsubsection_label">5.3.3</span> Stride</h4>
<div class="standard" id="magicparlabel-953">As far as we have seen, we slide the kernel across all locations right and down. However, we could skip some of this locations, effectively reducing the feature map output and the computational cost, but at the expense of the accuracy of the representations.</div>
<div class="standard" id="magicparlabel-954">If we add <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>w</mi>
</mrow>
</msub>
</mrow>
</mrow></math> for the height and width strides, the feature map size is<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mfrac>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>n</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub><mo>-</mo>
<msub>
<mrow><mi>k</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub><mo>+</mo><mn>1</mn><mo>+</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub>
</mrow>
</mfrac><mo> × </mo>
<mfrac>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>n</mi>
</mrow>
<mrow><mi>w</mi>
</mrow>
</msub><mo>-</mo>
<msub>
<mrow><mi>k</mi>
</mrow>
<mrow><mi>w</mi>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi>w</mi>
</mrow>
</msub><mo>+</mo><mn>1</mn><mo>+</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>w</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>w</mi>
</mrow>
</msub>
</mrow>
</mfrac><mn>.</mn>
</mrow>
</mrow></math></div>
<h4 class="subsubsection" id="magicparlabel-955"><span class="subsubsection_label">5.3.4</span> Pooling</h4>
<div class="standard" id="magicparlabel-956">Pooling consists in statistically summarizing a neighbourhood. A pooling layer takes a window of values and output one value. There are several pooling types:</div>
<ul class="itemize" id="magicparlabel-957"><li class="itemize_item">Max Pooling: take the maximum value of a window.</li>
<li class="itemize_item">Average pooling: take the eman value of a window.</li>
<li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>L</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow></math> norm, weighted average pooling, etc.</li>
</ul>
<div class="standard" id="magicparlabel-960">For example, the following illustrates the use of a 2x2 max pooling layer:</div>
<div class="standard" id="magicparlabel-961" style="text-align: center;"><img alt="image: 19_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado31.png" src="19_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado31.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-962">Pooling is useful to make our representations invariant to minor translations. For deep layers, minor modifications can mean great changes in the input image.</div>
<div class="standard" id="magicparlabel-963">Also, it reduces the amount of parameters and computation, and help reducing overfiting.</div>
<div class="standard" id="magicparlabel-964">It sets up a strong prior: invariance to local translation of learned kernels.</div>
<div class="standard" id="magicparlabel-965">It can be degrading for some tasks, where precise locations are required.</div>
<h3 class="subsection" id="magicparlabel-966"><span class="subsection_label">5.4</span> CNNs</h3>
<div class="standard" id="magicparlabel-967"><b>LeNet</b> by LeCun et al. in 1998 was designed after 10 uears of working with handwritten bank checks. It paved the basics of DL:</div>
<ul class="itemize" id="magicparlabel-968"><li class="itemize_item">Feature extraction by convolution layers.</li>
<li class="itemize_item">Classification by MLP layers.</li>
<li class="itemize_item">Layer of convolution followed by average pooling, and non-linearity, with sigmoid or tanh.</li>
<li class="itemize_item">SGD optimizer to perform backpropagation.</li>
</ul>
<div class="standard" id="magicparlabel-972">The architecture is the following:</div>
<div class="standard" id="magicparlabel-973" style="text-align: center;"><img alt="image: 20_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado32.png" src="20_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado32.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-974"><b>AlexNet</b> by Krizhevsky et al. in 2012 was on of the first DNN, and won the 2012 ImageNet challenge, causing a new DL area. They extended LeNet to larger images, with wider convolutions, and used ReLUs and max pooling, combined with dropout.</div>
<div class="standard" id="magicparlabel-975">A very interesting fact they observed was that as the layers were deeper, the receptive field was higher: The first layers learn basic patterns, while deeper layers learn more complicated shapes.</div>
<div class="standard" id="magicparlabel-976" style="text-align: center;"><img alt="image: 21_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado33.png" src="21_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado33.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-977"><b>Visual Geometry Group (VGG)</b> by Simonyan et al. in 2015 was the first network with blocks. the blocks consisted in:</div>
<ul class="itemize" id="magicparlabel-978"><li class="itemize_item">Convolution layer with padding to keep the same resolution.</li>
<li class="itemize_item">Non-linearity.</li>
<li class="itemize_item">Pooling layer to reduce resolution.</li>
</ul>
<div class="standard" id="magicparlabel-981">The architecture was the following:</div>
<div class="standard" id="magicparlabel-982" style="text-align: center;"><img alt="image: 22_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado34.png" src="22_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado34.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-983">The problem was how to maintain enough resolution to have a lot of blocks. The solution was to introduce more convolutions at low resolution, instead of few convolutions at high resolution.</div>
<div class="standard" id="magicparlabel-984">This increases the number of non-linearities and reduces the number of parameter to achieve an equal receptive field.</div>
<div class="standard" id="magicparlabel-985"><b>Network in Network (NiN)</b>, by Lin et al. in 2013: previous architectures improved by enhacing the number of convolutions (width) and deepening the network (depth). This presents two problems:</div>
<ul class="itemize" id="magicparlabel-986"><li class="itemize_item">The last fully connected layers consume large number of parameters.</li>
<li class="itemize_item">It is difficult to add non-linearity without using fully connected layers, that destroy the spatial structure.</li>
</ul>
<div class="standard" id="magicparlabel-988">NiN made improvements:</div>
<ul class="itemize" id="magicparlabel-989"><li class="itemize_item">It used <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mn>1</mn><mo> × </mo><mn>1</mn>
</mrow>
</mrow></math> convolutiones to increase the number of non-linearities.</li>
<li class="itemize_item">Also, a global average pool at the end to remove large fully connected layers.</li>
</ul>
<div class="standard" id="magicparlabel-991"><b>Inception Blocks</b>: Google was facing a problem. They needed fast deployment and inference, but large convolutions with lots of channels increase inference time. Their solution was to reduce the number of channels by applying several convolutions in parallel. Instead of choosing a single filter size for a given layer, Inception blocks apply multiple different-sized filters (e.g., 1x1, 3x3, 5x5 convolutions) in parallel to the same input feature map. This allows the network to capture information at various scales.</div>
<div class="standard" id="magicparlabel-992">An inception block looks as follows:</div>
<div class="standard" id="magicparlabel-993" style="text-align: center;"><img alt="image: 23_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado35.png" src="23_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado35.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-994"><b>GoogleNet</b>, by Szegedy et al. in 2015 was based on inception blocks. It used several classifiers throughout training to enhance discriminative features at first layers, and reduce vanishing gradients.</div>
<div class="standard" id="magicparlabel-995"><b>Residual Blocks</b>: these blocks make use of <b>residual connections</b>, or shortcut connections, which forward the input to the following layer. That is, if a layer is represented as <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, the same layer, when used inside a residual block, would be <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mi>x</mi>
</mrow>
</mrow></math>. This is illustrated below.</div>
<div class="standard" id="magicparlabel-996" style="text-align: center;"><img alt="image: 24_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado36.png" src="24_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado36.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-997">Residual blocks make the identity function easier to learn, and reduce vanishing or exploding gradient problems.</div>
<div class="standard" id="magicparlabel-998"><b>ResNet Blocks</b>: these blocks are a combination of VGG blocks and residual blocks. They use <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mn>3</mn><mo> × </mo><mn>3</mn>
</mrow>
</mrow></math> convolutions, with batch normalization to stabilize training and residual connection concatenated to the result of applying two of these convolutions. The residual connection can be either direct, or by means of a <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mn>1</mn><mo> × </mo><mn>1</mn>
</mrow>
</mrow></math> convolution. This is illustrated below:</div>
<div class="standard" id="magicparlabel-999" style="text-align: center;"><img alt="image: 25_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado37.png" src="25_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado37.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1000"><b>Residual Neural Networks (ResNet)</b>, by He et al. in 2016, is a network architecture consisting of blocks of residual blocks. There are different ResNets depending on how many blocks or layer are used. For example, the following is a ResNet-18, as it has 18 trainable layers (the first 7x7 conv, two 3x3 convs for each internal block, and there are eight of them, and the last fully connected layer):</div>
<div class="standard" id="magicparlabel-1001" style="text-align: center;"><img alt="image: 26_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado38.png" src="26_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado38.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1002">ResNet is a widely used backbone until today.</div>
<div class="standard" id="magicparlabel-1003"><b>ResNexts: Group convolution</b>, by Xie et al. in 2017. To add more non-linearities in ResNets we can add more layers, increase the width of convolutions or the number of channels. However, increasing the number of channels add quadratic complexity <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>F</mi>
</mrow>
<mrow>
<mrow><mi>i</mi><mi>n</mi>
</mrow>
</mrow>
</msub><mo> × </mo>
<msub>
<mrow><mi>F</mi>
</mrow>
<mrow>
<mrow><mi>o</mi><mi>u</mi><mi>t</mi>
</mrow>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow></math>. Group convolutions is a technique to reduce the computational complexity while still increasing the network's width. In group convolution, the input channels are divided into groups, and a convolution is performed independently on each group. This means that if you have <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>g</mi>
</mrow></math> groups, each group will have <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mfrac>
<mrow><mi>c</mi>
</mrow>
<mrow><mi>g</mi>
</mrow>
</mfrac>
</mrow></math> input channels and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mfrac>
<mrow><mi>b</mi>
</mrow>
<mrow><mi>g</mi>
</mrow>
</mfrac>
</mrow></math> filters applied to it, where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>c</mi>
</mrow></math> is the total number of input channels, and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>b</mi>
</mrow></math> is the total number of intermediate channels. Each filter in a group only interacts with the input channels within that group.</div>
<div class="standard" id="magicparlabel-1004">The following diagram shows a block of the ResNeXt architecture. The input channels are first split into <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>g</mi>
</mrow></math> groups. Each group is processed by its own set of 1x1 convolutions (to reduce dimensionality), followed by 3x3 convolutions (to capture spatial hierarchies). The outputs of these groups are then concatenated, and a final 1x1 convolution is applied to the concatenated result to produce the final output channels.</div>
<div class="standard" id="magicparlabel-1005" style="text-align: center;"><img alt="image: 27_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado39.png" src="27_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado39.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1006"><b>DenseNet</b>, by Huang et al. in 2017. This architecture is based on ResNet blocks, but instead of adding the input and the result of the convolution, it concatenates both of them plus the results of each dense blocks on the channel dimension. It makes use of <b>transition layers</b>, which are <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mn>1</mn><mo> × </mo><mn>1</mn>
</mrow>
</mrow></math> convolutions after concatenation, to reduce feature dimension.</div>
<div class="standard" id="magicparlabel-1007">Some other architectures address different problems. MobileNets, by Howard et al. in 2017 was thought to be deployed on mobiles, it separates convolution in two operations; EfficientNet, by Tan et al. in 2019, uses grid search without computational budget to modify the architecture at best width, depth or input resolution... </div>
<div class="standard" id="magicparlabel-1008">CNNs are being slowly replaced in some cases by <b>Visual Transformers</b>.</div>
<h4 class="subsubsection" id="magicparlabel-1009"><span class="subsubsection_label">5.4.1</span> Object Detection Problem</h4>
<div class="standard" id="magicparlabel-1010">The goal of the object detection problem is to find a bounding box that contain an object of interest. It implies two objectives:</div>
<ul class="itemize" id="magicparlabel-1011"><li class="itemize_item">Regression to shape the box.</li>
<li class="itemize_item">Classification to identify the object.</li>
</ul>
<div class="standard" id="magicparlabel-1013">If we consider this problem as a classification problem, one approach is to test various positions and scales to propose various anchor boxes. This is scalable only if the classifier is fast enough. However, CNNs are computationally expensive. A solution is to select only a subset of the boxes, by finding blobs likely to contain objects without class consideration.</div>
<div class="standard" id="magicparlabel-1014"><b>Region proposal: selective search</b>, by Uijlings et al. in 2013</div>
<div class="standard" id="magicparlabel-1015">Selective search is a widely used tool to provide regions of interest in images. The regions are computed following the pipeline:</div>
<ol class="enumerate" id="magicparlabel-1016"><li class="enumerate_item">Ascendant segmentation: The algorithm starts with a fine-grained segmentation of the image, where many small regions are created based on pixel similarity (color, texture, etc.). This is often done using a graph-based segmentation method.</li>
<li class="enumerate_item">Fuse regions at different scales: The segmented regions are then hierarchically merged based on various similarity measures. This process is iterative, with smaller regions combining to form larger ones. This step is crucial as it operates over multiple scales, allowing the algorithm to be more robust in detecting objects of different sizes and aspects.</li>
<li class="enumerate_item">Convert regions to potential boxes of interest: The resulting regions from the previous step are then used to generate bounding boxes, which are the "region proposals." These are the candidate regions that could contain objects and are subsequently used by an object detection model for classification.</li>
</ol>
<div class="standard" id="magicparlabel-1019">This process is depicted below:</div>
<div class="standard" id="magicparlabel-1020" style="text-align: center;"><img alt="image: 28_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado40.png" src="28_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado40.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1021"><b>Intersection over Union (IoU)</b></div>
<div class="standard" id="magicparlabel-1022">For two sets, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>A</mi>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>B</mi>
</mrow></math>, the Jaccard index, or Intersection over Union, is the reatio of the intersection area to their union are<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>J</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>A</mi><mo>,</mo><mi>B</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mfrac>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">|</mo>
<mrow><mi>A</mi><mo> ∩ </mo><mi>B</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">|</mo>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">|</mo>
<mrow><mi>A</mi><mo> ∪ </mo><mi>B</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">|</mo>
</mrow>
</mfrac><mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-1023"><b>Label region proposal</b></div>
<div class="standard" id="magicparlabel-1024">Region proposal anchor boxes are associated to ground-truth bounding boxes given their IoU (above a threshold). The class of an anchor is the same as its associated bounding box. If the anchor box is not associated to a bounding box, it is labeled as background.</div>
<div class="standard" id="magicparlabel-1025">The offsets, relative to the position of central coordinates between anchor box and boundingbox, label the region.</div>
<div class="standard" id="magicparlabel-1026">That is, proposal anchor boxes are labeled with the same label of a ground-truth bounding box if their IoU is above a given threshold.</div>
<div class="standard" id="magicparlabel-1027"><b>R-CNN</b>, by Girshick et al. in 2014</div>
<div class="standard" id="magicparlabel-1028">R-CNN takes region proposal as input. It computes features for each of the regions, and the classification is done using several SVMs. The regression of offsets is done by using a linear regression model.</div>
<div class="standard" id="magicparlabel-1029">To train R-CNNs, there is a supervised pretraining of a CNN backbone, generally on ImageNet, followed by an optional fine-tuning of the backbone, to learn specialised features based on a classification obkective. The backbone is then fixed.</div>
<div class="standard" id="magicparlabel-1030">For each input: crop and wrap proposal regions computed by selective search and compute the features. Train a binary SVM for each class, and apply linear regression to crrect small offsets between the prediction and the actual bounding box.</div>
<div class="standard" id="magicparlabel-1031" style="text-align: center;"><img alt="image: 29_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado41.png" src="29_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado41.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1032"><b>Evaluating detection models</b></div>
<div class="standard" id="magicparlabel-1033">The meaning of true positive, false positive and false negative for the detection problem is not the same as for regular classification problems. The threshold for True and Falso positive/negatives is based on the IoU and a given threshold. For example:</div>
<div class="standard" id="magicparlabel-1034" style="text-align: center;"><img alt="image: 30_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado42.png" src="30_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado42.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1035">Remember, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo>
<mfrac>
<mrow>
<mrow><mi>T</mi><mi>P</mi>
</mrow>
</mrow>
<mrow>
<mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi>
</mrow>
</mrow>
</mfrac>
</mrow>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo>
<mfrac>
<mrow>
<mrow><mi>T</mi><mi>P</mi>
</mrow>
</mrow>
<mrow>
<mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi>
</mrow>
</mrow>
</mfrac>
</mrow>
</mrow></math>.</div>
<div class="standard" id="magicparlabel-1036">The <b>average precision</b> is the area ander the prevision-recall curve.</div>
<div class="standard" id="magicparlabel-1037">The <b>mean-average precision</b> is the mean of average precision for various IoU threholds.</div>
<div class="standard" id="magicparlabel-1038">R-CNNs is slow in testing phase, because there are as many passes in the CNN as the number of region proposals, which is around 2000. Also, SVM and regression are a bit old school, so this architecture is not usable for real world applications.</div>
<div class="standard" id="magicparlabel-1039"><b>Fast R-CNN</b>, by Girshick in 2015</div>
<div class="standard" id="magicparlabel-1040">This modification of R-CNN was proposed to increase its efficiency. It passes thewhole image only once in the CNN backbone. The CNN is trainable, and not fixed, and the proposed regions are associated with computed features from the output feature map.</div>
<div class="standard" id="magicparlabel-1041">Each region of interest can have a different size, so selective search proposed regions are concatenated with features from the CNN to form the <b>Region of Interest (RoI) Pooling Layer</b>, which reshape the features to feed them to fully connected layers. From these features the classes and the offsets are predicted.</div>
<div class="standard" id="magicparlabel-1042">The following table compares R-CNN and Fast R-CNN:</div>
<div class="standard" id="magicparlabel-1043" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1071">R-CNN</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1074">Fast R-CNN</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1077">Test time per image</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1080">47 s</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1083"><b>0.32 s</b></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1086">Speedup</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1089">1x</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1092"><b>146x</b></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1095">Test time per image with selective search</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1098">50 s</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1101"><b>2 s</b></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1104">Speedup</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1107">1x</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1110"><b>25x</b></div>
</td>
</tr></tbody>
</table>
</div>
<div class="standard" id="magicparlabel-1111"><b>Faster R-CNN</b>, by Ren et al. in 2017</div>
<div class="standard" id="magicparlabel-1112">This new modification replaces selective search by a learned region proposal network. The rest is similar to Fast R-CNN.</div>
<div class="standard" id="magicparlabel-1113">The <b>region proposal network (RPN)</b> is a network learned to propose regions of interest. It slides a window on the feature size, and, at each location of the window, it makes a prediction for <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>k</mi>
</mrow></math> anchors (propositions), which are sampled by varying scale and aspect ratio. A small network predicts if there is an object there, and a small network predicts offsets with the bounding box.</div>
<div class="standard" id="magicparlabel-1114">The following table compares the three approaches:</div>
<div class="standard" id="magicparlabel-1115" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1145">R-CNN</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1148">Fast R-CNN</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1151">Faster R-CNN</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1154">Test time per image (with proposals)</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1157">50 s</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1160">2 s</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1163"><b>0.2 s</b></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1166">Speedup</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1169">1x</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1172">25x</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1175"><b>250x</b></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1178">mAP (VOC 2007)</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1181">66.0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1184"><b>66.9</b></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1187"><b>66.9</b></div>
</td>
</tr></tbody>
</table>
</div>
<div class="standard" id="magicparlabel-1188"><b>You Only Look Once (YOLO)</b>, by Redmon et al. in 2016</div>
<div class="standard" id="magicparlabel-1189">This model is a single stage detector: one network is used to predict the bounding boxes and the class probabilities. It is the first real time detector close to Faster R-CNN performance.</div>
<div class="standard" id="magicparlabel-1190">Its architecture consists of 24 convolutional layers and input dimension of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mn>448</mn><mo> × </mo><mn>448</mn>
</mrow>
</mrow></math>. For each input, it cuts it in <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mn>7</mn><mo> × </mo><mn>7</mn>
</mrow>
</mrow></math> grids of cells. Each cell computes the objectness and anchor boxes, considering the object at the center of the cell, and enabling the boxes to reach out the cell.</div>
<div class="standard" id="magicparlabel-1191">This model has been enhanced multiple times, with YoLo v2, v3, v4, v5,...</div>
<h4 class="subsubsection" id="magicparlabel-1192"><span class="subsubsection_label">5.4.2</span> Semantic Segmentation</h4>
<div class="standard" id="magicparlabel-1193">The goal in this case is to find an object class for each pixel. There are several associated problems:</div>
<ul class="itemize" id="magicparlabel-1194"><li class="itemize_item"><b>Semantic segmentation</b>: associate each pixel to a specific class.</li>
<li class="itemize_item"><b>Instance segmentation</b>: associate each pixel to a specific class and identify various instances from the same class.</li>
<li class="itemize_item"><b>Panoptic segmentation</b>: associate each pixel to once class, and prevent overlapping segments.</li>
</ul>
<div class="standard" id="magicparlabel-1197">These are illustrated below:</div>
<div class="standard" id="magicparlabel-1198" style="text-align: center;"><img alt="image: 31_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado43.png" src="31_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado43.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1199"><b>Mask R-CNN</b> by He et al. in 2017</div>
<div class="standard" id="magicparlabel-1200">Basically, it is Faster R-CNN with a third branch, which outputs the object mask. </div>
<div class="standard" id="magicparlabel-1201">It can use various backbones such as ResNet, DenseNet,...</div>
<h3 class="subsection" id="magicparlabel-1202"><span class="subsection_label">5.5</span> Data and Transfer</h3>
<div class="standard" id="magicparlabel-1203">Modern CNNs are made of millions of parameters, requiring a huge amount of data to train them. For example, ImageNet has 1.2M images for 1k classes, and is being replaced by ImageNet21k, that has 1B images.</div>
<div class="standard" id="magicparlabel-1204">Manually annotating such amount of images is costly in terms of money and time, and therefore there is the need for techniques to deal with lacking of data: data augmentation and transfer learning.</div>
<h4 class="subsubsection" id="magicparlabel-1205"><span class="subsubsection_label">5.5.1</span> Data Augmentation</h4>
<div class="standard" id="magicparlabel-1206">We already saw it! It increases the diversity of data by transforming the source data with invariants</div>
<h4 class="subsubsection" id="magicparlabel-1207"><span class="subsubsection_label">5.5.2</span> Transfer Learning</h4>
<div class="standard" id="magicparlabel-1208">Sometimes there is possibility to learn from scratch, by random initialisation, because of lack of data. In this case, we can make use of an already <b>pre-trained</b> backbone from a source task for the target task.</div>
<ul class="itemize" id="magicparlabel-1209"><li class="itemize_item">If the target task is similar to the source task, and the size is comparable, transfer learning works directly; while if the size is inferior, there is a risk of overfitting to the target task after few epochs.
<div class="standard" id="magicparlabel-1210">For small problems, the transfer learning is accomplished by learning a linear classifier with last layers of a pre-trained backbone.</div>
</li><li class="itemize_item">If the target task is very different from the the source task, and the problem is small, a linear classifier from lower level layers can be used; if the problem is large, we can fine-tune the backbone to the new task.</li>
</ul>
<div class="standard" id="magicparlabel-1212">In all cases, starting from initialized weights is better than nothing.</div>
<div class="standard" id="magicparlabel-1213"><b>Fine tuning</b></div>
<div class="standard" id="magicparlabel-1214">Fine tuning consists in training several layers of a pre-trained backbone:</div>
<div class="standard" id="magicparlabel-1215" style="text-align: center;"><img alt="image: 32_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado44.png" src="32_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado44.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1216"><b>Domain Adaptation</b></div>
<div class="standard" id="magicparlabel-1217">Sometimes, the target task and the source task share the same classes, but the target data has a different distribution than the source. This is an entire field in ML: how to adapt our features to the new domain, and have the best possible results.</div>
<h3 class="subsection" id="magicparlabel-1218"><span class="subsection_label">5.6</span> Self-Supervised Learning (SSL)</h3>
<div class="standard" id="magicparlabel-1219">We already know supervised learning. It consists on predicting a target associated to an input. </div>
<div class="standard" id="magicparlabel-1220">Self-Supervised Learning (SSL) is different. It consists on predicting a part of en input, from the input. This means the model learns to understand and generate data by teaching itself.</div>
<div class="standard" id="magicparlabel-1221">A <b>pretext task</b> is a task designed to teach the model something about the structure of the input data without using labels. For example, the pretext task might involve predicting the next word in a sentence, or the color version of a grayscale image. The idea is to construct targets from the data itself and learn from these artificially created labels.</div>
<div class="standard" id="magicparlabel-1222">SSL presents several advantages:</div>
<ul class="itemize" id="magicparlabel-1223"><li class="itemize_item">Reduces the Need for Labeled Data: Labeling is often expensive and time-consuming, and SSL offers a way to learn useful representations without the need for extensive labeled datasets. </li>
<li class="itemize_item">Better Pre-training: SSL can serve as pre-training for neural networks, allowing them to learn general features from the data that can then be refined with a smaller amount of labeled data for a specific task (using transfer learning).</li>
</ul>
<div class="standard" id="magicparlabel-1225">All this is illustrated below:</div>
<div class="standard" id="magicparlabel-1226" style="text-align: center;"><img alt="image: 33_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado45.png" src="33_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado45.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1227">For images, several pretext tasks have been firstly proposed, like rotation, patch localization, colorization, counting,...</div>
<div class="standard" id="magicparlabel-1228">For videos, which add the time dimension, we find the same pretext tasks as in images, plus some others specific to videos, which can be masking frames, shuffling frames,...</div>
<div class="standard" id="magicparlabel-1229"><b>Contrastive Learning</b></div>
<div class="standard" id="magicparlabel-1230">Constrastive Learning is a pretext task for SSL. It aligns positively pairs of images, and push away other images. We find the problem of defining what is a positive pair, which can be solved by using data augmentation.</div>
<div class="standard" id="magicparlabel-1231">The following figure illustrates this concept:</div>
<div class="standard" id="magicparlabel-1232" style="text-align: center;"><img alt="image: 34_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado46.png" src="34_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado46.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1233"><b>SimCLR</b>, by Chen et al. in 2020</div>
<div class="standard" id="magicparlabel-1234">This model defines a siamese pipeline for contrastive learning. Positive pairs are formed from strong data augmentation, like color jittering, gaussian blur, grayscale, etc. Both paris go through an encoder and a projector, which consists on several stacked MLP layers. Then, contrastive loss is applied.</div>
<div class="standard" id="magicparlabel-1235" style="text-align: center;"><img alt="image: 35_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado47.png" src="35_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado47.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1236"><b>SCE</b>, by Denize et al. in 2021</div>
<div class="standard" id="magicparlabel-1237">This model tries to address the problem of how to deal with negatives that share semantic information with the positive pairs. For example, two different cats share more information than two different dogs. Pushing negatives with high semantic similarities makes training unstable, and so SCE predicts positive pairs and estimated relations among instances.</div>
<div class="standard" id="magicparlabel-1238"><b>Deep Cluster</b>, by Caron et al. in 2018</div>
<div class="standard" id="magicparlabel-1239">Clustering associates several input data to a same pseudo-label, without supervision.</div>
<div class="standard" id="magicparlabel-1240">Deep Cluster applies clustering to traing a backbone using several stages:</div>
<ul class="itemize" id="magicparlabel-1241"><li class="itemize_item">Estimate pseudo-labels for each instance with a fixed backbone.</li>
<li class="itemize_item">Train the backbone on this label.</li>
<li class="itemize_item">Repeat the operations until convergence.</li>
</ul>
<h2 class="section" id="magicparlabel-1244"><span class="section_label">6</span> Recurrent Neural Networks</h2>
<h3 class="subsection" id="magicparlabel-1245"><span class="subsection_label">6.1</span> Introduction</h3>
<div class="standard" id="magicparlabel-1246">Recurrent Neural Networks are a family of Neural Network architectures specially designed to deal with sequential data, such as text in NLP, speech signals in speech2text tasks, temporal data in videos, or other temporal signals, like ECGs, time series, etc.</div>
<div class="standard" id="magicparlabel-1247"><b>Sequential data</b> is data with a temporal component, that usually implies correlation in the temporal dimension. More precisely, a <b>sequence</b> consists in a set of vectors <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow></math>, where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>t</mi><mo> ∈ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mn>1</mn><mo>,</mo><mi> τ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
</mrow></math> is a temporal index.</div>
<div class="standard" id="magicparlabel-1248">Note that the temporal index is not necessarily related to time, but to order. For example, text is not temporal, but words are ordered.</div>
<div class="standard" id="magicparlabel-1249"><b>Memory</b> is essential for us to understand and interpret what we perceive. Memory can be understood as a persistent format of information. MLP don't have this persistence, each data point is processed independently of the rest. Therefore, RNNs introduce a way to store information, by adding inner loops that enables them to preserve information at each time-step.</div>
<h3 class="subsection" id="magicparlabel-1250"><span class="subsection_label">6.2</span> Recurrent Neural Networks</h3>
<div class="standard" id="magicparlabel-1251">A Recurrent Neural Network (RNN) is defined with a state, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msup>
<mrow><mi>h</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>t</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup>
</mrow></math>, by recurrence. This state depends on the current input, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msup>
<mrow><mi>x</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>t</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup>
</mrow></math>, and the previous state, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msup>
<mrow><mi>h</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup>
</mrow></math>:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>h</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>t</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup><mo>=</mo><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msup>
<mrow><mi>h</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup><mo>,</mo>
<msup>
<mrow><mi>x</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>t</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup><mo>;</mo><mi> θ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math> The computational graph can be represented with a loop, or unfolded, making it direct and acyclic, as:</div>
<div class="standard" id="magicparlabel-1252" style="text-align: center;"><img alt="image: 36_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado50.png" src="36_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado50.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1253">A RNN is trained to predict the future, given past information, in such a way that <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msup>
<mrow><mi>h</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>t</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup>
</mrow></math> is a summary of the sequence until the timestep <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>t</mi>
</mrow></math>. This summarization implies information compression, or loss of information, and the training must keep relevant information for the task.</div>
<div class="standard" id="magicparlabel-1254">Going back to the definition of the state, we can then write it alternative as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>h</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>t</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup><mo>=</mo><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msup>
<mrow><mi>h</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup><mo>,</mo>
<msup>
<mrow><mi>x</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>t</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup><mo>;</mo><mi> θ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msup>
<mrow><mi>g</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>t</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msup>
<mrow><mi>x</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>t</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup><mo>,</mo>
<msup>
<mrow><mi>x</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup><mo>,</mo><mn>...</mn><mo>,</mo>
<msup>
<mrow><mi>x</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mn>1</mn>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msup>
<mrow><mi>g</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>t</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup>
</mrow></math> takes the whole sequence as input, of variable length.</div>
<div class="standard" id="magicparlabel-1255">On the other hand, using the factorized form, with <b>transition function <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>f</mi>
</mrow></math></b>, the function is the same at each timestep, allowing for parameter sharing and generalization.</div>
<div class="standard" id="magicparlabel-1256"><div class="flex_color_box"><div class="theorem" id="magicparlabel-1260"><div class="theorem_item"><span class="theorem_label">Theorem 6.1.</span>
RNNs are universal approximators.</div>
</div>
</div></div>
<div class="standard" id="magicparlabel-1261">We measure the performance of a RNN through the cost function <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>L</mi>
</mrow></math>, between the output <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>o</mi>
</mrow></math> and the ground truth <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>y</mi>
</mrow></math>. Internatlly, we use softmax:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow>
<mover>
<mrow><mi>y</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>=</mo>
<mfrac>
<mrow>
<msup>
<mrow><mi>e</mi>
</mrow>
<mrow>
<msub>
<mrow><mi>o</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
</mrow>
</msup>
</mrow>
<mrow>
<mrow>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow><mi>j</mi>
</mrow>
</msub>
<msup>
<mrow><mi>e</mi>
</mrow>
<mrow>
<msub>
<mrow><mi>o</mi>
</mrow>
<mrow><mi>j</mi>
</mrow>
</msub>
</mrow>
</msup>
</mrow>
</mrow>
</mfrac><mn>.</mn>
</mrow>
</mrow></math> The parameters of a RNN are:</div>
<ul class="itemize" id="magicparlabel-1262"><li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>U</mi>
</mrow></math>: input to hidden layers.</li>
<li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>W</mi>
</mrow></math>: recurrent connections between hidden layers.</li>
<li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>V</mi>
</mrow></math>: hidden layers to output.</li>
</ul>
<div class="standard" id="magicparlabel-1265">That is:</div>
<div class="standard" id="magicparlabel-1266" style="text-align: center;"><img alt="image: 37_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado51.png" src="37_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado51.png" style="width:60%;"/>
</div>
<h4 class="subsubsection" id="magicparlabel-1267"><span class="subsubsection_label">6.2.1</span> Direct Propagation</h4>
<div class="standard" id="magicparlabel-1268">In the case of discrete variable prediction (like words), <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>o</mi>
</mrow></math> represents log-likelihoods of possible output values, and the normalize probability is <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mover>
<mrow><mi>y</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>o</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>.</div>
<div class="standard" id="magicparlabel-1269">Direct propagation works by:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<msup>
<mrow><mi>a</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>t</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup>
</mtd>
<mtd>
<mrow><mo>=</mo><mi>b</mi><mo>+</mo><mi>W</mi>
<msup>
<mrow><mi>h</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup><mo>+</mo><mi>U</mi>
<msup>
<mrow><mi>x</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>t</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup><mo>,</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<msup>
<mrow><mi>h</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>t</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup>
</mtd>
<mtd>
<mrow><mo>=</mo><mi> φ </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msup>
<mrow><mi>a</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>t</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<msup>
<mrow><mi>o</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>t</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup>
</mtd>
<mtd>
<mrow><mo>=</mo><mi>c</mi><mo>+</mo><mi>V</mi>
<msup>
<mrow><mi>h</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>t</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup><mo>,</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<msup>
<mrow>
<mover>
<mrow><mi>y</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>t</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup>
</mtd>
<mtd>
<mrow><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msup>
<mrow><mi>o</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>t</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mtd>
</mtr>
</mtable></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> φ </mi>
</mrow></math> is the activation function.</div>
<h4 class="subsubsection" id="magicparlabel-1270"><span class="subsubsection_label">6.2.2</span> Recurrent Neural Networks with output recurrence</h4>
<div class="standard" id="magicparlabel-1271">We don't link directly hidden layers recurrently, but rather we link the output to a hidden layer. This has the cons of using less information than <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msup>
<mrow><mi>h</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msup>
</mrow></math>, being therefore a less expressive model, but the pros of being easier to train, enabling parallelization and efficient backpropagation.</div>
<div class="standard" id="magicparlabel-1272" style="text-align: center;"><img alt="image: 38_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado52.png" src="38_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado52.png" style="width:60%;"/>
</div>
<h4 class="subsubsection" id="magicparlabel-1273"><span class="subsubsection_label">6.2.3</span> Recurrent Neural Networks with unique output</h4>
<div class="standard" id="magicparlabel-1274">In this case, it needs the complete input sequence, and the output is a summary of the input. This kind of setting is usually used a simpler module of a more complex architecture.</div>
<div class="standard" id="magicparlabel-1275" style="text-align: center;"><img alt="image: 39_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado53.png" src="39_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado53.png" style="width:60%;"/>
</div>
<h4 class="subsubsection" id="magicparlabel-1276"><span class="subsubsection_label">6.2.4</span> More architectures</h4>
<div class="standard" id="magicparlabel-1277">In this figure we observe different possible setups, which apply to different use cases.</div>
<div class="standard" id="magicparlabel-1278" style="text-align: center;"><img alt="image: 40_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado54.png" src="40_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado54.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1279">For instance:</div>
<ul class="itemize" id="magicparlabel-1280"><li class="itemize_item">Automatic captioning, converting images to text sequences, could be done with a one to many architecture.</li>
<li class="itemize_item">Sentiment analysis, converting a text sequence to a predicted label, could be done with a many to one architecture.</li>
<li class="itemize_item">Machine translation, converting text sequence to text sequence, could be done with a many to many architecture.</li>
<li class="itemize_item">Frame-label video classification, converting a video sequence into a sequence of labels, could also be done with a many to many architecture.</li>
</ul>
<h4 class="subsubsection" id="magicparlabel-1284"><span class="subsubsection_label">6.2.5</span> Bi-Directional Recurrent Neural Network</h4>
<div class="standard" id="magicparlabel-1285">Sometimes, the future of the sequence can also be helpful, for example in NLP, in Character recognition and even in Speech recognition. For this, there exist the Bi-Directional RNN (BRNN), which combine a RNN processing from past to future, with state <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>h</mi>
</mrow></math>, and a RNN processing from future to past, with state <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>g</mi>
</mrow></math>, and the output combines the two of them.</div>
<div class="standard" id="magicparlabel-1286" style="text-align: center;"><img alt="image: 41_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado55.png" src="41_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado55.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1287">Moreover, BRNN can be applied to images with 4 direction, applying them up,down and left,right. This is a bit outdated approach for visual recognition, with architectures as ReNet for classification and ReSeg for segmentation.</div>
<h4 class="subsubsection" id="magicparlabel-1288"><span class="subsubsection_label">6.2.6</span> Deep Recurrent Neural Network</h4>
<div class="standard" id="magicparlabel-1289">A classical RNN has an input layer, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>U</mi>
</mrow></math>, a hidden layer, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>W</mi>
</mrow></math>, and an output layer <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>V</mi>
</mrow></math>. However, we can add hidden layers, allowing to go higher in abstraction.</div>
<div class="standard" id="magicparlabel-1290">The representation improves hierarchically:</div>
<ul class="itemize" id="magicparlabel-1291"><li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>h</mi>
</mrow></math>, the first state, represents temporal dependencies on inputs <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>x</mi>
</mrow></math>.</li>
<li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>z</mi>
</mrow></math>, the second state, represents temporal dependencies on the representations <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>h</mi>
</mrow></math>.</li>
</ul>
<div class="standard" id="magicparlabel-1293" style="text-align: center;"><img alt="image: 42_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado56.png" src="42_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado56.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1294">Also, we can add an MLP before every recurrent layer, increasing the representation capacity of the network, at the expense of a harder training. A possibility to improve these is to use skip connections.</div>
<h3 class="subsection" id="magicparlabel-1295"><span class="subsection_label">6.3</span> Recurrent Neural Networks Training</h3>
<div class="standard" id="magicparlabel-1296">The principle is the same as with MLP and CNNs: we choose a cost function a minimize it during training using gradient descent with backpropagation.</div>
<div class="standard" id="magicparlabel-1297">However, we need to take into account the recurrency of the network, and so we use what is called <b>Backpropagation through time </b>(BPTT), which consists in applying backpropagation to an unrolled graph of the RNN.</div>
<div class="standard" id="magicparlabel-1298">The idea is to forward through the entire sequence to compute the loss, and then go backwards through the entire sequence to compute gradient.</div>
<div class="standard" id="magicparlabel-1299">However, this can be very inefficient, so we also find the <b>truncated BPTT</b>, which does BPTT to chunks of the sequence, instead of the complete sequence. The hidden states are carried forward in time until the end of the sequence, but we only backpropagation for some smaller number of steps.</div>
<div class="standard" id="magicparlabel-1300" style="text-align: center;"><img alt="image: 43_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado57.png" src="43_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado57.png" style="width:50%;"/>
</div>
<h3 class="subsection" id="magicparlabel-1301"><span class="subsection_label">6.4</span> Long-Short Term Memory (LSTM) and Gated Recurrent Units (GRU)</h3>
<div class="standard" id="magicparlabel-1302">Classical RNN present two main limitations:</div>
<ol class="enumerate" id="magicparlabel-1303"><li class="enumerate_item">A computation limitation: they are hard to train, and prone to gradient vanishing and explosion.</li>
<li class="enumerate_item">Long term interactions are hard to model.</li>
</ol>
<div class="standard" id="magicparlabel-1305">Some upgraded variants of RNNs are Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU).</div>
<div class="standard" id="magicparlabel-1306"><b>Gradient vanishing and exploding</b>: the reason RNNs are prone to gradient vanishing/exploding is that the gradient needs to go through the unfolded computational graph. If the largest singular value of the hidden matrix is greater than 1, a recurrent multiplication by it would tend to increase the gradient vector, leading to gradient explosion. The opposite happens when the largest singular value is smaller than 1, leading to gradient vanishing.</div>
<div class="standard" id="magicparlabel-1307">The solution to gradient explosion can be using gradient clipping.</div>
<div class="standard" id="magicparlabel-1308">The solution to gradient vanishing is to change the RNN architecture.</div>
<div class="standard" id="magicparlabel-1309"><b>Long-term dependency problem</b>: RNNs can connect past information to present data, and, in theory, time distance of information is irrelevant. However, in practice, classical RNNs cannot model long sequences (more than 20 points).</div>
<h4 class="subsubsection" id="magicparlabel-1310"><span class="subsubsection_label">6.4.1</span> LSTM</h4>
<div class="standard" id="magicparlabel-1311">LSTM is based on a standard RNN whose neuron activates with <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi>
</mrow>
</mrow></math>.</div>
<div class="standard" id="magicparlabel-1312" style="text-align: center;"><img alt="image: 44_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado58.png" src="44_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado58.png" style="width:50%;"/>
</div>
<div class="standard" id="magicparlabel-1313"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>C</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow></math> is the cell state, which flows through the entire chain and is updated with a sum instead of a product. This avoids memory vanishing and the gradient explosion and vanish. </div>
<div class="standard" id="magicparlabel-1314">Then, there are three gates foverned by sigmoid units, which define the control of in and out information.</div>
<div class="standard" id="magicparlabel-1315">A key component is the forget gate,<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>f</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi> σ </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>W</mi>
</mrow>
<mrow><mi>f</mi>
</mrow>
</msub><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>h</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mi>f</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> which can affect the affect the previous state, letting it through or not (forgetting).</div>
<div class="standard" id="magicparlabel-1316">Then, the input gate layer,<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>i</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi> σ </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>W</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>h</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> which processes the input, contributing to the cell state.</div>
<div class="standard" id="magicparlabel-1317">Then, there is the classical neuron, activated with <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi>
</mrow>
</mrow></math>, which also contributes to the cell state, by<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow>
<mover>
<mrow><mi>C</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>W</mi>
</mrow>
<mrow><mi>C</mi>
</mrow>
</msub><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>h</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mi>C</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-1318">Finally, to update the cell state it is<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>C</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>f</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo> ⋅ </mo>
<msub>
<mrow><mi>C</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>i</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo> ⋅ </mo>
<msub>
<mrow>
<mover>
<mrow><mi>C</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mn>.</mn>
</mrow>
</mrow></math> Notice that the output is computed as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>h</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>C</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mi> σ </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>W</mi>
</mrow>
<mrow><mi>o</mi>
</mrow>
</msub><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>h</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mi>o</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<h4 class="subsubsection" id="magicparlabel-1319"><span class="subsubsection_label">6.4.2</span> GRU</h4>
<div class="standard" id="magicparlabel-1320">GRU is a variant of LSTM, simpler and faster, and with similar performance. It works by applying:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<msub>
<mrow><mi>u</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mrow><mo>=</mo><mi> σ </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>W</mi>
</mrow>
<mrow><mi>u</mi>
</mrow>
</msub><mo> ⋅ </mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>U</mi>
</mrow>
<mrow><mi>u</mi>
</mrow>
</msub><mo> ⋅ </mo>
<msub>
<mrow><mi>h</mi>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mi>u</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mrow><mi>r</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mrow><mo>=</mo><mi> σ </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>W</mi>
</mrow>
<mrow><mi>r</mi>
</mrow>
</msub><mo> ⋅ </mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>U</mi>
</mrow>
<mrow><mi>r</mi>
</mrow>
</msub><mo> ⋅ </mo>
<msub>
<mrow><mi>h</mi>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mi>r</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mrow>
<mover>
<mrow><mi>h</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mrow><mo>=</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>W</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub><mo> ⋅ </mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>r</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo> ○ </mo>
<msub>
<mrow><mi>U</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub><mo> ⋅ </mo>
<msub>
<mrow><mi>h</mi>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mi>h</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mrow><mi>h</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mrow><mo>=</mo>
<msub>
<mrow><mi>u</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo> ○ </mo>
<msub>
<mrow>
<mover>
<mrow><mi>h</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>+</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>1</mn><mo>-</mo>
<msub>
<mrow><mi>u</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ○ </mo>
<msub>
<mrow><mi>h</mi>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mn>.</mn>
</mrow>
</mtd>
</mtr>
</mtable></math></div>
<div class="standard" id="magicparlabel-1321" style="text-align: center;"><img alt="image: 45_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado59.png" src="45_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado59.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1322">Even with these improvements, RNN struggle with long sequences, and remain difficult and slow to train, because of thei autoregressive nature. In 2017, a Google paper introduced a novel architecture that changed everything: the Transformer.</div>
<h2 class="section" id="magicparlabel-1323"><span class="section_label">7</span> Deep Generative Modelling</h2>
<div class="standard" id="magicparlabel-1324">Recall that the objective of supervised learning is to learn a mapping, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>f</mi>
</mrow></math>, between a data sample, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>x</mi>
</mrow></math>, and its label, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>y</mi>
</mrow></math>:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>y</mi><mo>=</mo><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math> This can be formalized by learning the conditional distribution between labels, given a data point, that is, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>y</mi><mo>|</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>.</div>
<div class="standard" id="magicparlabel-1325">For this task, we need to have access the annotated pairs <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>,</mo><mi>y</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow></math>. The applications of supervised learning are classification, object detection, segmentation, etc.</div>
<div class="standard" id="magicparlabel-1326">On the other hand, unsupervised learning tries to learn the underlying hidden data structure, formalized by the unconditional distribution data points, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>. </div>
<div class="standard" id="magicparlabel-1327">In this case, we don't need any annotations to the data. The applications of unsupervised learning are generative modelling, clustering, feature learning, summarization, etc.</div>
<div class="standard" id="magicparlabel-1328"><b>Generative modelling</b> consists in learning a model, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>M</mi>
</mrow></math>, that can approximate the data distribution, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi>M</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ∼ </mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> and should enable sampling easily. That is, a generative model aims to provide:</div>
<ul class="itemize" id="magicparlabel-1329"><li class="itemize_item">Density estimation: it can evaluate the likelihood of a data point belonging to the original distribution.</li>
<li class="itemize_item">Sampling: it can provide likely samples.</li>
</ul>
<h3 class="subsection" id="magicparlabel-1331"><span class="subsection_label">7.1</span> Variational Auto-Encoders (VAE)</h3>
<h4 class="subsubsection" id="magicparlabel-1332"><span class="subsubsection_label">7.1.1</span> Auto-Encoders</h4>
<div class="standard" id="magicparlabel-1333">AEs are a type of unsupervised model used to learn low-dimensional latent representations, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>z</mi>
</mrow></math>, of data samples, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>x</mi>
</mrow></math>. This means that the aim is to somehow try to summarize the input data space, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>X</mi>
</mrow></math>, into a smaller space, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>Z</mi>
</mrow></math>, so <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>d</mi><mi>i</mi><mi>m</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>Z</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>&lt;</mo><mi>d</mi><mi>i</mi><mi>m</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>X</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>. <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>Z</mi>
</mrow></math> should capture relevant characteristics of data, and the choice of its dimensionality is a compromise between interpretability and the amount of information to keep.</div>
<div class="standard" id="magicparlabel-1334">In Figure <a href="#fig_Illustration_of_autoencoders_">3</a> we can see an illustration of autoencoders. The input is processed by the encoder, which is able to embed it into a smaller space, the <b>latent space</b>. The decoder can take the point in the latent space and reconstruct the original image.</div>
<div class="standard" id="magicparlabel-1335">To train them, this process is applied to the training set, and the loss function is the distance between the reconstructed images and the original ones:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mstyle mathvariant="script"><mi>L</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>M</mi><mo>,</mo><mi>X</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>x</mi><mo> ∈ </mo><mi>X</mi>
</mrow>
</mrow>
</msub>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">∥</mo>
<mrow><mi>M</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>-</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">∥</mo>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msup><mn>.</mn>
</mrow>
</mrow></math></div>
<div class="float-figure"><div class="plain_layout" id="magicparlabel-1340" style="text-align: center;"><img alt="image: 46_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___ne-Learning_LectureNotes_source_autoencoder.png" src="46_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___ne-Learning_LectureNotes_source_autoencoder.png" style="width:10%;"/>
</div>
<div class="plain_layout" id="magicparlabel-1341"><span class="float-caption-Standard float-caption float-caption-standard">Figure 3:  <a id="fig_Illustration_of_autoencoders_"></a>
Illustration of autoencoders. Source: <span class="flex_url">https://www.v7labs.com/blog/autoencoders-guide</span>.</span></div>
</div>
<div class="standard" id="magicparlabel-1350">The problem of AEs is that they don't allow generation, because the distribution of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>z</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> is difficult to sample from.</div>
<div class="standard" id="magicparlabel-1351">To see this more clearly, let's discuss an example done by Joseph Rocca, <a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">Understanding VAEs</a>.</div>
<div class="standard" id="magicparlabel-1352">In the following image, we observe how, when using AEs, points in the latent space can have no real meaning, making it difficult to sample and generate new objects, which is the ultimate goal of generative models.</div>
<div class="standard" id="magicparlabel-1353" style="text-align: center;"><img alt="image: 47_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado60.png" src="47_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado60.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1354">What is desirable, is that somehow, points in the latent space that are close to each other, should have a similar real meaning, once they are decoded:</div>
<div class="standard" id="magicparlabel-1355" style="text-align: center;"><img alt="image: 48_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado61.png" src="48_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado61.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1356">This cannot be achieved with AEs, and this led to the introduction of Variational Auto-Encoders (VAEs).</div>
<h4 class="subsubsection" id="magicparlabel-1357"><span class="subsubsection_label">7.1.2</span> Variational Auto-Encodersi</h4>
<div class="standard" id="magicparlabel-1358">In this case, the latent representation is not directly learnt, but rather a <b>latent distribution</b>. In Figure <a href="#fig_Illustration_of_a">4</a>, we observe the illustration of VAEs.</div>
<div class="float-figure"><div class="plain_layout" id="magicparlabel-1363" style="text-align: center;"><img alt="image: 49_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado62.png" src="49_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado62.png"/>
</div>
<div class="plain_layout" id="magicparlabel-1364"><span class="float-caption-Standard float-caption float-caption-standard">Figure 4:  Illustration of a variational auto-encoder.<a id="fig_Illustration_of_a"></a>
. Source: <a href="https://en.wikipedia.org/wiki/Variational_autoencoder">Wikipedia</a>.</span></div>
</div>
<div class="standard" id="magicparlabel-1369">In this case, training is more complex. We want to mazimize the data likelihood. More precisely, its log-likelihood:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mstyle mathvariant="script"><mi>L</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi> θ </mi><mo>,</mo><mi> φ </mi><mo>,</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo> log </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math> Notice that this is not dependent on the distribution of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>z</mi>
</mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi> φ </mi>
</mrow>
</msub>
</mrow></math>, so we can write:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> log </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>z</mi><mo> ∼ </mo>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi> φ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>z</mi><mo>|</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo> log </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>,</mo>
</mrow>
</mrow></math> and applying the reversed Bayes' Rule:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow><mi>z</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo> log </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>=</mo>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow><mi>z</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>l</mi><mi>o</mi><mi>g</mi>
<mfrac>
<mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>|</mo><mi>z</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
<mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>z</mi><mo>|</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</mfrac>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mn>.</mn>
</mrow>
</mrow></math> Here, we can multiply and divide by <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi> φ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>z</mi><mo>|</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> and use the properties of logarithms, to get:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow><mi>z</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo> log </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>=</mo>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow><mi>z</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo> log </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>|</mo><mi>z</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>-</mo>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow><mi>z</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo> log </mo>
<mfrac>
<mrow>
<mrow>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi> φ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>z</mi><mo>|</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
<mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>z</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</mfrac>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>+</mo>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow><mi>z</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo> log </mo>
<mfrac>
<mrow>
<mrow>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi> φ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>z</mi><mo>|</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
<mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>z</mi><mo>|</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</mfrac>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mn>.</mn>
</mrow>
</mrow></math> Now, recalling the definition of the Kullback-Leibler Divergence:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>D</mi>
</mrow>
<mrow>
<mrow><mi>K</mi><mi>L</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>P</mi><mo> ∥ </mo><mi>Q</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow><mi>P</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo> log </mo>
<mfrac>
<mrow>
<mrow><mi>P</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
<mrow>
<mrow><mi>Q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</mfrac>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>,</mo>
</mrow>
</mrow></math> we can rewrite:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mstyle mathvariant="script"><mi>L</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi> θ </mi><mo>,</mo><mi> φ </mi><mo>,</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow><mi>z</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo> log </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>|</mo><mi>z</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>-</mo>
<msub>
<mrow><mi>D</mi>
</mrow>
<mrow>
<mrow><mi>K</mi><mi>L</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi> φ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>z</mi><mo>|</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ∥ </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>z</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo>
<msub>
<mrow><mi>D</mi>
</mrow>
<mrow>
<mrow><mi>K</mi><mi>L</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi> φ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>z</mi><mo>|</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ∥ </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>z</mi><mo>|</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math> Now, the last term is very difficult to compute, because <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>z</mi><mo>|</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> is intractable. Since it is bigger than 0, we can bound <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mstyle mathvariant="script"><mi>L</mi>
</mstyle>
</mrow></math> from below by<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mstyle mathvariant="script"><mi>L</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi> θ </mi><mo>,</mo><mi> φ </mi><mo>,</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ≥ </mo>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow><mi>z</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo> log </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>|</mo><mi>z</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>-</mo>
<msub>
<mrow><mi>D</mi>
</mrow>
<mrow>
<mrow><mi>K</mi><mi>L</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi> φ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>z</mi><mo>|</mo><mi>x</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ∥ </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>z</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> this is called the <b>Evidence Lower Bound</b>. The first term is called the <b>reconstruction term</b>, and can be approximated by sampling and serves to ensure good reconstructions of the output. The second term is the <b>regularisation term</b>, which makes the distribution of the encoder close to the prior.</div>
<div class="standard" id="magicparlabel-1370">The impact of the regularisation is that it enables for <b>continuity</b> in the latent representation, so that similar data samples have similar latent representations; and it helps to ensure <b>completeness</b> of the latent space, so that all latent points have a meaningful decoding.</div>
<div class="standard" id="magicparlabel-1371">Joseph Rocca illustrates this as:</div>
<div class="standard" id="magicparlabel-1372" style="text-align: center;"><img alt="image: 50_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado63.png" src="50_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado63.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1373">In addition to enabling continuity and completeness, or maybe thanks to this, VAEs enable a much easier sampling and generation. We can just sample <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>z</mi>
</mrow></math> from a normal distribution, and process it through the decoder.</div>
<h3 class="subsection" id="magicparlabel-1374"><span class="subsection_label">7.2</span> Generative Adversarial Networks</h3>
<div class="standard" id="magicparlabel-1375">Now, the objective is to sample for a high dimensional and multi-modal distribution, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, by learning a mapping, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>G</mi>
</mrow></math>, between a simple distribution from which we can sample and the high dimensional distribution. For this, Generative Adversarial Networks (GANs), consist of two components:</div>
<ul class="itemize" id="magicparlabel-1376"><li class="itemize_item"><b>Discriminator</b>: tries to distinguish between generated and real data.</li>
<li class="itemize_item"><b>Generator</b>: tries to fool the discriminator by creating likely samples.</li>
</ul>
<div class="standard" id="magicparlabel-1378">This is illustrated below:</div>
<div class="standard" id="magicparlabel-1379" style="text-align: center;"><img alt="image: 51_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado64.png" src="51_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado64.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1380">To train these models, we follow a minimax approach: the discriminator aims at maximizing its success rate, while the generator aims to minimizing the success rate of the discriminator. This can be expressed as:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow>
<mstyle mathvariant="script"><mi>J</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>D</mi><mo>,</mo><mi>G</mi><mo>,</mo><mi>X</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
<mtd>
<mrow><mo>=</mo>
<msub>
<mrow><mo> min </mo>
</mrow>
<mrow><mi>G</mi>
</mrow>
</msub>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi>D</mi>
</mrow>
</msub><mi>V</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>D</mi><mo>,</mo><mi>G</mi><mo>,</mo><mi>X</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mrow></mrow>
</mtd>
<mtd>
<mrow><mo>=</mo>
<msub>
<mrow><mo> min </mo>
</mrow>
<mrow><mi>G</mi>
</mrow>
</msub>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi>D</mi>
</mrow>
</msub>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>x</mi><mo> ∼ </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow>
<mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo> log </mo><mi>D</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>+</mo>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>z</mi><mo> ∼ </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi>z</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>z</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo> log </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>1</mn><mo>-</mo><mi>D</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>G</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>z</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mn>.</mn>
</mrow>
</mtd>
</mtr>
</mtable></math> This can be divided:</div>
<ul class="itemize" id="magicparlabel-1381"><li class="itemize_item">The <b>discriminator loss</b> is<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mstyle mathvariant="script"><mi>J</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>D</mi><mo>,</mo><mi>X</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi>D</mi>
</mrow>
</msub>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>x</mi><mo> ∼ </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow>
<mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo> log </mo><mi>D</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>+</mo>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>z</mi><mo> ∼ </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi>z</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>z</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo> log </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>1</mn><mo>-</mo><mi>D</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>G</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>z</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mn>.</mn>
</mrow>
</mrow></math></li>
<li class="itemize_item">The <b>generator loss</b> is<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mstyle mathvariant="script"><mi>J</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>G</mi><mo>,</mo><mi>X</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mo> min </mo>
</mrow>
<mrow><mi>G</mi>
</mrow>
</msub>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>z</mi><mo> ∼ </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi>z</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>z</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo> log </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>1</mn><mo>-</mo><mi>D</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>G</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>z</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mn>.</mn>
</mrow>
</mrow></math> </li>
</ul>
<div class="standard" id="magicparlabel-1383">Once the training is done, generation is quite straightforward. We only need to sample <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>z</mi><mo> ∼ </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi>z</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>z</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> and process it with the generator module.</div>
<div class="standard" id="magicparlabel-1384">An interesting property of GANs is that they show continuous representations of the latent space, as shown in the figure below:</div>
<div class="float-figure"><div class="plain_layout" id="magicparlabel-1389" style="text-align: center;"><img alt="image: 52_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado65.png" src="52_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado65.png" style="width:60%;"/>
</div>
<div class="plain_layout" id="magicparlabel-1390"><span class="float-caption-Standard float-caption float-caption-standard">Figure 5:  Interpolating in the latent space of GANs. Source: [<a href="#LyXCite-radford2015"><span class="bib-label">2</span></a>].</span></div>
</div>
<div class="standard" id="magicparlabel-1395">In the following table, we compare GANs and VAEs in different aspects:</div>
<div class="standard" id="magicparlabel-1396" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1430">Sample Quality</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1433">Inference</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1436">Mode Coverage</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1439">Training Quality Assesment</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1442">Training Stability</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1445">VAE</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1448">Intermediate</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1451">Good</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1454">Good</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1457">Good</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1460">Good</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1463">GAN</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1466">Very good</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1469">Bad</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1472">Intermediate</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1475">Intermediate</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1478">Intermediate</div>
</td>
</tr></tbody>
</table>
</div>
<h3 class="subsection" id="magicparlabel-1479"><span class="subsection_label">7.3</span> Other Approaches</h3>
<h4 class="subsubsection" id="magicparlabel-1480"><span class="subsubsection_label">7.3.1</span> Normalizing Flows</h4>
<div class="standard" id="magicparlabel-1481">The idea is to transform a simple distribution into acomplex one, by applying a sequence of invertible transformation functions.</div>
<div class="standard" id="magicparlabel-1482">The pros of this approach is that it provides good sample quality, exact inference and stable training. However, it requires invertible neural networks.</div>
<h4 class="subsubsection" id="magicparlabel-1483"><span class="subsubsection_label">7.3.2</span> Pixel RNN</h4>
<div class="standard" id="magicparlabel-1484">This approach consists in generated image pixels starting from the corner. The dependencies between the current pixel and the previous ones is modelled using an RNN. The biggest drawback of this approach is that sequential generation is very slow.</div>
<h2 class="section" id="magicparlabel-1485"><span class="section_label">8</span> Denoising Diffusion Models</h2>
<div class="standard" id="magicparlabel-1486">To train diffusion models, there are two processes involved:</div>
<ul class="itemize" id="magicparlabel-1487"><li class="itemize_item">Forward process: gradually adds noice to the input, until obtaining a completely noisy image.</li>
<li class="itemize_item">Reverse process: learns to generate data by denoising.</li>
</ul>
<div class="standard" id="magicparlabel-1489">This is illustrated in this Figure:</div>
<div class="standard" id="magicparlabel-1490" style="text-align: center;"><img alt="image: 53_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado66.png" src="53_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado66.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1491">The forward process can be seen as a Markov process, with starting state <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow></math> and transition probability <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> for <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>t</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>...</mn><mo>,</mo><mi>T</mi>
</mrow>
</mrow></math>:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mover accent="false"><mo> → </mo>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow></mover>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mover accent="false"><mo> → </mo>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow></mover>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub><mover accent="false"><mo> → </mo>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>3</mn>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow></mover><mn>...</mn><mover accent="false"><mo> → </mo>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>T</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>T</mi><mo>-</mo><mn>2</mn>
</mrow>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow></mover>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>T</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mover accent="false"><mo> → </mo>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>T</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow></mover>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msub>
</mrow>
</mrow></math> The transition probability or <b>Markov kernel</b> is usually chosen to be<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="script"><mi>N</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>;</mo>
<msqrt>
<mrow><mn>1</mn><mo>-</mo>
<msub>
<mrow><mi> β </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
</msqrt>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi> β </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mi>I</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo><mspace width="10px"></mspace>
<msub>
<mrow><mi> β </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo> ∈ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>0</mn><mo>,</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> but it can be any Markov Diffusion Kernel. The property of these kernels is that they converge to a standard distribution. In this case, they converge to a normal distribution, always the same, no matter the input, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow></math>.</div>
<div class="standard" id="magicparlabel-1492">Thus, the forward trajectory can be expressed as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mn>0</mn><mo>;</mo><mi>T</mi>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msubsup>
<mrow><mo> ∏ </mo>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>=</mo><mn>1</mn>
</mrow>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msubsup><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math> And to go to step <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>t</mi>
</mrow></math>, from the step <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow></math>, we just need to sample from the kernel distribution.</div>
<div class="standard" id="magicparlabel-1493">It is also possible to go advance several steps at once. For this, we can define the parameters:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mtable>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi> α </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mn>1</mn><mo>-</mo>
<msub>
<mrow><mi> β </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
</mtd>
<mtd><mrow></mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow>
<msub>
<mrow>
<mover>
<mrow><mi> α </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<msubsup>
<mrow><mo> ∏ </mo>
</mrow>
<mrow>
<mrow><mi>s</mi><mo>=</mo><mn>1</mn>
</mrow>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msubsup>
<msub>
<mrow><mi> α </mi>
</mrow>
<mrow><mi>s</mi>
</mrow>
</msub>
</mrow>
</mtd>
<mtd><mrow></mrow>
</mtd>
</mtr>
</mtable><mo>,</mo>
</mrow>
</mrow></math> and use the Markov kernel<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="script"><mi>N</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>;</mo>
<msqrt>
<msub>
<mrow>
<mover>
<mrow><mi> α </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</msqrt>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>,</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>1</mn><mo>-</mo>
<msub>
<mrow>
<mover>
<mrow><mi> α </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mi>I</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> which is equivalent to do<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<msqrt>
<msub>
<mrow>
<mover>
<mrow><mi> α </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</msqrt>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>+</mo>
<msqrt>
<mrow><mn>1</mn><mo>-</mo>
<msub>
<mrow>
<mover>
<mrow><mi> α </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
</msqrt><mi> ϵ </mi>
</mrow>
</mrow></math> with <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> ϵ </mi><mo> ∼ </mo>
<mstyle mathvariant="script"><mi>N</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>0</mn><mo>,</mo><mi>I</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>. This allows to go to any node in the chain in just one step.</div>
<div class="standard" id="magicparlabel-1494">The parameter <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi> β </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow></math> is the <b>noise scheduler</b>, and it's chosen to have increasing variance, in such a way that <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> is a standard Gaussian. If the <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi> β </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow></math>-s are small enough, the reverse transitions are also Gaussian, but <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> in intractable, but using the Bayes rule we know that<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ∝ </mo><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> and we can use a model to approximate this reverse process.</div>
<div class="standard" id="magicparlabel-1495">That is, we are going to train a model to approximate the reverse, denoising, process. This parametric reverse model:</div>
<ul class="itemize" id="magicparlabel-1496"><li class="itemize_item">Is also a Markov chain, starting at <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msub>
</mrow></math>.</li>
<li class="itemize_item">It starts with <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="script"><mi>N</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msub><mo>;</mo><mn>0</mn><mo>,</mo><mi>I</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, since <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ≈ </mo>
<mstyle mathvariant="script"><mi>N</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>0</mn><mo>,</mo><mi>I</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>.</li>
<li class="itemize_item">Its transitions are learned.</li>
</ul>
<div class="standard" id="magicparlabel-1499">So, how may we achieve this? </div>
<div class="standard" id="magicparlabel-1500">The reverse process is:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mover accent="false"><mo> ← </mo>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow></mover>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mover accent="false"><mo> ← </mo>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow></mover>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub><mover accent="false"><mo> ← </mo>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>3</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow></mover><mn>...</mn><mover accent="false"><mo> ← </mo>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>T</mi><mo>-</mo><mn>2</mn>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>T</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow></mover>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>T</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mover accent="false"><mo> ← </mo>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>T</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow></mover>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msub>
</mrow>
</mrow></math> So the reverse transitions are<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="script"><mi>N</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>;</mo><mo>,</mo>
<msubsup>
<mrow><mi> σ </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msubsup><mi>I</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi> μ </mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo><mi>t</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> will be our trainable network! The reverse trajectory is then<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mn>0</mn><mo>;</mo><mi>T</mi>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msubsup>
<mrow><mo> ∏ </mo>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>=</mo><mn>1</mn>
</mrow>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msubsup>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math> To train this, we follow a maximum likelihood approach:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mstyle mathvariant="script"><mi>J</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>x</mi><mo>,</mo><mi> θ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo>-</mo><mo> log </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> Now, we can marginalize by integrating over the latents:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo>-</mo><mo> log </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo>-</mo><mo> log </mo>
<mrow><mo> ∫ </mo>
</mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mn>0</mn><mo>:</mo><mi>T</mi>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow><mo> ⁢ </mo>
<mrow><mo> ⅆ </mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mn>1</mn><mo>:</mo><mi>T</mi>
</mrow>
</mrow>
</msub>
</mrow><mo>,</mo>
</mrow>
</mrow></math> which is a shortened notation for <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo>-</mo><mo> log </mo>
<mrow><mo> ∫ </mo>
</mrow>
<mrow><mn>...</mn>
<mrow><mo> ∫ </mo>
</mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo><mn>...</mn><mo>,</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow><mo> ⁢ </mo>
<mrow><mo> ⅆ </mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow><mn>...</mn>
</mrow><mo> ⁢ </mo>
<mrow><mo> ⅆ </mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msub>
</mrow>
</mrow>
</mrow></math>. Here, we can multiply and divide by <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mn>1</mn><mo>:</mo><mi>T</mi>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo>-</mo><mo> log </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo>-</mo><mo> log </mo>
<mrow><mo> ∫ </mo>
</mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mn>0</mn><mo>:</mo><mi>T</mi>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mfrac>
<mrow>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mn>1</mn><mo>:</mo><mi>T</mi>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
<mrow>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mn>1</mn><mo>:</mo><mi>T</mi>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</mfrac>
</mrow><mo> ⁢ </mo>
<mrow><mo> ⅆ </mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mn>1</mn><mo>:</mo><mi>T</mi>
</mrow>
</mrow>
</msub>
</mrow><mo>,</mo>
</mrow>
</mrow></math> which can be seen as an expectation:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo>-</mo><mo> log </mo>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo>-</mo><mo> log </mo>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mn>1</mn><mo>:</mo><mi>T</mi>
</mrow>
</mrow>
</msub><mo> ∼ </mo><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mn>1</mn><mo>:</mo><mi>T</mi>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mfrac>
<mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mn>0</mn><mo>:</mo><mi>T</mi>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
<mrow>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mn>1</mn><mo>:</mo><mi>T</mi>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</mfrac>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>,</mo>
</mrow>
</mrow></math> and in this point we can use the Jensen's inequality:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo>-</mo><mo> log </mo>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mn>1</mn><mo>:</mo><mi>T</mi>
</mrow>
</mrow>
</msub><mo> ∼ </mo><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mn>1</mn><mo>:</mo><mi>T</mi>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mfrac>
<mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mn>0</mn><mo>:</mo><mi>T</mi>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
<mrow>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mn>1</mn><mo>:</mo><mi>T</mi>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</mfrac>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo> ≤ </mo>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mn>1</mn><mo>:</mo><mi>T</mi>
</mrow>
</mrow>
</msub><mo> ∼ </mo><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mn>1</mn><mo>:</mo><mi>T</mi>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo>-</mo><mo> log </mo>
<mfrac>
<mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mn>0</mn><mo>:</mo><mi>T</mi>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
<mrow>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mn>1</mn><mo>:</mo><mi>T</mi>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</mfrac>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>=</mo>
<msub>
<mrow><mi>L</mi>
</mrow>
<mrow>
<mrow><mi>V</mi><mi>L</mi><mi>B</mi>
</mrow>
</mrow>
</msub><mn>.</mn>
</mrow>
</mrow></math> We have reached <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>L</mi>
</mrow>
<mrow>
<mrow><mi>V</mi><mi>L</mi><mi>B</mi>
</mrow>
</mrow>
</msub>
</mrow></math>, the <b>variational lower bound</b>. After some derivations, it is possible to arrive to<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow><mi>q</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo>+</mo>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>&gt;</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>-</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>,</mo>
</mrow>
</mrow></math> here <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
</mrow></math> has no learnable parameters, so we can remove it, and we name:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
</mrow></math> and<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mn>.</mn>
</mrow>
</mrow></math>Now, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>D</mi>
</mrow>
<mrow>
<mrow><mi>K</mi><mi>L</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>P</mi><mi> ∥ </mi><mi>Q</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> is always non-negative, and is 0 when <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>P</mi><mo>=</mo><mi>Q</mi>
</mrow>
</mrow></math>. Also, when conditioned on <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow></math>, the reverse process becomes tractable:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="script"><mi>N</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>;</mo>
<msub>
<mrow>
<mover>
<mrow><mi> μ </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
<msub>
<mrow>
<mover>
<mrow><mi> β </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mi>I</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow>
<mover>
<mrow><mi> μ </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mfrac>
<mrow>
<mrow>
<msqrt>
<msub>
<mrow>
<mover>
<mrow><mi> α </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</msqrt>
<msub>
<mrow><mi> β </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
</mrow>
<mrow>
<mrow><mn>1</mn><mo>-</mo>
<msub>
<mrow>
<mover>
<mrow><mi> α </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
</mrow>
</mfrac>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>+</mo>
<mfrac>
<mrow>
<mrow>
<msqrt>
<msub>
<mrow><mi> α </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</msqrt><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>1</mn><mo>-</mo>
<msub>
<mrow>
<mover>
<mrow><mi> α </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
<mrow>
<mrow><mn>1</mn><mo>-</mo>
<msub>
<mrow>
<mover>
<mrow><mi> α </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
</mrow>
</mfrac>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow>
<mover>
<mrow><mi> β </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<mfrac>
<mrow>
<mrow><mn>1</mn><mo>-</mo>
<msub>
<mrow>
<mover>
<mrow><mi> α </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow>
</mrow>
<mrow>
<mrow><mn>1</mn><mo>-</mo>
<msub>
<mrow>
<mover>
<mrow><mi> α </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
</mrow>
</mfrac>
<msub>
<mrow><mi> β </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
</mrow></math>. </div>
<div class="standard" id="magicparlabel-1501">Of course, we don't have <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow></math> available at inference time, so what we want is <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>p</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> to be as close as possible to <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>. For this, we use the normal law and approximate our <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow>
<mover>
<mrow><mi> μ </mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>x</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> in our approximated reverse process.</div>
<div class="standard" id="magicparlabel-1502"><br/>
</div>
<h1 class="part" id="magicparlabel-1503"><span class="part_label">Part II</span> Reinforcement Learning</h1>
<h2 class="section" id="magicparlabel-1504"><span class="section_label">9</span> Introduction</h2>
<div class="standard" id="magicparlabel-1505">People and animals learn by interacting with the environment that sorround them, differing from certain other types of learning. This process is active, rather than passive: the subject needs to perform interactions with the environment, to obtain knowledge. Also, the interactions are usually sequential, with future interactions possibly depending on earlier ones.</div>
<div class="standard" id="magicparlabel-1506">Not only this, but we are goal oriented: we act towards an objective. And, more importantly, we can learn without examples of optimal behavior! Instead, we optimise some reward signal obtained from the outcome of our actions.</div>
<div class="standard" id="magicparlabel-1507">It is in these observation that <b>Reinforcement Learning (RL)</b> arises as a learning paradigm, based on the <b>interaction loop</b>: there is an agent in an environment; the agent can make actions in the environment, and get observations from it.</div>
<div class="standard" id="magicparlabel-1508" style="text-align: center;"><img alt="image: 54_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___arning_LectureNotes_source_interaction_loop.png" src="54_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___arning_LectureNotes_source_interaction_loop.png" style="width:60%;"/>
</div>
<div class="standard" id="magicparlabel-1509">RL relies on the reward hypothesis:</div>
<div class="standard" id="magicparlabel-1510"><div class="flex_color_box"><div class="conjecture" id="magicparlabel-1514"><div class="conjecture_item"><span class="conjecture_label">Conjecture 9.1.</span>
Reward Hypothesis</div>
<div class="conjecture_item">Any goal can be formalized as the outcome of maximizing a cumulative reward.</div>
</div>
</div></div>
<div class="standard" id="magicparlabel-1516">This hypothesis basically says that every objective that an agent can have, can be stated in terms of maximizing a reward associated to the actions of the agent with respect to this objective.</div>
<div class="standard" id="magicparlabel-1517">For example, if the objective of the agent is to fly a helicopter from point A to point B, then the reward could be negatively affected by the distance to point B, by the time taken to reach B,...</div>
<div class="standard" id="magicparlabel-1518">Now, it is important to realize that there exist different reasons to learn:</div>
<ul class="itemize" id="magicparlabel-1519"><li class="itemize_item">Find solutions to problems.</li>
<li class="itemize_item">Adapt online to unforseen circumstances.</li>
</ul>
<div class="standard" id="magicparlabel-1521">Well, RL can provide algorithm for both cases! Note that the second point is not just about generalization, but also to cope with the so-called data shift, efficiently, during operation.</div>
<div class="standard" id="magicparlabel-1522">With all this, now we can define RL:</div>
<div class="standard" id="magicparlabel-1523"><div class="flex_color_box"><div class="definition" id="magicparlabel-1527"><div class="definition_item"><span class="definition_label">Definition 9.1.</span>
Reinforcement Learning is the science and framework of learning to make decisions from interaction.</div>
</div>
</div></div>
<div class="standard" id="magicparlabel-1528">This requires us to think about time, consequences of actions, experience gathering, future prediction, uncertainty,...</div>
<div class="standard" id="magicparlabel-1529">It has a huge potential scope and is a formalisation of the AI problem.</div>
<h2 class="section" id="magicparlabel-1530"><span class="section_label">10</span> Definition and components</h2>
<div class="standard" id="magicparlabel-1531"><div class="flex_color_box"><div class="definition" id="magicparlabel-1535"><div class="definition_item"><span class="definition_label">Definition 10.1.</span>
The <b>environment</b> is the world of the problem at hand, with <b>agents</b> in it that can perform actions, over time.</div>
<div class="definition_item">At each time step <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>t</mi>
</mrow></math>, the agent:</div>
<ul class="itemize" id="magicparlabel-1537"><li class="itemize_item">Receives observation <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>O</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow></math> and reward <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow></math> from the environment.</li>
<li class="itemize_item">Executes action <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow></math>.</li>
</ul>
<div class="definition_item">And the environment:</div>
<ul class="itemize" id="magicparlabel-1540"><li class="itemize_item">Receives action <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow></math>.</li>
<li class="itemize_item">Emits observation <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>O</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow></math> and reward <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow></math>.</li>
</ul>
</div>
</div></div>
<div class="standard" id="magicparlabel-1542">But, what is a reward?</div>
<div class="standard" id="magicparlabel-1543">A <b>reward</b>, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow></math>, is a scalar feedback signal which indicates how well the agent is doing at step <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>t</mi>
</mrow></math>: it defines how well the goal is being accomplished!</div>
<div class="standard" id="magicparlabel-1544">Therefore, the agent's job is to maximize the cummulative reward of the future steps, i.e., <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>2</mn>
</mrow>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>3</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mn>...</mn>
</mrow>
</mrow></math> This is called the <b>return</b>. But, when one thinks about it carefully, one realizes that it is hard to know the future rewards with such precision. Therefore, it is also usual to use the <b>value</b>, which is the expected return, taking into account the current state, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>s</mi>
</mrow></math>:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mn>.</mn>
</mrow>
</mrow></math> This depends on the actions the agents takes, and the goal is to maximize it! To achieve this, the agent must pick suitable actions.</div>
<div class="standard" id="magicparlabel-1545">Therefore, rewards and values define the utility of states and actions, and in this setup there is no supervised feedback.</div>
<div class="standard" id="magicparlabel-1546">Note, also, that this values can be defined recursively as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo>
<msub>
<mrow><mi>G</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
</mrow>
</mrow></math> <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-1547">The <b>environment state</b> is the environment's internal state, which is usually invisible or partially visible to the agent. It is very important, but it can also contain lots of irrelevant information.</div>
<div class="standard" id="magicparlabel-1548">An environment is <b>fully observable</b> when the agent can see the full environment state, so every observation reveals the whole environment state. That is, the agent state could just be the observation:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>O</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-1549">Note that <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow></math> is the agent state, not the environment state!</div>
<h3 class="subsection" id="magicparlabel-1550"><span class="subsection_label">10.1</span> Maximising the value by taking actions</h3>
<div class="standard" id="magicparlabel-1551">As we have outlined, the goal is to select the actions that maximise the value. For this, we may need to take into account that actions may have long term consequences, delaying rewards. Thus, it may be better to sacrifice immediate reward to gain long-term reward.</div>
<div class="standard" id="magicparlabel-1552">The decision making process that for a given state chooses which action to take is called a <b>policy</b>. </div>
<div class="standard" id="magicparlabel-1553">To decide which action to take, we can also condition the value on actions:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>,</mo>
</mrow>
</mrow></math> so, for a given state <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>s</mi>
</mrow></math>, a possible set of actions <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msubsup>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
<mrow><mi>s</mi>
</mrow>
</msubsup>
</mrow></math>, we could decide which action to take as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow>
<mrow><mi>a</mi><mo> ∈ </mo>
<msubsup>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
<mrow><mi>s</mi>
</mrow>
</msubsup>
</mrow>
</mrow>
</msub><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-1554">Then, the <b>history</b> is the full sequence of observation, actions and rewards:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow>
<mstyle mathvariant="script"><mi>H</mi>
</mstyle>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>O</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>O</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo><mn>...</mn><mo>,</mo>
<msub>
<mrow><mi>O</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>O</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mn>.</mn>
</mrow>
</mrow></math></div>
<h3 class="subsection" id="magicparlabel-1555"><span class="subsection_label">10.2</span> Markov Decision Processes</h3>
<div class="standard" id="magicparlabel-1556">Markov Decision Processes (MDPs) are a useful mathematical framework, defined as:</div>
<div class="standard" id="magicparlabel-1557"><div class="flex_color_box"><div class="definition" id="magicparlabel-1561"><div class="definition_item"><span class="definition_label">Definition 10.2.</span>
A decision process is Markov if<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>r</mi><mo>,</mo><mi>s</mi><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>r</mi><mo>,</mo><mi>s</mi><mo>|</mo>
<msub>
<mrow>
<mstyle mathvariant="script"><mi>H</mi>
</mstyle>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
</div>
</div></div>
<div class="standard" id="magicparlabel-1562">This means that the current state is the only information needed to make a decision, we don't need the full story. For example, think in a chess game: there are many ways to arrive to a certain position, but it really does not matter how to got to the position, the past does not affect your choice now.</div>
<div class="standard" id="magicparlabel-1563">In order for a process to be Markov, full observability is required. When the situation is of <b>partial observability</b>, the observations are not Markovian, so using the observation as state is not enough to make the decision. This is called a <b>partially observable Markov decision process (POMDP)</b>. Note that the environment state can still be Markov, but the agent does not know it. In this case, we might be able to construct a Markov agent state.</div>
<div class="standard" id="magicparlabel-1564">In the general case, the agent state is a function of the history:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>=</mo><mi>u</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>O</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>u</mi>
</mrow></math> is a <b>state update function</b>.</div>
<div class="standard" id="magicparlabel-1565">Usually, the agent state is much smaller than the environment state.</div>
<div class="example" id="magicparlabel-1566"><div class="example_item"><span class="example_label">Example 10.1.</span>
A not Markov process:</div>
<div class="example_item">Consider the following maze to be the full environment:</div>
<div class="standard" id="magicparlabel-1568" style="text-align: center;"><img alt="image: 55_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado2.png" src="55_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado2.png"/>
</div>
<div class="example_item">And consider the following observations:</div>
<div class="standard" id="magicparlabel-1570" style="text-align: center;"><img alt="image: 56_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado3.png" src="56_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado3.png"/>
<img alt="image: 57_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado4.png" src="57_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado4.png"/>
</div>
<div class="example_item">They are indistinguishable! This process is not Markov, because only taking into account the current state, we cannot identify where we are.</div>
</div>
<div class="standard" id="magicparlabel-1572">To deal with partial observability, agent can construct suitable state representations. Some examples of agent states are:</div>
<ul class="itemize" id="magicparlabel-1573"><li class="itemize_item">Last observation: <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>O</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
</mrow></math> (might not be enough).</li>
<li class="itemize_item">Complete history: <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow>
<mstyle mathvariant="script"><mi>H</mi>
</mstyle>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
</mrow></math> (might be too large).</li>
<li class="itemize_item">A generic update: <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>u</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>O</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> (but how to design <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>u</mi>
</mrow></math>?)</li>
</ul>
<div class="standard" id="magicparlabel-1576">Constructing a fully Markovian agent state is often not feasible and, more importantly, the state should allow for good policies and value predictions.</div>
<h3 class="subsection" id="magicparlabel-1577"><span class="subsection_label">10.3</span> Policies</h3>
<div class="standard" id="magicparlabel-1578">As we saw, a <b>policy</b> defines the agent's behavior: it is a map from the agent state to an action. Policies can be deterministic,<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>A</mi><mo>=</mo><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>S</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> or stochastic,<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>A</mi><mo>|</mo><mi>S</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>A</mi><mo>|</mo><mi>S</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<h3 class="subsection" id="magicparlabel-1579"><span class="subsection_label">10.4</span> Value Functions</h3>
<div class="standard" id="magicparlabel-1580">We saw the value before, which is the expected return. However, it is usual to introduce a <b>discount factor</b>, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> γ </mi><mo> ∈ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mn>0</mn><mo>,</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
</mrow></math>, which trades off importance of immediate and long-term rewards. This way, the value becomes<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo><mi> π </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>=</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>2</mn>
</mrow>
</mrow>
</msub><mo>+</mo>
<msup>
<mrow><mi> γ </mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msup>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>3</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mn>...</mn><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo><mi> π </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mn>.</mn>
</mrow>
</mrow></math> The value depends on the policy, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> π </mi>
</mrow></math>, and can be used to evaluate the desirability of states, as well as to select between actions.</div>
<div class="standard" id="magicparlabel-1581">Note the role of the discount factor: the higher it is, the higher the focus on long term outcomes.</div>
<div class="standard" id="magicparlabel-1582">Now, using the recursive expression of the return, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi>
<msub>
<mrow><mi>G</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow>
</mrow></math>, we can rewrite the value as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>a</mi><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo> ∼ </mo><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>,</mo>
</mrow>
</mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>A</mi><mo> ∼ </mo><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> means <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>A</mi>
</mrow></math> is chosen by policy <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> π </mi>
</mrow></math> in state <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>s</mi>
</mrow></math>. This is known as a <b>Bellman equation</b>. A similar equation holds for the optimal value, i.e., the highest possible value:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi>a</mi>
</mrow>
</msub>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mn>.</mn>
</mrow>
</mrow></math> Note how this does not depend on a policy, it is just the maximum achievable value from the current state.</div>
<h4 class="subsubsection" id="magicparlabel-1583"><span class="subsubsection_label">10.4.1</span> Value Function Approximations</h4>
<div class="standard" id="magicparlabel-1584">Agents often approximate value functions, and with an accurate value function approximation, the agent can behave optimally, or very well, even in intractably big domains.</div>
<h3 class="subsection" id="magicparlabel-1585"><span class="subsection_label">10.5</span> Model</h3>
<div class="standard" id="magicparlabel-1586">A <b>model</b> predicts what the environment will do next. For example, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mstyle mathvariant="script"><mi>P</mi>
</mstyle>
</mrow></math> predicts the next state, given the current state and an action:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mstyle mathvariant="script"><mi>P</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>s</mi><mo>'</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ≈ </mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>'</mo><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math> Or <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mstyle mathvariant="script"><mi>R</mi>
</mstyle>
</mrow></math> predicts the next immediate reward:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mstyle mathvariant="script"><mi>R</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ≈ </mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mn>.</mn>
</mrow>
</mrow></math> Note that a model does not immediately give us a good policy! We still need to plan and see how actions and states are related. </div>
<div class="example" id="magicparlabel-1587"><div class="example_item"><span class="example_label">Example 10.2.</span>
Consider the following maze, where the rewards are -1 per time-step, the actions are to go N, E, S and W, and the states are the agent's location:</div>
<div class="standard" id="magicparlabel-1588" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
</tr><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-1793">Start</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
</tr><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
</tr><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
</tr><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
</tr><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2080">End</div>
</td>
</tr><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
</tr></tbody>
</table>
</div>
<div class="example_item">The following arrows represent the policy, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, for each state <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>s</mi>
</mrow></math>:</div>
<div class="standard" id="magicparlabel-2128" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
</tr><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2281"><img alt="image: 58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" src="58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2288"><img alt="image: 58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" src="58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2295"><img alt="image: 58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" src="58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2302"><img alt="image: 58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" src="58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2309"><img alt="image: 58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" src="58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2316"><img alt="image: 59_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado9.png" src="59_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado9.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2333">Start</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2344"><img alt="image: 58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" src="58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2351"><img alt="image: 60_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado6.png" src="60_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado6.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2364"><img alt="image: 60_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado6.png" src="60_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado6.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2374"><img alt="image: 59_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado9.png" src="59_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado9.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
</tr><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2405"><img alt="image: 60_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado6.png" src="60_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado6.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2412"><img alt="image: 61_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado8.png" src="61_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado8.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2425"><img alt="image: 59_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado9.png" src="59_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado9.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2432"><img alt="image: 61_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado8.png" src="61_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado8.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
</tr><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2466"><img alt="image: 60_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado6.png" src="60_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado6.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2473"><img alt="image: 61_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado8.png" src="61_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado8.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2483"><img alt="image: 59_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado9.png" src="59_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado9.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
</tr><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2517"><img alt="image: 59_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado9.png" src="59_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado9.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2527"><img alt="image: 60_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado6.png" src="60_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado6.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2537"><img alt="image: 58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" src="58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2544"><img alt="image: 59_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado9.png" src="59_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado9.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
</tr><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2575"><img alt="image: 58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" src="58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2582"><img alt="image: 58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" src="58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2589"><img alt="image: 60_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado6.png" src="60_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado6.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2596"><img alt="image: 61_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado8.png" src="61_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado8.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2606"><img alt="image: 58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" src="58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2613"><img alt="image: 58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" src="58_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___achine-Learning_LectureNotes_source_pegado5.png" style="width:30%;"/>
</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2620">End</div>
</td>
</tr><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
</tr></tbody>
</table>
</div>
<div class="example_item">In the following one, the numbers represent the value <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> of each state <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>s</mi>
</mrow></math>:</div>
</div>
<div class="standard" id="magicparlabel-2668" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
</tr><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2821">-14</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2828">-13</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2835">-12</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2842">-11</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2849">-10</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2856">-9</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2873">Start</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2884">-16</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2891">-15</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2904">-12</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2914">-8</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
</tr><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2945">-16</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2952">-17</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2965">-6</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-2972">-7</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
</tr><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3006">-18</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3013">-19</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3023">-5</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
</tr><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3057">-24</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3067">-20</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3077">-4</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3084">-3</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
</tr><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3115">-23</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3122">-22</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3129">-21</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3136">-22</div>
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3146">-2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3153">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3160">End</div>
</td>
</tr><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
<td align="center" valign="top">
</td>
</tr></tbody>
</table>
</div>
<div class="standard" id="magicparlabel-3207">The grid layout represents the partial transition model <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msubsup>
<mrow>
<mstyle mathvariant="script"><mi>P</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>s</mi><mi>s</mi><mo>'</mo>
</mrow>
</mrow>
<mrow><mi>a</mi>
</mrow>
</msubsup>
</mrow></math>, and numbers represent the immediate reward, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msubsup>
<mrow>
<mstyle mathvariant="script"><mi>R</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>s</mi><mi>s</mi><mo>'</mo>
</mrow>
</mrow>
<mrow><mi>a</mi>
</mrow>
</msubsup>
</mrow></math> from each state <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>s</mi>
</mrow></math>, which is -1 for all <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>a</mi>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>s</mi><mo>'</mo>
</mrow>
</mrow></math> in this case.</div>
<h3 class="subsection" id="magicparlabel-3208"><span class="subsection_label">10.6</span> Agent categories</h3>
<div class="standard" id="magicparlabel-3209">An agent is <b>model free</b> when the behavior of the environment is not known. The agent needs a policy or a value function to operate and there is no model. On the other hand, it is <b>model based</b> when the environment is known by means of a model. In this case, a policy and a value function might be optional, since it is possible that the agent can operate just knowing the model.</div>
<div class="standard" id="magicparlabel-3210">Model free agents are simpler, while model based agents are more sample efficient.</div>
<div class="standard" id="magicparlabel-3211">Another categorization is the following:</div>
<ul class="itemize" id="magicparlabel-3212"><li class="itemize_item">Value based: there is no policy, it is implicit in the value function.</li>
<li class="itemize_item">Policy based: there is no value function, the model operates only by means of the policy.</li>
<li class="itemize_item">Actor critic: they have both a policy and a value function.</li>
</ul>
<h3 class="subsection" id="magicparlabel-3215"><span class="subsection_label">10.7</span> Subproblems of RL</h3>
<div class="standard" id="magicparlabel-3216"><b>Prediction </b>consists in evaluating the future, for a given policy, i.e., what are the values in each state?</div>
<div class="standard" id="magicparlabel-3217"><b>Control</b> refers to the problem of optimising the future to find the best policy, i.e., which actions to take?</div>
<div class="standard" id="magicparlabel-3218">These two problems are strongly related, because the best actions to take will be decided using our predictions about the future:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi> π </mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo> arg </mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-3219">Two fundamental problems in RL are:</div>
<ul class="itemize" id="magicparlabel-3220"><li class="itemize_item">Learning: the environment is initially unknown and the agent interacts with it to learn.</li>
<li class="itemize_item">Planning/search: a model of the environment is given or learnt, and the agent plans in this model.</li>
</ul>
<div class="standard" id="magicparlabel-3222">In order to learn, we need to define all components of the problem as functions:</div>
<ul class="itemize" id="magicparlabel-3223"><li class="itemize_item">Policy: <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> π </mi><mo>:</mo><mi>S</mi><mo> → </mo><mi>A</mi>
</mrow>
</mrow></math> (or probabilities over <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>A</mi>
</mrow></math>).</li>
<li class="itemize_item">Value functions: <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>v</mi><mo>:</mo><mi>S</mi><mo> → </mo>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
</mrow></math>.</li>
<li class="itemize_item">Models: <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo>:</mo><mi>S</mi><mo> → </mo><mi>S</mi>
</mrow>
</mrow></math> or <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>r</mi><mo>:</mo><mi>S</mi><mo> → </mo>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
</mrow></math>.</li>
<li class="itemize_item">State update: <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>u</mi><mo>:</mo><mi>S</mi><mo> × </mo><mi>O</mi><mo> → </mo><mi>S</mi>
</mrow>
</mrow></math>.</li>
</ul>
<div class="standard" id="magicparlabel-3227">Then, we can use, for example, neural networks and deep learning techniques to learn. But we do need to be careful, because in RL it is usual to violate assumptios made in supervised learning, such as having i.i.d. samples, or stationarity.</div>
<h2 class="section" id="magicparlabel-3228"><span class="section_label">11</span> Markov Decision Processes</h2>
<div class="standard" id="magicparlabel-3229">We saw the notion of MDP, and now we formalize it:</div>
<div class="standard" id="magicparlabel-3230"><div class="flex_color_box"><div class="definition" id="magicparlabel-3234"><div class="definition_item"><span class="definition_label">Definition 11.1.</span>
A <b>Markov Decision Process (MDP)</b> is a tuple <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>S</mi><mo>,</mo><mi>A</mi><mo>,</mo><mi>p</mi><mo>,</mo><mi> γ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow></math> where:</div>
<ul class="itemize" id="magicparlabel-3235"><li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>S</mi>
</mrow></math> is the set of all possible states with the Markov Property.</li>
<li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>A</mi>
</mrow></math> is the set of all possible actions.</li>
<li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>r</mi><mo>,</mo><mi>s</mi><mo>'</mo><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> is the joint probability of a reward, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>r</mi>
</mrow></math>, and next state, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>s</mi><mo>'</mo>
</mrow>
</mrow></math>, given a state <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>s</mi>
</mrow></math> and an action <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>a</mi>
</mrow></math>.</li>
<li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> γ </mi><mo> ∈ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mn>0</mn><mo>,</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
</mrow></math> is a discount factor that trades off later rewards to earlier ones.</li>
</ul>
</div>
</div></div>
<div class="remark" id="magicparlabel-3239"><div class="remark_item"><span class="remark_label">Remark 11.1.</span>
<math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>p</mi>
</mrow></math> defines the dynamics of the problem.</div>
<div class="remark_item">Sometimes, it is useful to marginalise out the state transitions or expected rewards:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>'</mo><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow><mi>r</mi>
</mrow>
</msub><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>'</mo><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> to obtain the probability of arriving to a certain state.</div>
<div class="remark_item">Also, the expected reward:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>R</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>=</mo>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow><mi>r</mi>
</mrow>
</msub><mi>r</mi>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>s</mi><mo>'</mo>
</mrow>
</mrow>
</msub><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>r</mi><mo>,</mo><mi>s</mi><mo>'</mo><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
</div>
<div class="standard" id="magicparlabel-3242">There is an alternative equivalent definition, which introduces the notion of the expected reward into the concept, and takes it out of the probabity function:</div>
<div class="standard" id="magicparlabel-3243"><div class="flex_color_box"><div class="definition" id="magicparlabel-3247"><div class="definition_item"><span class="definition_label">Definition 11.2.</span>
A MDP is a tuple <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>S</mi><mo>,</mo><mi>A</mi><mo>,</mo><mi>p</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi> γ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow></math> where:</div>
<ul class="itemize" id="magicparlabel-3248"><li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>S</mi>
</mrow></math> is the set of all possible states with the Markov Property.</li>
<li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>A</mi>
</mrow></math> is the set of all possible actions.</li>
<li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>'</mo><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> is the probability of transitioning to <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>s</mi><mo>'</mo>
</mrow>
</mrow></math>, fiven a state <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>s</mi>
</mrow></math> and an action <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>a</mi>
</mrow></math>.</li>
<li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>r</mi><mo>:</mo><mi>S</mi><mo> × </mo><mi>A</mi><mo> → </mo>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow>
</mrow></math> is the expected reward, achieved on a transition starting in <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow></math>,<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>r</mi><mo>=</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>R</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mn>.</mn>
</mrow>
</mrow></math></li>
<li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> γ </mi><mo> ∈ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mn>0</mn><mo>,</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
</mrow></math> is a discount factor that trades off later rewards to earlier ones.</li>
</ul>
</div>
</div></div>
<div class="standard" id="magicparlabel-3253">Now, we have to clarify what is the Markov Property:</div>
<div class="standard" id="magicparlabel-3254"><div class="flex_color_box"><div class="definition" id="magicparlabel-3258"><div class="definition_item"><span class="definition_label">Definition 11.3.</span>
Consider a sequence of random variables, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">}</mo>
</mrow>
<mrow>
<mrow><mi>t</mi><mo> ∈ </mo>
<mstyle mathvariant="double-struck"><mi>N</mi>
</mstyle>
</mrow>
</mrow>
</msub>
</mrow></math>, indexed by time and taken from a set of states <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>S</mi>
</mrow></math>. Consider also the set of actions <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>A</mi>
</mrow></math> and rewards in <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mstyle mathvariant="double-struck"><mi>R</mi>
</mstyle>
</mrow></math>.</div>
<div class="definition_item">A state <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>s</mi>
</mrow></math> has the <b>Markov Property</b> when, for all <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>s</mi><mo>'</mo><mo> ∈ </mo><mi>S</mi>
</mrow>
</mrow></math>,<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>'</mo><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>'</mo><mo>|</mo>
<msub>
<mrow><mi>h</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> for all possible histories <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>h</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo><mn>...</mn><mo>,</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo><mn>...</mn><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo><mn>...</mn><mo>,</mo>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">}</mo>
</mrow>
</mrow></math>.</div>
</div>
</div></div>
<div class="standard" id="magicparlabel-3260">Therefore, in an MDP, the current state captures all relevant information from the history, it is a sufficient statistic of the past. So, once the state is known, the history may be thrown away.</div>
<div class="exercise" id="magicparlabel-3261"><div class="exercise_item"><span class="exercise_label">Exercise 11.1.</span>
In an MDP, which of the following statemest are true?</div>
<ol class="enumerate" id="magicparlabel-3262"><li class="enumerate_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>'</mo><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>'</mo><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo><mn>...</mn><mo>,</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo><mn>...</mn><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>: false, the RHS does not condition on <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>a</mi>
</mrow>
</mrow></math>.</li>
<li class="enumerate_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>'</mo><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>'</mo><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo><mn>...</mn><mo>,</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>: true.</li>
<li class="enumerate_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>'</mo><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>'</mo><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo><mn>...</mn><mo>,</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>: false, the RHS does not condition on <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>a</mi>
</mrow>
</mrow></math>.</li>
<li class="enumerate_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>=</mo><mi>r</mi><mo>,</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>'</mo><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>=</mo><mi>r</mi><mo>,</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>'</mo><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo><mn>...</mn><mo>,</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>: true.</li>
</ol>
</div>
<div class="standard" id="magicparlabel-3266">It is also worth noting that most MDPs are discounted, and there are several reasons for these:</div>
<ul class="itemize" id="magicparlabel-3267"><li class="itemize_item">Problem specification: immediate rewards may actually be more valuable. For instance, animal/human behavior shows preference for immediate reward.</li>
<li class="itemize_item">Solution side: it is mathematically convenient to discount rewards, because it allows for easier proofs of convergence, and avoids infinite returns in cyclic Markov processes.</li>
</ul>
<div class="standard" id="magicparlabel-3269">As we outlined previously, the <b>goal of an RL agent</b> is to find a behavior policy that maximises the expected return <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow></math>. Recall our definition for value funtion<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo><mi> π </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mn>.</mn>
</mrow>
</mrow></math> Similarly, we can define the <b>state-action values</b>, as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>a</mi><mo>,</mo><mi> π </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>,</mo>
</mrow>
</mrow></math> and there is the following connection between them:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow><mi>a</mi>
</mrow>
</msub><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>a</mi><mo>|</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo><mi> π </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>,</mo><mi> ∀ </mi><mi>s</mi><mo> ∈ </mo><mi>S</mi><mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-3270">Also, we can define the maximum possible value functions:</div>
<div class="standard" id="magicparlabel-3271"><div class="flex_color_box"><div class="definition" id="magicparlabel-3275"><div class="definition_item"><span class="definition_label">Definition 11.4.</span>
The <b>optimal state value function</b>, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, is the maximum value function over all policies,<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math> The <b>optimal state-action value function</b>, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>q</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, is the amximum state-action value function over all policies,<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>q</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
</div>
</div></div>
<div class="standard" id="magicparlabel-3276">The optimal value function specifies the best possible performance in the MDP. We can consider the MDP to be solved when we know the optimal value function.</div>
<div class="standard" id="magicparlabel-3277">In addition, value functions allow us to define a partial ordering over policies, having<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> π </mi><mo> ≥ </mo><mi> π </mi><mo>'</mo><mo> ⟺ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ≥ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow>
<mrow><mi> π </mi><mo>'</mo>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo><mi> ∀ </mi><mi>s</mi><mo> ∈ </mo><mi>S</mi><mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-3278">With this partial ordering, the following theorem state that optimal policies exist for every MDP:</div>
<div class="standard" id="magicparlabel-3279"><div class="flex_color_box"><div class="theorem" id="magicparlabel-3283"><div class="theorem_item"><span class="theorem_label">Theorem 11.1.</span>
<b>Optimal Policies Theorem</b></div>
<div class="theorem_item">For any MDP:</div>
<ul class="itemize" id="magicparlabel-3285"><li class="itemize_item"><i>There exists an optimal policy <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msup>
<mrow><mi> π </mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup>
</mrow></math> that is better than or equal to all other policies,<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi> π </mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo> ≥ </mo><mi> π </mi><mo>,</mo><mi> ∀ </mi><mi> π </mi><mn>.</mn>
</mrow>
</mrow></math></i></li>
<li class="itemize_item"><i>All optimal policies achieve the optimal value function,<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow>
<msup>
<mrow><mi> π </mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></i></li>
<li class="itemize_item"><i>All optimal policies achieve the optimal state-action value function, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>q</mi>
</mrow>
<mrow>
<msup>
<mrow><mi> π </mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msup>
<mrow><mi>q</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></i></li>
</ul>
</div>
</div></div>
<div class="standard" id="magicparlabel-3288">To find an optimal policy, we can maximise over <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>q</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi> π </mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mtable>
<mtr>
<mtd><mn>1</mn>
</mtd>
<mtd>
<mrow><mi>i</mi><mi>f</mi><mspace width="10px"></mspace><mi>a</mi><mo>=</mo><mo> arg </mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow>
<mrow><mi>a</mi><mo> ∈ </mo><mi>A</mi>
</mrow>
</mrow>
</msub>
<msup>
<mrow><mi>q</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mn>0</mn>
</mtd>
<mtd>
<mrow><mi>o</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>w</mi><mi>i</mi><mi>s</mi><mi>e</mi>
</mrow>
</mtd>
</mtr>
</mtable><mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-3289">That is, the optimal policy is to take action <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>a</mi>
</mrow></math> in state <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>s</mi>
</mrow></math> if <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>a</mi>
</mrow></math> is the action that gives the highes state-action value given state <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>s</mi>
</mrow></math>.</div>
<div class="remark" id="magicparlabel-3290"><div class="remark_item"><span class="remark_label">Remark 11.2.</span>
There is always a deterministic optimal policy for any MDP, and we know <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>q</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, we know the optimal policy immediately.</div>
<div class="remark_item">Also, there can be multiple optimal policies, and if multiple actions maximize <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mo> ⋅ </mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, we can pick any of them.</div>
</div>
<div class="standard" id="magicparlabel-3292">Now, recall the Bellman Equations we saw previously. The following theorem explains how to express the value functions by means of these equations:</div>
<div class="standard" id="magicparlabel-3293"><div class="flex_color_box"><div class="theorem" id="magicparlabel-3297"><div class="theorem_item"><span class="theorem_label">Theorem 11.2.</span>
<b>Bellman Expectation Equations</b></div>
<div class="theorem_item">Given an MDP, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>M</mi><mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>S</mi><mo>,</mo><mi>A</mi><mo>,</mo><mi>p</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi> γ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, for any policy <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> π </mi>
</mrow></math>, the value functions obey the followin expectation equations:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow><mi>a</mi>
</mrow>
</msub><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>r</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mi> γ </mi>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>s</mi><mo>'</mo>
</mrow>
</mrow>
</msub><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>'</mo><mo>|</mo><mi>a</mi><mo>,</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>'</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>,</mo>
</mrow>
</mrow></math> and<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi>r</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mi> γ </mi>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>s</mi><mo>'</mo>
</mrow>
</mrow>
</msub><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>'</mo><mo>|</mo><mi>a</mi><mo>,</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>a</mi><mo>'</mo><mo> ∈ </mo><mi>A</mi>
</mrow>
</mrow>
</msub><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>a</mi><mo>'</mo><mo>|</mo><mi>s</mi><mo>'</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>'</mo><mo>,</mo><mi>a</mi><mo>'</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
</div>
</div></div>
<div class="standard" id="magicparlabel-3299"><div class="flex_color_box"><div class="theorem" id="magicparlabel-3303"><div class="theorem_item"><span class="theorem_label">Theorem 11.3.</span>
<b>Bellman Optimality Equations</b></div>
<div class="theorem_item">Given an MDP, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>M</mi><mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>S</mi><mo>,</mo><mi>A</mi><mo>,</mo><mi>p</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi> γ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, the optimal value functions obey the following expectation equations:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow>
<mrow><mi>a</mi><mo> ∈ </mo><mi>A</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>r</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mi> γ </mi>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>s</mi><mo>'</mo>
</mrow>
</mrow>
</msub><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>'</mo><mo>|</mo><mi>a</mi><mo>,</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>'</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>,</mo>
</mrow>
</mrow></math><math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>q</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi>r</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mi> γ </mi>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>s</mi><mo>'</mo>
</mrow>
</mrow>
</msub><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>'</mo><mo>|</mo><mi>a</mi><mo>,</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow>
<mrow><mi>a</mi><mo>'</mo><mo> ∈ </mo><mi>A</mi>
</mrow>
</mrow>
</msub>
<msup>
<mrow><mi>q</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>'</mo><mo>,</mo><mi>a</mi><mo>'</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
</div>
</div></div>
<div class="remark" id="magicparlabel-3305"><div class="remark_item"><span class="remark_label">Remark 11.3.</span>
There can not be a policy with a higher value than <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo><mi> ∀ </mi><mi>s</mi>
</mrow>
</mrow></math>.</div>
</div>
<div class="standard" id="magicparlabel-3306"><em>Intuition on the proof for the Bellman Optimality </em>Equations<em>:</em></div>
<div class="standard" id="magicparlabel-3307">An optimal policy can be found by maximising over <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>q</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>,<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi> π </mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mtable>
<mtr>
<mtd><mn>1</mn>
</mtd>
<mtd>
<mrow><mi>i</mi><mi>f</mi><mspace width="10px"></mspace><mi>a</mi><mo>=</mo><mo> arg </mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow>
<mrow><mi>a</mi><mo> ∈ </mo><mi>A</mi>
</mrow>
</mrow>
</msub>
<msup>
<mrow><mi>q</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mn>0</mn>
</mtd>
<mtd>
<mrow><mi>o</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>w</mi><mi>i</mi><mi>s</mi><mi>e</mi><mn>.</mn>
</mrow>
</mtd>
</mtr>
</mtable>
</mrow>
</mrow></math> Applyin the Bellman Expectation Equation:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow>
<msup>
<mrow><mi> π </mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
</mrow>
</mtd>
<mtd>
<mrow><mi>r</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mi> γ </mi>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>s</mi><mo>'</mo>
</mrow>
</mrow>
</msub><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>'</mo><mo>|</mo><mi>a</mi><mo>,</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>a</mi><mo>'</mo><mo> ∈ </mo><mi>A</mi>
</mrow>
</mrow>
</msub>
<msup>
<mrow><mi> π </mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>a</mi><mo>'</mo><mo>|</mo><mi>s</mi><mo>'</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow>
<msup>
<mrow><mi> π </mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>'</mo><mo>,</mo><mi>a</mi><mo>'</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mo>=</mo>
</mtd>
<mtd>
<mrow><mi>r</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mi> γ </mi>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>s</mi><mo>'</mo>
</mrow>
</mrow>
</msub><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>'</mo><mo>|</mo><mi>a</mi><mo>,</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow>
<mrow><mi>a</mi><mo>'</mo><mo> ∈ </mo><mi>A</mi>
</mrow>
</mrow>
</msub>
<msup>
<mrow><mi>q</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>'</mo><mo>,</mo><mi>a</mi><mo>'</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mtd>
</mtr>
</mtable></math></div>
<div class="standard" id="magicparlabel-3308">We can also express the Bellman equations in matrix form, as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>V</mi><mo>=</mo>
<msup>
<mrow><mi>R</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msup><mo>+</mo><mi> γ </mi>
<msup>
<mrow><mi>P</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msup><mi>V</mi><mo>,</mo>
</mrow>
</mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>=</mo><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msubsup>
<mrow><mi>R</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msubsup><mo>=</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo> ∼ </mo><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msubsup>
<mrow><mi>P</mi>
</mrow>
<mrow>
<mrow><mi>i</mi><mi>j</mi>
</mrow>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msubsup><mo>=</mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>j</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow><mi>a</mi>
</mrow>
</msub><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>a</mi><mo>|</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>j</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>.</div>
<div class="standard" id="magicparlabel-3309">This is a linear equations, that can be solved directly:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>I</mi><mo>-</mo><mi> γ </mi>
<msup>
<mrow><mi>P</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msup>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mi>V</mi><mo>=</mo>
</mrow>
</mtd>
<mtd>
<msup>
<mrow><mi>R</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msup>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow><mi>V</mi><mo>=</mo>
</mrow>
</mtd>
<mtd>
<mrow>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>I</mi><mo>-</mo><mi> γ </mi>
<msup>
<mrow><mi>P</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msup>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mrow>
<mrow><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msup>
<msup>
<mrow><mi>R</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msup><mn>.</mn>
</mrow>
</mtd>
</mtr>
</mtable></math> The computational complexity is <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>O</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">|</mo>
<mi>S</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">|</mo>
</mrow>
<mrow><mn>3</mn>
</mrow>
</msup>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, making this only feasible for small problems. This makes it helpful to design other methods for larger problems. For example, there are iterative methods such as dynamic programming, monte-calro evaluation and temporal-difference learning.</div>
<h3 class="subsection" id="magicparlabel-3310"><span class="subsection_label">11.1</span> Solving Reinforcement Learning Problems with Bellman Equations</h3>
<ol class="enumerate" id="magicparlabel-3311"><li class="enumerate_item"> Estimating <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub>
</mrow></math> or <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub>
</mrow></math> is called <b>policy evaluation</b>, or <b>prediction</b>:

<ol class="enumerate" id="magicparlabel-3316"><li class="enumerate_item">Given a policy, what is my expected return under that behavior?</li>
<li class="enumerate_item">Given this treatment protocol/trading strategy, what is my expected return?</li>
</ol>
</li><li class="enumerate_item"> Estimating <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msub>
</mrow></math> or <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msub>
</mrow></math> is sometimes called <b>control</b>, because these can be used for <b>policy optimisation</b>:

<ol class="enumerate" id="magicparlabel-3323"><li class="enumerate_item">What is the optimal way of behaving? What is the optimal value function?</li>
<li class="enumerate_item">What is the optimal treatment? What is the optimal control policy to minimise time, fuel consumption, etc?</li>
</ol>
</li></ol>
<div class="exercise" id="magicparlabel-3325"><div class="exercise_item"><span class="exercise_label">Exercise 11.2.</span>
Consider the following MDP:</div>
<div class="standard" id="magicparlabel-3326" style="text-align: center;"><img alt="image: 62_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___e-Learning_LectureNotes_source_mdp_exercise.png" src="62_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___e-Learning_LectureNotes_source_mdp_exercise.png" style="width:60%;"/>
</div>
<div class="exercise_item">The actions have a 0.9 probability of success and with 0.1 probability we remain in the same state.</div>
<div class="exercise_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mn>0</mn>
</mrow>
</mrow></math> for all transitions that end up in <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mo>-</mo><mn>1</mn>
</mrow>
</mrow></math> for all other transitions.</div>
<div class="exercise_item">Discount factor: <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> γ </mi><mo>=</mo><mn>0.9</mn>
</mrow>
</mrow></math>.</div>
<div class="exercise_item">Questions:</div>
<ul class="itemize" id="magicparlabel-3331"><li class="itemize_item">What is <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub>
</mrow></math> for <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mo> → </mo>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo><mi> ∀ </mi><mi>s</mi>
</mrow>
</mrow></math>?
<div class="standard" id="magicparlabel-3332">According to the Bellman Equations:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
</mrow>
</mtd>
<mtd>
<mrow>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow><mi>a</mi>
</mrow>
</msub><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>r</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mi> γ </mi>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>s</mi><mo>'</mo>
</mrow>
</mrow>
</msub><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>'</mo><mo>|</mo><mi>a</mi><mo>,</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>'</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>,</mo>
</mrow>
</mtd>
</mtr>
</mtable></math> <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
</mrow>
</mtd>
<mtd>
<mrow><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mi> ✓ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>r</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mi> ✓ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mn>0.9</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>+</mo><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo> × </mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>r</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo> × </mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mn>0.9</mn><mo> ⋅ </mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo> × </mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mo>=</mo>
</mtd>
<mtd>
<mrow><mn>0.9</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo>-</mo><mn>1</mn><mo>+</mo><mn>0.9</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mn>0</mn><mo>+</mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>+</mo><mn>0.09</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mo>=</mo>
</mtd>
<mtd>
<mrow><mo>-</mo><mn>0.9</mn><mo>+</mo><mn>0.81</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mn>0.09</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mtd>
</mtr>
</mtable></math>Isolating, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, we get<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mn>0.91</mn>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo>-</mo><mn>0.9</mn><mo>+</mo><mn>0.81</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ⟹ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo>-</mo><mn>0.99</mn><mo>+</mo><mn>0.89</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math> Now, let's go for <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
</mrow>
</mtd>
<mtd>
<mrow><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mi> ✓ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>r</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mi> ✓ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mn>0.9</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>+</mo><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo> × </mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo>-</mo><mn>1</mn><mo>+</mo><mn>0.9</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mo>=</mo>
</mtd>
<mtd>
<mrow><mn>0.9</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mn>0</mn><mo>+</mo><mn>0.9</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mn>0</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>+</mo><mn>0.1</mn><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo>-</mo><mn>1</mn><mo>+</mo><mn>0.9</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mo>=</mo>
</mtd>
<mtd>
<mrow><mn>0.81</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>-</mo><mn>0.1</mn><mo>+</mo><mn>0.09</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mo>=</mo>
</mtd>
<mtd>
<mrow><mn>0.81</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mo>-</mo><mn>1</mn><mo>+</mo><mn>0.9</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>-</mo><mn>0.1</mn><mo>+</mo><mn>0.09</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mo>=</mo>
</mtd>
<mtd>
<mrow><mo>-</mo><mn>0.91</mn><mo>+</mo><mn>0.82</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mtd>
</mtr>
</mtable></math>Therefore,<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mfrac>
<mrow>
<mrow><mo>-</mo><mn>0.91</mn>
</mrow>
</mrow>
<mrow><mn>0.18</mn>
</mrow>
</mfrac><mo>=</mo><mo>-</mo><mn>5.06.</mn>
</mrow>
</mrow></math> This means that<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo>-</mo><mn>5.49.</mn>
</mrow>
</mrow></math> Finally, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
</mrow>
</mtd>
<mtd>
<mrow><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>r</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mn>0.9</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>+</mo><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo> × </mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo>-</mo><mn>1</mn><mo>+</mo><mn>0.9</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mo>=</mo>
</mtd>
<mtd>
<mrow><mn>0.9</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mn>0</mn><mo>+</mo><mn>0.9</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo>-</mo><mn>5.49</mn><mo>+</mo><mn>0</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>+</mo><mn>0.1</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo>-</mo><mn>1</mn><mo>+</mo><mn>0.9</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mo>=</mo>
</mtd>
<mtd>
<mrow><mo>-</mo><mn>4.45</mn><mo>-</mo><mn>0.1</mn><mo>+</mo><mn>0.09</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mo>=</mo>
</mtd>
<mtd>
<mrow><mo>-</mo><mn>4.55</mn><mo>+</mo><mn>0.09</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mtd>
</mtr>
</mtable></math> So<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mfrac>
<mrow>
<mrow><mo>-</mo><mn>4.55</mn>
</mrow>
</mrow>
<mrow><mn>0.91</mn>
</mrow>
</mfrac><mo>=</mo><mo>-</mo><mn>5.</mn>
</mrow>
</mrow></math> That is, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo>-</mo><mn>5.49</mn><mo>,</mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo>-</mo><mn>5</mn>
</mrow>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo>-</mo><mn>5.06</mn>
</mrow>
</mrow></math>.</div>
</li><li class="itemize_item">What is <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub>
</mrow></math> for the uniformly random policy?
<div class="standard" id="magicparlabel-3334">This is: with probability 0.1, stay in the same state, with probability 0.45 choose <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow></math> and with probability 0.45 choose <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow></math>. For <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
</mrow></math>:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
</mrow>
</mtd>
<mtd>
<mrow><mn>0.1</mn><mo> ⋅ </mo><mn>0.9</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mn>0.45</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo>-</mo><mn>1</mn><mo>+</mo><mn>0.9</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>+</mo><mn>0.45</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo>-</mo><mn>1</mn><mo>+</mo><mn>0.9</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mo>=</mo>
</mtd>
<mtd>
<mrow><mn>0.09</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>-</mo><mn>0.9</mn><mo>+</mo><mn>0.405</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mn>0.405</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mtd>
</mtr>
</mtable></math> so<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo>-</mo><mn>0.99</mn><mo>+</mo><mn>0.445</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mn>0.445</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math> For <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow></math>:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
</mrow>
</mtd>
<mtd>
<mrow><mn>0.1</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo>-</mo><mn>1</mn><mo>+</mo><mn>0.9</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>+</mo><mn>0.45</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mn>0</mn><mo>+</mo><mn>0.9</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>+</mo><mn>0.45</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mn>0</mn><mo>+</mo><mn>0.9</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mo>=</mo>
</mtd>
<mtd>
<mrow><mo>-</mo><mn>0.1</mn><mo>+</mo><mn>0.09</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mn>0.81</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mtd>
</mtr>
</mtable></math>so<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo>-</mo><mn>0.1</mn><mo>+</mo><mn>0.9</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math> For <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow></math>:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
</mrow>
</mtd>
<mtd>
<mrow><mn>0.1</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo>-</mo><mn>1</mn><mo>+</mo><mn>0.9</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>+</mo><mn>0.45</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mn>0</mn><mo>+</mo><mn>0.9</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>+</mo><mn>0.45</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mn>0</mn><mo>+</mo><mn>0.9</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mo>=</mo>
</mtd>
<mtd>
<mrow><mo>-</mo><mn>0.1</mn><mo>+</mo><mn>0.09</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mn>0.81</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mtd>
</mtr>
</mtable></math> so<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo>-</mo><mn>0.1</mn><mo>+</mo><mn>0.9</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math> Therefore_<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
</mrow>
</mtd>
<mtd>
<mrow><mo>-</mo><mn>0.99</mn><mo>+</mo><mn>0.89</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mo>-</mo><mn>0.1</mn><mo>+</mo><mn>0.9</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mo>=</mo>
</mtd>
<mtd>
<mrow><mo>-</mo><mn>0.99</mn><mo>-</mo><mn>0.089</mn><mo>+</mo><mn>0.8</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mo>=</mo>
</mtd>
<mtd>
<mrow><mo>-</mo><mn>1.08</mn><mo>+</mo><mn>0.8</mn><mo> ⋅ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mtd>
</mtr>
</mtable></math> That is,<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo>-</mo><mn>5.4.</mn>
</mrow>
</mrow></math> And,<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo>-</mo><mn>4.96.</mn>
</mrow>
</mrow></math></div>
</li><li class="itemize_item">Same policy evaluation problems for <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> γ </mi><mo>=</mo><mn>0</mn>
</mrow>
</mrow></math>? What do you notice?
<div class="standard" id="magicparlabel-3336">First, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mo> → </mo>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo><mi> ∀ </mi><mi>s</mi>
</mrow>
</mrow></math>:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
</mrow>
</mtd>
<mtd>
<mrow><mn>0.1</mn><mo> ⋅ </mo><mn>0</mn><mo>+</mo><mn>0.9</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mo>-</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mo>=</mo>
</mtd>
<mtd>
<mrow><mo>-</mo><mn>0.9.</mn>
</mrow>
</mtd>
</mtr>
</mtable></math><math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
</mrow>
</mtd>
<mtd>
<mrow><mn>0.1</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mo>-</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mn>0.9</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mn>0</mn>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mo>=</mo>
</mtd>
<mtd>
<mrow><mo>-</mo><mn>0.1.</mn>
</mrow>
</mtd>
</mtr>
</mtable></math><math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
</mrow>
</mtd>
<mtd>
<mrow><mo>-</mo><mn>0.1.</mn>
</mrow>
</mtd>
</mtr>
</mtable></math> Second, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> the random policy:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>0</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
</mrow>
</mtd>
<mtd>
<mrow><mn>0.1</mn><mo> ⋅ </mo><mn>0</mn><mo>+</mo><mn>0.45</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mo>-</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mn>0.45</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mo>-</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mo>=</mo>
</mtd>
<mtd>
<mrow><mo>-</mo><mn>0.9.</mn>
</mrow>
</mtd>
</mtr>
</mtable></math><math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
</mrow>
</mtd>
<mtd>
<mrow><mn>0.1</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mo>-</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mn>0.9</mn><mo> ⋅ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mn>0</mn>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mo>=</mo>
</mtd>
<mtd>
<mrow><mo>-</mo><mn>0.1.</mn>
</mrow>
</mtd>
</mtr>
</mtable></math><math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
</mrow>
</mtd>
<mtd>
<mrow><mo>-</mo><mn>0.1.</mn>
</mrow>
</mtd>
</mtr>
</mtable></math> We can observe that if we don't take the discount factor into account, two very different policies can give us the same (short term) values.</div>
</li></ul>
</div>
<h3 class="subsection" id="magicparlabel-3337"><span class="subsection_label">11.2</span> Dynamic Programming</h3>
<div class="standard" id="magicparlabel-3338"><div class="flex_color_box"><div class="plain_layout" id="magicparlabel-3347">Dynamic programming refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov Decision Process (MDP).</div>
<div class="plain_layout" id="magicparlabel-3348" style="text-align: right;">-Sutton &amp; Barto, 2018</div>
</div></div>
<div class="standard" id="magicparlabel-3349">We will discuss several dynamic programming methods to solve MDPs, all of which consist of two important parts:</div>
<ul class="itemize" id="magicparlabel-3350"><li class="itemize_item">Policy evaluation.</li>
<li class="itemize_item">Policy improvement.</li>
</ul>
<h4 class="subsubsection" id="magicparlabel-3352"><span class="subsubsection_label">11.2.1</span> Policy Evaluation</h4>
<div class="standard" id="magicparlabel-3353">We start by discussing how to estimate<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>|</mo><mi>s</mi><mo>,</mo><mi> π </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mn>.</mn>
</mrow>
</mrow></math> The idea is to turn the equality into an update rule. The process is described in the following algorithm:</div>
<div class="float-listings"><pre class="listings Python">def policy_eval(S, R, pi, gamma):

	v = [0 for s in S]
	
	repeat until converge:
		for s in S:
			v_new(s) = E[R + gamma * v(S_t+1) | S_t = s, pi]

		v = v_new

	return v</pre></div>
<div class="standard" id="magicparlabel-3369">Note that this algorithm always converge under appropriate conditions, like <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> γ </mi><mo>&lt;</mo><mn>1</mn>
</mrow>
</mrow></math>. We will delve into this later.</div>
<div class="example" id="magicparlabel-3370"><div class="example_item"><span class="example_label">Example 11.1.</span>
Policy Evaluation example.</div>
<div class="example_item">Take the following MDP:</div>
<div class="standard" id="magicparlabel-3372" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3406">1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3409">2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3412">3</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3415">4</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3418">5</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3421">6</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3424">7</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3427">8</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3430">9</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3433">10</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3436">11</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3439">12</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3442">13</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3445">14</div>
</td>
<td align="center" valign="top">
</td>
</tr></tbody>
</table>
</div>
<div class="example_item">The possible actions are to go up, down, left, right and when you reach the red cells, you finish. Each transitions costs -1 point.</div>
<div class="example_item">Let's evaluate the random policy with <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> γ </mi><mo>=</mo><mn>1</mn>
</mrow>
</mrow></math>.</div>
<div class="example_item">Initialization:</div>
<div class="standard" id="magicparlabel-3456" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3483">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3486">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3489">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3492">0</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3495">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3498">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3501">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3504">0</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3507">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3510">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3513">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3516">0</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3519">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3522">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3525">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3528">0</div>
</td>
</tr></tbody>
</table>
</div>
<div class="example_item">Step 1: we do<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>R</mi><mo>+</mo><mi> γ </mi><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>S</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>|</mo><mi>S</mi><mo>,</mo><mi> π </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>,</mo>
</mrow>
</mrow></math> so for example, for cell <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mn>1</mn>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow></math>, it is:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mn>1</mn>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>3</mn>
</mrow>
</mfrac><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mo>-</mo><mn>1</mn><mo>+</mo><mn>1</mn><mo> ⋅ </mo><mn>0</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ⋅ </mo><mn>3</mn><mo>=</mo><mo>-</mo><mn>1.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-3530" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3557">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3560">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3563">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3566">-1</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3569">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3572">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3575">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3578">-1</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3581">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3584">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3587">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3590">-1</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3593">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3596">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3599">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3602">0</div>
</td>
</tr></tbody>
</table>
</div>
<div class="example_item">Step 2:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mn>1</mn>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>3</mn>
</mrow>
</mfrac><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mo>-</mo><mn>1</mn><mo>+</mo><mn>1</mn><mo> ⋅ </mo><mn>0</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo>
<mfrac>
<mrow><mn>2</mn>
</mrow>
<mrow><mn>3</mn>
</mrow>
</mfrac><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mo>-</mo><mn>1</mn><mo>-</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo>-</mo><mn>1.7.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-3604" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3631">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3634">-1.7</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3637">-2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3640">-2</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3643">-1.7</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3646">-2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3649">-2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3652">-2</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3655">-2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3658">-2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3661">-2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3664">-1.7</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3667">-2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3670">-2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3673">-1.7</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3676">0</div>
</td>
</tr></tbody>
</table>
</div>
<div class="example_item">Step 3:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mn>1</mn>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>3</mn>
</mrow>
</mfrac><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mo>-</mo><mn>1</mn><mo>+</mo><mn>0</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo>
<mfrac>
<mrow><mn>2</mn>
</mrow>
<mrow><mn>3</mn>
</mrow>
</mfrac><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mo>-</mo><mn>1</mn><mo>-</mo><mn>2</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo>-</mo><mn>2.3.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-3678" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3705">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3708">-2.3</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3711">-2.9</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3714">-3</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3717">-2.3</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3720">-2.9</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3723">-3</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3726">-2.9</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3729">-2.9</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3732">–3</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3735">-2.9</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3738">-2.3</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3741">-3</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3744">-2.9</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3747">-2.3</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3750">0</div>
</td>
</tr></tbody>
</table>
</div>
<div class="example_item">And so on... It would converge at</div>
<div class="standard" id="magicparlabel-3752" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3779">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3782">-14</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3785">-20</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3788">-22</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3791">-14</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3794">-18</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3797">-20</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3800">-20</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3803">-20</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3806">-20</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3809">-18</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3812">-14</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3815">-22</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3818">-20</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3821">-14</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3824">0</div>
</td>
</tr></tbody>
</table>
</div>
</div>
<h4 class="subsubsection" id="magicparlabel-3825"><span class="subsubsection_label">11.2.2</span> Policy Improvement</h4>
<div class="standard" id="magicparlabel-3826">We can use the values computed with policy evaluation to improve the policy. The simplest way to achieve this is with a <b>greedy policy improvement</b> approach, which is as follows:</div>
<div class="float-listings"><pre class="listings Python">def policy_improvement(S, R, pi, gamma)
	pi_new = {}

	v = [0 for s in S]

	for s in S:
		v(s) = policy_eval(S, R, pi, gamma)
		pi_new[v] = argmax_a E[R + gamma*v(S_t+1) | S_t = s, A_t = a]
		pi = pi_new

	return pi_new</pre></div>
<div class="claim" id="magicparlabel-3842"><div class="claim_item"><span class="claim_label">Claim 11.1.</span>
It is possible to show that<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow>
<msub>
<mrow><mi> π </mi>
</mrow>
<mrow>
<mrow><mi>n</mi><mi>e</mi><mi>w</mi>
</mrow>
</mrow>
</msub>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ≥ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo><mi> ∀ </mi><mi>s</mi><mn>.</mn>
</mrow>
</mrow></math></div>
</div>
<div class="example" id="magicparlabel-3843"><div class="example_item"><span class="example_label">Example 11.2.</span>
We can use this greedy approach combined with the previous example:</div>
<div class="example_item">Initialization, with random policy:</div>
<div class="standard" id="magicparlabel-3845" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3872">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3875">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3878">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3881">0</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3884">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3887">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3890">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3893">0</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3896">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3899">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3902">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3905">0</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3908">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3911">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3914">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3917">0</div>
</td>
</tr></tbody>
</table>
<table><tbody><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3944">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3947"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↓ </mo><mo>,</mo><mo> ← </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3950"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↓ </mo><mo>,</mo><mo> ← </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3953"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↓ </mo><mo>,</mo><mo> ← </mo>
</mrow>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3956"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ↓ </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3959"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ↓ </mo><mo>,</mo><mo> ← </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3962"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ↓ </mo><mo>,</mo><mo> ← </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3965"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ↓ </mo><mo>,</mo><mo> ← </mo>
</mrow>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3968"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ↓ </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3971"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ↓ </mo><mo>,</mo><mo> ← </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3974"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ↓ </mo><mo>,</mo><mo> ← </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3977"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ↓ </mo><mo>,</mo><mo> ← </mo>
</mrow>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3980"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3983"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ← </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-3986"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ← </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
</td>
</tr></tbody>
</table>
</div>
<div class="example_item">Step 1:</div>
<div class="standard" id="magicparlabel-3991" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4018">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4021">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4024">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4027">-1</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4030">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4033">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4036">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4039">-1</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4042">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4045">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4048">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4051">-1</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4054">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4057">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4060">-1</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4063">0</div>
</td>
</tr></tbody>
</table>
<table><tbody><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4090">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4093"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> ← </mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4096"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↓ </mo><mo>,</mo><mo> ← </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4099"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↓ </mo><mo>,</mo><mo> ← </mo>
</mrow>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4102"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> ↑ </mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4105"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ↓ </mo><mo>,</mo><mo> ← </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4108"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ↓ </mo><mo>,</mo><mo> ← </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4111"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ↓ </mo><mo>,</mo><mo> ← </mo>
</mrow>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4114"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ↓ </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4117"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ↓ </mo><mo>,</mo><mo> ← </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4120"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ↓ </mo><mo>,</mo><mo> ← </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4123"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> ↓ </mo>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4126"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4129"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ← </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4132"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> → </mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
</td>
</tr></tbody>
</table>
</div>
<div class="example_item">Step 2:</div>
<div class="standard" id="magicparlabel-4137" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4164">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4167">-1.7</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4170">-2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4173">-2</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4176">-1.7</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4179">-2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4182">-2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4185">-2</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4188">-2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4191">-2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4194">-2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4197">-1.7</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4200">-2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4203">-2</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4206">-1.7</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4209">0</div>
</td>
</tr></tbody>
</table>
<table><tbody><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4236">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4239"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> ← </mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4242"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> ← </mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4245"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↓ </mo><mo>,</mo><mo> ← </mo>
</mrow>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4248"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> ↑ </mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4251"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ← </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4254"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ↓ </mo><mo>,</mo><mo> ← </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4257"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> ↓ </mo>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4260"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> ↑ </mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4263"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ↓ </mo><mo>,</mo><mo> ← </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4266"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↓ </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4269"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> ↓ </mo>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4272"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4275"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> → </mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4278"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> → </mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
</td>
</tr></tbody>
</table>
</div>
<div class="example_item">Step 3:</div>
<div class="standard" id="magicparlabel-4283" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4310">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4313">-2.3</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4316">-2.9</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4319">-3</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4322">-2.3</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4325">-2.9</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4328">-3</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4331">-2.9</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4334">-2.9</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4337">–3</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4340">-2.9</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4343">-2.3</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4346">-3</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4349">-2.9</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4352">-2.3</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4355">0</div>
</td>
</tr></tbody>
</table>
<table><tbody><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4382">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4385"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> ← </mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4388"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> ← </mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4391"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↓ </mo><mo>,</mo><mo> ← </mo>
</mrow>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4394"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> ↑ </mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4397"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ← </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4400"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ↓ </mo><mo>,</mo><mo> ← </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4403"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> ↓ </mo>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4406"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> ↑ </mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4409"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ↓ </mo><mo>,</mo><mo> ← </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4412"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↓ </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4415"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> ↓ </mo>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4418"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4421"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> → </mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4424"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> → </mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
</td>
</tr></tbody>
</table>
</div>
<div class="example_item">Step converged:</div>
<div class="standard" id="magicparlabel-4429" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4456">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4459">-14</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4462">-20</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4465">-22</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4468">-14</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4471">-18</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4474">-20</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4477">-20</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4480">-20</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4483">-20</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4486">-18</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4489">-14</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4492">-22</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4495">-20</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4498">-14</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4501">0</div>
</td>
</tr></tbody>
</table>
<table><tbody><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4528">0</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4531"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> ← </mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4534"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> ← </mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4537"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↓ </mo><mo>,</mo><mo> ← </mo>
</mrow>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4540"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> ↑ </mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4543"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ← </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4546"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ↓ </mo><mo>,</mo><mo> ← </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4549"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> ↓ </mo>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4552"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> ↑ </mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4555"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> ↓ </mo><mo>,</mo><mo> ← </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4558"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↓ </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4561"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> ↓ </mo>
</mrow></math></div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4564"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> ↑ </mo><mo>,</mo><mo> → </mo>
</mrow>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4567"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> → </mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4570"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> → </mo>
</mrow></math></div>
</td>
<td align="center" valign="top">
</td>
</tr></tbody>
</table>
</div>
<div class="example_item">Observe how in the second iteration we already found the optimal policy!</div>
<div class="example_item">In this example, we showed how we can use evaluation to improve our policy, and in fact we obtained the optimal policy. However, the greedy approach does not always ensure reaching the optimal policy.</div>
</div>
<div class="standard" id="magicparlabel-4576">This approach is called <b>policy iteration</b>:</div>
<ul class="itemize" id="magicparlabel-4577"><li class="itemize_item">Policy evaluation: estimate <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msup>
</mrow></math>.</li>
<li class="itemize_item">Policy improvement: generate <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> π </mi><mo>'</mo><mo> ≥ </mo><mi> π </mi>
</mrow>
</mrow></math>.</li>
</ul>
<div class="standard" id="magicparlabel-4579">It is natural to ask if policy evaluation need to converge to <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msup>
</mrow></math> or if we should stop the evaluation at some point. Ways to stop it are to put a threshold of minimum change between iterations, or simply after <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>k</mi>
</mrow></math> iterations. One extreme, which is in fact quite usual in practice, is to stop after <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>k</mi><mo>=</mo><mn>1</mn>
</mrow>
</mrow></math>, which is equivalent to <b>value iteration</b>:</div>
<div class="float-listings"><pre class="listings Python">def value_iter(S, R, pi, gamma):

	v = [0 for s in S]
	
	repeat until converge
		for s in S:
			v_new(s) = max_a E[R + gamma * v(S_t+1) | S_t = s, A_t = a]
		
		v = v_new

	return v</pre></div>
<div class="standard" id="magicparlabel-4595">In the following table, we sum up the different approaches:</div>
<div class="standard" id="magicparlabel-4596" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4618">Problem</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4621">Bellman Equation</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4624">Algorithm</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4627">Prediction</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4630">Bellman Expectation Eq</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4633">Iterative Policy Eval</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4636">Control</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4639">Bellman Expectation Eq + (Greedy) Policy Improvement</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4642">Policy Iteration</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4645">Control</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4648">Bellman Optimality Eq</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4651">Value Iteration</div>
</td>
</tr></tbody>
</table>
</div>
<div class="standard" id="magicparlabel-4652">Observations</div>
<ul class="itemize" id="magicparlabel-4653"><li class="itemize_item">The algorithms are based on state-value functions <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> or <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>v</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, with complexity <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>O</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">|</mo>
<mi>A</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">|</mo>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">|</mo>
<mi>S</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">|</mo>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msup>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> per iteration.</li>
<li class="itemize_item">It could also be applied to action-value functions <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> or <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>q</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, with complexity <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>O</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">|</mo>
<mi>A</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">|</mo>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msup>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">|</mo>
<mi>S</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">|</mo>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msup>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> per iteration.</li>
</ul>
<h3 class="subsection" id="magicparlabel-4655"><span class="subsection_label">11.3</span> Extensions to Dynamic Programming</h3>
<h4 class="subsubsection" id="magicparlabel-4656"><span class="subsubsection_label">11.3.1</span> Asynchronous Dynamic Programming</h4>
<div class="standard" id="magicparlabel-4657">DP methods described so far used <b>synchronous</b> updates, meaning all states are updated in parallel. In contrast, <b>asynchronous DP</b> backs up states individually, in any order. This can significantly reduce computation, and it is guaranteed to converge if all states continue to be selected.</div>
<div class="standard" id="magicparlabel-4658">We are going to see three approaches for ADP:</div>
<h5 class="paragraph" id="magicparlabel-4659"><span class="paragraph_label"></span> In-Place DP</h5>
<div class="standard" id="magicparlabel-4660">Before, with synchronous value iteration, we stored two copies of the value function:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow>
<mrow><mi>n</mi><mi>e</mi><mi>w</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi>a</mi>
</mrow>
</msub>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>,</mo>
</mrow>
</mrow></math><math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>v</mi><mo>=</mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow>
<mrow><mi>n</mi><mi>e</mi><mi>w</mi>
</mrow>
</mrow>
</msub><mn>.</mn>
</mrow>
</mrow></math></div>
<ol class="lyxlist" id="magicparlabel-4661"><li class="labeling_item"><span class="lyxlist">Now,</span>
 in-place value iteration stores only one copy of the value function, doing:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi>a</mi>
</mrow>
</msub>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mn>.</mn>
</mrow>
</mrow></math></li>
</ol>
<h5 class="paragraph" id="magicparlabel-4662"><span class="paragraph_label"></span> Prioritised Sweeping</h5>
<div class="standard" id="magicparlabel-4663">We can use the magnitude of the Bellman error to guide the state selection. For example:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>E</mi><mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">|</mo>
<mrow>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi>a</mi>
</mrow>
</msub>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>-</mo><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">|</mo>
<mn>.</mn>
</mrow>
</mrow></math> We then backup the state with the largest remaining Bellman error, and update the Bellman error of affected states after each backup. This reuires knowledge of reverse dynamics (which are the predecessor states). It can be implemented efficiently with a priority queue.</div>
<h5 class="paragraph" id="magicparlabel-4664"><span class="paragraph_label"></span> Real-Time DP</h5>
<div class="standard" id="magicparlabel-4665">The idea in this case is to only update states that are relevant to the agent. For example, if the agent is in state <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow></math>, we update that state value, or the states that it expects to be in soon.</div>
<h5 class="paragraph" id="magicparlabel-4666"><span class="paragraph_label"></span> Full-Width backups</h5>
<div class="standard" id="magicparlabel-4667">Standard DP uses full-width backups. This means that, for each backup, it being synchronous or asynchronous:</div>
<ul class="itemize" id="magicparlabel-4668"><li class="itemize_item">Every successor state and action is considered.</li>
<li class="itemize_item">Using the true model of transitions and reward function.</li>
</ul>
<div class="standard" id="magicparlabel-4670">DP is effective for medium-sized problems (with millions of states). For large problems DP suffers from the curse of dimensionality, since the number of states grows exponentially with the number of state variables, and even one full backup can be too expensive.</div>
<h5 class="paragraph" id="magicparlabel-4671"><span class="paragraph_label"></span> Sample Backups</h5>
<div class="standard" id="magicparlabel-4672">This approach consists in using sample rewards and sample transitions <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>s</mi><mo>'</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow></math> instead of the reward function <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>R</mi>
</mrow></math>, and the transition dynamics, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>P</mi>
</mrow></math>. </div>
<div class="standard" id="magicparlabel-4673">It presents some advantages:</div>
<ul class="itemize" id="magicparlabel-4674"><li class="itemize_item">It's model free: knowledge about the MDP is not required.</li>
<li class="itemize_item">Breaks the curse of dimensionality through sampling.</li>
<li class="itemize_item">Cost of backup is constant, independent of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>n</mi><mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">|</mo>
<mi>S</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">|</mo>
</mrow>
</mrow></math>.</li>
</ul>
<h2 class="section" id="magicparlabel-4677"><span class="section_label">12</span> Model-Free Prediction</h2>
<h3 class="subsection" id="magicparlabel-4678"><span class="subsection_label">12.1</span> Monte Carlo Algorithms</h3>
<div class="standard" id="magicparlabel-4679">We can use experience samples to learn without a model. The direct sampling of episodes is called <b>Monte Carlo</b>, and this is a model-free approach, because we don't need knowledge about the MDP, only the samples.</div>
<h4 class="subsubsection" id="magicparlabel-4680"><span class="subsubsection_label">12.1.1</span> Monte Carlo Policy Evaluation</h4>
<div class="standard" id="magicparlabel-4681">We consider sequential decision problems, and our goal is to learn <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub>
</mrow></math> from episodes of experience under policy <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> π </mi>
</mrow></math>:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub><mo>,</mo><mn>...</mn><mo>,</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>k</mi>
</mrow>
</msub><mo> ∼ </mo><mi> π </mi><mn>.</mn>
</mrow>
</mrow></math> The <b>return</b> is the total discounted reward, for an episode ending at time <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>T</mi><mo>&lt;</mo><mi>t</mi>
</mrow>
</mrow></math>:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mn>...</mn><mo>+</mo>
<msup>
<mrow><mi> γ </mi>
</mrow>
<mrow>
<mrow><mi>T</mi><mo>-</mo><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msup>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msub><mn>.</mn>
</mrow>
</mrow></math> The <b>value function</b> is the expected return:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo><mi> π </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mn>.</mn>
</mrow>
</mrow></math> We could also use just the sample average return, instead of the expected return.</div>
<h4 class="subsubsection" id="magicparlabel-4682"><span class="subsubsection_label">12.1.2</span> First-Visit Monte Carlo Policy Evaluation</h4>
<div class="float-listings"><pre class="listings">def FV_MC_PE:
	# Initialization
	foreach s:
		N(s) = 0
		G(s) = 0

	# Computation
	loop:
		# Sample episode
		e_i = s[i,1], a[i,1], r[i,1], s[i,2], a[i,2], r[i,2], ..., s[i,T_i]

		# Compute return from time step t onwards in episode i
		G[i,t] = r[i,t] + gamma * r[i,t+1] + ... + gamma ** (T_i - 1) * r[i, T_i]

		foreach time step t till the end of episode i:
			s = e_i[i, t].state
			if not visited[s, i]:
				N(s) = N(s) + 1		# Increment total visits counter
				G(s) = G(s) + G[i,t]	# Increment total return
				V_pi(s) = G(s) / N(s) 	# Update estimate					</pre></div>
<div class="standard" id="magicparlabel-4707">Properties:</div>
<ul class="itemize" id="magicparlabel-4708"><li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msup>
<mrow><mi>V</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msup>
</mrow></math> is an unbiased estimator of the true <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
</mrow></math>.</li>
<li class="itemize_item">By the law of large numbers, as <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>N</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> → </mo><mi> ∞ </mi>
</mrow>
</mrow></math> we have <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>V</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msup><mo> → </mo>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
</mrow></math>.</li>
</ul>
<h4 class="subsubsection" id="magicparlabel-4710"><span class="subsubsection_label">12.1.3</span> Every-Visit Monte Carlo Policy Evaluation</h4>
<div class="float-listings"><pre class="listings">def EV_MC_PE:
	# Initialization
	foreach s:
		N(s) = 0
		G(s) = 0

	# Computation
	loop:
		# Sample episode
		e_i = s[i,1], a[i,1], r[i,1], s[i,2], a[i,2], r[i,2], ..., s[i,T_i]

		# Compute return from time step t onwards in episode i
		G[i,t] = r[i,t] + gamma * r[i,t+1] + ... + gamma ** (T_i - 1) * r[i, T_i]

		foreach time step t till the end of episode i:
			s = e_i[i, t].state
			N(s) = N(s) + 1		# Increment total visits counter
			G(s) = G(s) + G[i,t]	# Increment total return
			V_pi(s) = G(s) / N(s) 	# Update estimate					</pre></div>
<div class="standard" id="magicparlabel-4734">Properties:</div>
<ul class="itemize" id="magicparlabel-4735"><li class="itemize_item"><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msup>
<mrow><mi>V</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msup>
</mrow></math> is a biased estimator of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
</mrow></math>.</li>
<li class="itemize_item">But it is consistent and often with better MSE.</li>
</ul>
<h4 class="subsubsection" id="magicparlabel-4737"><span class="subsubsection_label">12.1.4</span> Incremental Monte Carlo Policy Evaluation</h4>
<div class="float-listings"><pre class="listings">def EV_MC_PE:
	# Initialization
	foreach s:
		N(s) = 0
		G(s) = 0

	# Computation
	loop:
		# Sample episode
		e_i = s[i,1], a[i,1], r[i,1], s[i,2], a[i,2], r[i,2], ..., s[i,T_i]

		# Compute return from time step t onwards in episode i
		G[i,t] = r[i,t] + gamma * r[i,t+1] + ... + gamma ** (T_i - 1) * r[i, T_i]

		for j=1:T_i:
			V_pi(s[j,t]) = V_pi(s[j,t]) + alpha * (G[j,t] - V_pi(s[j,t]))					</pre></div>
<div class="standard" id="magicparlabel-4758">These algorithms can be used to learn value predictions, but when episodes are long, the learning process can be slow, because we have to wait until an episode ends before we can learn. In addition, the return can have high variance. Therefore, it would be nice to have other methods.</div>
<h3 class="subsection" id="magicparlabel-4759"><span class="subsection_label">12.2</span> Temporal Difference Learning</h3>
<div class="standard" id="magicparlabel-4760">The core of TD learning is the temporal difference error, which measures the difference between the estimated value of the current state and the estimated value of the next state, combined with the reward received for transitioning between these states. This error guides the update of the value function.</div>
<div class="standard" id="magicparlabel-4761">Therefore, TD is model-free and learns directly from experience. It can also learn from incomplete episodes, by bootstrapping.</div>
<div class="standard" id="magicparlabel-4762">TD can learn during each episode, it does not need to complete it.</div>
<h4 class="subsubsection" id="magicparlabel-4763"><span class="subsubsection_label">12.2.1</span> Temporal Difference Learning by Sampling Bellman Equations</h4>
<div class="standard" id="magicparlabel-4764">Recall the Bellman equations,<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo> ∼ </mo><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>,</mo>
</mrow>
</mrow></math> which can be approximated by iterating<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow>
<mrow><mi>k</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi>k</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo> ∼ </mo><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mn>.</mn>
</mrow>
</mrow></math> But we can also sample this, as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math> However, this is likely to be very noisy, so it is better to take a small step,<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo>
<msub>
<mrow><mi> α </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mo>-</mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-4765">The red part is the target value, and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi> δ </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>-</mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> is called the <b>TD error</b>.</div>
<div class="standard" id="magicparlabel-4766">We can visualize the difference between the Dynamic Programming approach, the Monte Carlo approach and the Temporal Difference approach in the following figure:</div>
<div class="float-figure"><div class="plain_layout" id="magicparlabel-4771"><img alt="image: 63_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado25.png" src="63_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado25.png" style="width:40%;"/>
<img alt="image: 64_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado26.png" src="64_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado26.png" style="width:40%;"/>
<img alt="image: 65_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado27.png" src="65_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado27.png" style="width:40%;"/>
</div>
<div class="plain_layout" id="magicparlabel-4772"><span class="float-caption-Standard float-caption float-caption-standard">Figure 6:  Backups for the different methods. DP (left), MC (center), TD (right).</span></div>
</div>
<div class="standard" id="magicparlabel-4777">In the context of reinforcement learning, <b>bootstrapping</b> refers to a method where the current estimates are updated based on other estimated values, rather than solely on actual rewards and complete trajectories. <b>Sampling</b>, on the other side, refers to learning from actual experiences or interactions with the environment.</div>
<div class="standard" id="magicparlabel-4778" style="text-align: center;"><table><tbody><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4800">Method</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4803">Bootstrapping</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4806">Sampling</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4809">DP</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4812">Yes</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4815">No</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4818">MC</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4821">No</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4824">Yes</div>
</td>
</tr><tr><td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4827">TD</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4830">Yes</div>
</td>
<td align="center" valign="top">
<div class="plain_layout" id="magicparlabel-4833">Yes</div>
</td>
</tr></tbody>
</table>
</div>
<div class="standard" id="magicparlabel-4834">Note now that the idea of TD can also be applied to action values, and we can update the value <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> towards the estimated return <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> by<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ← </mo>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mi> α </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo><mi> γ </mi>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>-</mo>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math> This approach is known as <b>SARSA</b>, since it uses <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow></math>.</div>
<h3 class="subsection" id="magicparlabel-4835"><span class="subsection_label">12.3</span> Comparing MC and TD</h3>
<ul class="itemize" id="magicparlabel-4836"><li class="itemize_item">TD can learn before knowing the final outcome, online after every step, while MC must wait until the end of the episode before the return is known.</li>
<li class="itemize_item">TD can learn without of the final outcome. It can learn from incomplete sequences and it works it continuing (non-terminating) environments. MC, on the other hand, can only learn from complete sequences, and only works for episodic (terminating) environments.</li>
<li class="itemize_item">TD is independent of the temporal span of the prediction, being able to learn from single transitions, while MC must store all predictions to update at the end of an episode.</li>
<li class="itemize_item">TD needs reasonable value estimates.</li>
<li class="itemize_item">MC returns an unbiased estimate <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, while TD returns a biased estimate of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, but the TD target has lower variance, because the total return depends on many random actions, transitions and rewards, while the TD target depends on one random action, transition and reward.</li>
<li class="itemize_item">In some case, TD can have irreducible bias.</li>
<li class="itemize_item">When the world is partially observable, MC would implicitly account for all the latent variables.</li>
<li class="itemize_item">The function to approximate the values may fit poorly.</li>
<li class="itemize_item">In the tabular case, both MC and TD will converge to <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub>
</mrow></math>.</li>
</ul>
<h3 class="subsection" id="magicparlabel-4845"><span class="subsection_label">12.4</span> Batch Monte Carlo and Temporal Difference</h3>
<div class="standard" id="magicparlabel-4846">Tabular MC and TD converge as experience keeps going to infinity and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi> α </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo> → </mo><mn>0</mn>
</mrow>
</mrow></math>. But what about finite experience?</div>
<div class="standard" id="magicparlabel-4847">Consider a fixed batch of experience:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow><mi>e</mi><mi>p</mi><mi>i</mi><mi>s</mi><mi>o</mi><mi>d</mi><mi>e</mi><mspace width="10px"></mspace><mn>1</mn><mo>:</mo>
</mrow>
</mtd>
<mtd>
<mrow>
<msubsup>
<mrow><mi>S</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msubsup><mo>,</mo>
<msubsup>
<mrow><mi>A</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msubsup><mo>,</mo>
<msubsup>
<mrow><mi>R</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msubsup><mo>,</mo><mn>...</mn><mo>,</mo>
<msubsup>
<mrow><mi>S</mi>
</mrow>
<mrow>
<msub>
<mrow><mi>T</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msubsup>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<mi>⋮
    </mi>
</mtd>
<mtd><mrow></mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow><mi>e</mi><mi>p</mi><mi>i</mi><mi>s</mi><mi>o</mi><mi>d</mi><mi>e</mi><mspace width="10px"></mspace><mi>K</mi><mo>:</mo>
</mrow>
</mtd>
<mtd>
<mrow>
<msubsup>
<mrow><mi>S</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
<mrow><mi>K</mi>
</mrow>
</msubsup><mo>,</mo>
<msubsup>
<mrow><mi>A</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
<mrow><mi>K</mi>
</mrow>
</msubsup><mo>,</mo>
<msubsup>
<mrow><mi>R</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
<mrow><mi>K</mi>
</mrow>
</msubsup><mo>,</mo><mn>...</mn><mo>,</mo>
<msubsup>
<mrow><mi>S</mi>
</mrow>
<mrow>
<msub>
<mrow><mi>T</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
<mrow><mi>K</mi>
</mrow>
</msubsup>
</mrow>
</mtd>
</mtr>
</mtable></math> We can repeatedly sample each episode <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>k</mi><mo> ∈ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mn>1</mn><mo>,</mo><mi>K</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
</mrow>
</mrow></math> and apply MC or TD(0), which is equivalent to sampling from an empirical model.</div>
<div class="standard" id="magicparlabel-4848">MC converges to the best mean-square fit for the observed returns:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msubsup>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>k</mi><mo>=</mo><mn>1</mn>
</mrow>
</mrow>
<mrow><mi>K</mi>
</mrow>
</msubsup>
<msubsup>
<mrow><mo> ∑ </mo>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>=</mo><mn>1</mn>
</mrow>
</mrow>
<mrow>
<msub>
<mrow><mi>T</mi>
</mrow>
<mrow><mi>k</mi>
</mrow>
</msub>
</mrow>
</msubsup>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msubsup>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
<mrow><mi>k</mi>
</mrow>
</msubsup><mo>-</mo><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msubsup>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
<mrow><mi>k</mi>
</mrow>
</msubsup>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msup><mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-4849">TD converges to solution of the maximum likelihood Markov model, given the data.</div>
<div class="example" id="magicparlabel-4850"><div class="example_item"><span class="example_label">Example 12.1.</span>
Consider two states, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>A</mi>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>B</mi>
</mrow></math>, with no discounting, and the following 8 episodes of experience:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd><mi>A</mi>
</mtd>
<mtd>
<mrow><mo>:</mo><mn>0</mn><mo>,</mo><mi>B</mi><mo>:</mo><mn>0</mn>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mi>B</mi>
</mtd>
<mtd>
<mrow><mo>:</mo><mn>1</mn>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mi>B</mi>
</mtd>
<mtd>
<mrow><mo>:</mo><mn>1</mn>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mi>B</mi>
</mtd>
<mtd>
<mrow><mo>:</mo><mn>1</mn>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mi>B</mi>
</mtd>
<mtd>
<mrow><mo>:</mo><mn>1</mn>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mi>B</mi>
</mtd>
<mtd>
<mrow><mo>:</mo><mn>1</mn>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mi>B</mi>
</mtd>
<mtd>
<mrow><mo>:</mo><mn>1</mn>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd><mi>B</mi>
</mtd>
<mtd>
<mrow><mo>:</mo><mn>0</mn>
</mrow>
</mtd>
</mtr>
</mtable></math></div>
</div>
<div class="standard" id="magicparlabel-4851">What are <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>A</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>B</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>?</div>
<div class="standard" id="magicparlabel-4852">MC:</div>
<div class="standard" id="magicparlabel-4853">All episodes regarding <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>A</mi>
</mrow></math> have 0 reward, therefore<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>A</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mn>0.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-4854">Episodes regarding <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>B</mi>
</mrow></math> have 1 reward 75% of the time, so<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>B</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mn>0.75.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-4855">TD:</div>
<div class="standard" id="magicparlabel-4856">In this case, we observe that<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>p</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>=</mo><mi>B</mi><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>A</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mn>1</mn><mo>,</mo>
</mrow>
</mrow></math> that is, whenever we go through state <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>A</mi>
</mrow></math>, then we go to state <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>B</mi>
</mrow></math>. This means that the reward of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>A</mi>
</mrow></math> is the same as the reward for <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>B</mi>
</mrow></math>,<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>A</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>B</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mn>0.75.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-4857">TD exploits the Markov property, and so it can be helpful in fully-observable environments. On the other side, MC does not rely the Markov property, so it can be useful in partially-observable environments. </div>
<div class="standard" id="magicparlabel-4858">When the data is finite or we are approximating the functions, the solutions may differ.</div>
<h3 class="subsection" id="magicparlabel-4859"><span class="subsection_label">12.5</span> Multi-Step Temporal Difference</h3>
<div class="standard" id="magicparlabel-4860">TD uses value estimates, which can be innacurate. In addition, the information can propagate quite slowly. On the other side, the information propagates faster, but the updates are noisier. We can go in between the two methods!</div>
<div class="standard" id="magicparlabel-4861">The idea is by applying TD, but instead of doing just one step, we allow it to target <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>n</mi>
</mrow></math> steps into the future.</div>
<div class="standard" id="magicparlabel-4862">The returns, in this case, are:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mtable>
<mtr>
<mtd>
<mrow><mi>n</mi><mo>=</mo><mn>1</mn>
</mrow>
</mtd>
<mtd><mrow></mrow>
</mtd>
<mtd>
<mrow>
<msubsup>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mn>1</mn>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msubsup><mo>=</mo>
</mrow>
</mtd>
<mtd>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow><mi>n</mi><mo>=</mo><mn>2</mn>
</mrow>
</mtd>
<mtd><mrow></mrow>
</mtd>
<mtd>
<mrow>
<msubsup>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mn>2</mn>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msubsup><mo>=</mo>
</mrow>
</mtd>
<mtd>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>2</mn>
</mrow>
</mrow>
</msub><mo>+</mo>
<msup>
<mrow><mi> γ </mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msup><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>2</mn>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<mi>⋮
    </mi>
</mtd>
<mtd><mrow></mrow>
</mtd>
<mtd>
<mi>⋮
    </mi>
</mtd>
<mtd><mrow></mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<mrow><mi>n</mi><mo>=</mo><mi> ∞ </mi>
</mrow>
</mtd>
<mtd><mrow></mrow>
</mtd>
<mtd>
<mrow>
<msubsup>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> ∞ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msubsup><mo>=</mo>
</mrow>
</mtd>
<mtd>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>2</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mn>...</mn><mo>+</mo>
<msup>
<mrow><mi> γ </mi>
</mrow>
<mrow>
<mrow><mi>T</mi><mo>-</mo><mi>t</mi><mo>-</mo><mn>1</mn>
</mrow>
</mrow>
</msup>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msub>
</mrow>
</mtd>
</mtr>
</mtable></math>As we can see, for <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>n</mi><mo>=</mo><mn>1</mn>
</mrow>
</mrow></math> we obtain the regular TD, and for <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>n</mi><mo>=</mo><mi> ∞ </mi>
</mrow>
</mrow></math> we obtain MC.</div>
<div class="standard" id="magicparlabel-4863">The <b>learning process</b> is done by<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ← </mo><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo><mi> α </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msubsup>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>n</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</msubsup><mo>-</mo><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<h2 class="section" id="magicparlabel-4864"><span class="section_label">13</span> Model-Free Control</h2>
<div class="standard" id="magicparlabel-4865">Model-Free Control refers to learning optimal policies directly from experience (i.e., from interaction with the environment) without needing a model of the environment. In model-based control, you know the dynamics of the environment (transition probabilities and rewards), but in model-free control, you do not.</div>
<h3 class="subsection" id="magicparlabel-4866"><span class="subsection_label">13.1</span> Monte-Carlo Control</h3>
<div class="standard" id="magicparlabel-4867">In the previous chapter, we focused on how to estimate the value function, given a policy. Now, we are going to try to discover new policies using these estimations. Monte-Carlo control is based on estimating the value of actions through sampling. This means that the agent tries out actions and observe the outcomes, and learns from these.</div>
<div class="standard" id="magicparlabel-4868">When we have a model, we saw how we can use a greedy policy improvement over <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>, as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> π </mi><mo>'</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo> arg </mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi>a</mi>
</mrow>
</msub>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi><mi>v</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mn>.</mn>
</mrow>
</mrow></math> However, when we don't have the model, we cannot use this approach. Instead, we can use a greedy policy improvement over the action-value function, as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> π </mi><mo>'</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo> arg </mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi>a</mi>
</mrow>
</msub>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow>
<mrow><mi> π </mi><mo>'</mo>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> which makes the action-value function very convenient for this problem.</div>
<div class="standard" id="magicparlabel-4869">The idea, then, would be something like:</div>
<ul class="itemize" id="magicparlabel-4870"><li class="itemize_item">Policy evaluation: Monte-Carlo policy evaluation, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>q</mi><mo> ∼ </mo>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub>
</mrow>
</mrow></math>. That is, from state <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow></math>, we try some actions, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo><mn>...</mn><mo>,</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mi>k</mi>
</mrow>
</msub>
</mrow>
</mrow></math>. and estimate <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo><mn>...</mn><mo>,</mo><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mi>k</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>.</li>
<li class="itemize_item">Policy improvement: Finding a better policy, given the value function estimated. Here, the policy is improved by making it greedy with respect to the estimated action-value function (<math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>q</mi>
</mrow></math>). This is:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mo> arg </mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>...</mn><mo>,</mo><mi>k</mi>
</mrow>
</mrow>
</msub><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></li>
</ul>
<div class="standard" id="magicparlabel-4872">But there is also a problem with this approach! We are learning by interacting with the environment, so we cannot sample all states <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>s</mi>
</mrow></math> and actions <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>a</mi>
</mrow></math>: since learning is from interaction, not all state-action pairs may be visited, which makes it difficult to estimate their values accurately. This is particularly a problem in environments with a large number of states or actions.</div>
<div class="standard" id="magicparlabel-4873">We find a trade-off between <b>exploitation</b> (use the information we already know) and <b>exploration</b> (try new actions to discover more about the environment). One approach that increases the exploration is the <b><math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> ε </mi>
</mrow></math>-greedy approach</b>:</div>
<ul class="itemize" id="magicparlabel-4874"><li class="itemize_item">With probability <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mn>1</mn><mo>-</mo><mi> ε </mi>
</mrow>
</mrow></math>, select greedy action,<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>a</mi><mo>=</mo><mo> arg </mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow>
<mrow><mi>a</mi><mo> ∈ </mo>
<mstyle mathvariant="script"><mi>A</mi>
</mstyle>
</mrow>
</mrow>
</msub><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></li>
<li class="itemize_item">With probability <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> ε </mi>
</mrow></math>, select a random action.</li>
</ul>
<div class="standard" id="magicparlabel-4876">This can be written, for each action <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>a</mi>
</mrow></math>, as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi> π </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>a</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mtable>
<mtr>
<mtd>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>1</mn><mo>-</mo><mi> ε </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo>
<mfrac>
<mrow><mi> ε </mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">|</mo>
<mstyle mathvariant="script"><mi>A</mi>
</mstyle>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">|</mo>
</mrow>
</mfrac>
</mrow>
</mtd>
<mtd>
<mrow><mi>i</mi><mi>f</mi><mspace width="10px"></mspace><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi>b</mi>
</mrow>
</msub><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo><mi>b</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd>
<mfrac>
<mrow><mi> ε </mi>
</mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">|</mo>
<mstyle mathvariant="script"><mi>A</mi>
</mstyle>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">|</mo>
</mrow>
</mfrac>
</mtd>
<mtd>
<mrow><mi>o</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>w</mi><mi>i</mi><mi>s</mi><mi>e</mi>
</mrow>
</mtd>
</mtr>
</mtable><mn>.</mn>
</mrow>
</mrow></math> This approach keeps exploring and obtaining new information.</div>
<div class="standard" id="magicparlabel-4877">We can write the model-free control as the following algorithm:</div>
<ul class="itemize" id="magicparlabel-4878"><li class="itemize_item">Repeat:

<ul class="itemize" id="magicparlabel-4879"><li class="itemize_item">Sample episode <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mn>1</mn><mo>,</mo><mn>...</mn><mo>,</mo><mi>k</mi><mo>,</mo><mn>...</mn>
</mrow>
</mrow></math> using <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> π </mi><mo>:</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub><mo>,</mo><mn>...</mn><mo>,</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>T</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">}</mo>
<mo> ∼ </mo><mi> π </mi>
</mrow>
</mrow></math>.</li>
<li class="itemize_item">For each state <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow></math> and action <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow></math> in the episode, do<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ← </mo><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo>
<msub>
<mrow><mi> α </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>-</mo><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></li>
<li class="itemize_item">Improve policy:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> ε </mi><mo> ← </mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mi>k</mi>
</mrow>
</mfrac><mo>,</mo>
</mrow>
</mrow></math><math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> π </mi><mo> ← </mo><mi> ε </mi><mo>-</mo><mi>g</mi><mi>r</mi><mi>e</mi><mi>e</mi><mi>d</mi><mi>y</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>q</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></li>
</ul>
</li></ul>
<div class="standard" id="magicparlabel-4882">Some possibilities for <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi> α </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow></math> are <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi> α </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow>
<mrow><mi>N</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</mfrac><mo>,</mo>
</mrow>
</mrow></math> the inverse of the amount of possible actions in state <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>t</mi>
</mrow></math>; or<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi> α </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mi>k</mi>
</mrow>
</mfrac><mo>,</mo>
</mrow>
</mrow></math> the inverse of the amount of episodes considered.</div>
<h4 class="subsubsection" id="magicparlabel-4883"><span class="subsubsection_label">13.1.1</span> Greedy in the Limit with Infinite Exploration (GLIE)</h4>
<div class="standard" id="magicparlabel-4884">GLIE is a strategy to ensure that the learning algorithm both explores the environment adequately and also converges to a near-optimal policy. </div>
<div class="standard" id="magicparlabel-4885"><em>Infinite Exploration</em> ensures that every state-action pair is explored infinitely often. In practical terms, this means that the learning algorithm never stops trying out new actions, even if it has already found actions that seem to give good results. This continuous exploration is crucial for making sure that the algorithm doesn't miss out on potentially better actions that haven't been tried enough. Infinite exploration is often implemented using strategies like <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> ε </mi>
</mrow></math>-greedy. In other words, all state-actions are explored infinitely many times:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mo> lim </mo>
</mrow>
<mrow>
<mrow><mi>t</mi><mo> → </mo><mi> ∞ </mi>
</mrow>
</mrow>
</msub>
<msub>
<mrow><mi>N</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi> ∞ </mi><mo>,</mo><mi> ∀ </mi><mi>s</mi><mo>,</mo><mi>a</mi><mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-4886"><em>Greedy in the Limit</em> means that as the learning process continues (i.e., as the number of iterations goes to infinity), the policy becomes increasingly greedy with respect to the estimated value function. In other words, over time, the policy relies more and more on the knowledge it has gained about the environment, and less on random exploration. This is typically achieved by gradually reducing the <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> ε </mi>
</mrow></math> parameter in an <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> ε </mi>
</mrow></math>-greedy policy. As <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> ε </mi>
</mrow></math> approaches zero, the policy becomes completely greedy, always choosing the action that it currently believes to be the best. This means that<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mo> lim </mo>
</mrow>
<mrow>
<mrow><mi>t</mi><mo> → </mo><mi> ∞ </mi>
</mrow>
</mrow>
</msub>
<msub>
<mrow><mi> π </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>a</mi><mo>|</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="script"><mi>I</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>a</mi><mo>=</mo><mo> arg </mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow>
<mrow><mi>a</mi><mo>'</mo>
</mrow>
</mrow>
</msub>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi><mo>'</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-4887">An example is using <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> ε </mi>
</mrow></math>-greedy with <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi> ε </mi>
</mrow>
<mrow><mi>k</mi>
</mrow>
</msub><mo>=</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mi>k</mi>
</mrow>
</mfrac>
</mrow>
</mrow></math>.</div>
<div class="standard" id="magicparlabel-4888"><div class="flex_color_box"><div class="theorem" id="magicparlabel-4892"><div class="theorem_item"><span class="theorem_label">Theorem 13.1.</span>
GLIE Model-Free Control converges to the optimal action-value funtion, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi>r</mi>
</mrow>
</msub><mo> → </mo>
<msup>
<mrow><mi>q</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup>
</mrow>
</mrow></math>.</div>
</div>
</div></div>
<h3 class="subsection" id="magicparlabel-4893"><span class="subsection_label">13.2</span> Temporal-Difference Learning for Control</h3>
<div class="standard" id="magicparlabel-4894">We saw that TD learning has several advantages over Monte-Carlo, as it shows lower variance, it learns online, and from incomplete sequences. Therefore, it is natural to think about using TD instead of MC for control. Instead of estimating <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> with MC, we do it with TD.</div>
<div class="standard" id="magicparlabel-4895">We can also update the action-value functions with SARSA, as studied in the previous section, by<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo>
<msub>
<mrow><mi> α </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>-</mo><mi>q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math> Once we evaluate <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>q</mi><mo> ∼ </mo>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub>
</mrow>
</mrow></math>, we improve the policy with the <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> ε </mi>
</mrow></math>-greedy approach.</div>
<div class="standard" id="magicparlabel-4896">More concretely, the following algorithm represents the approach called <b>tabular SARSA</b>:</div>
<div class="float-listings"><pre class="listings">initialize q(s,a) arbitrarily

for each episode:
	initialize s
	choose a from a using policy derived from q

	for each step in the episode:
		take action a
		observe r, s'
		choose a' from s' using policy derived from q
		
		q(s,a) &lt;- q(s,a) + alpha * [ r + gamma * q(s',a') - q(s,a) ]
		
		s &lt;- s'
		a &lt;- a'
	until s is terminal</pre></div>
<div class="standard" id="magicparlabel-4917"><div class="flex_color_box"><div class="theorem" id="magicparlabel-4921"><div class="theorem_item"><span class="theorem_label">Theorem 13.2.</span>
Tabular SARSA converges to the optimal aciton-value function, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>q</mi><mo> → </mo>
<msup>
<mrow><mi>q</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup>
</mrow>
</mrow></math>, if the policy is GLIE.</div>
</div>
</div></div>
<div class="standard" id="magicparlabel-4922">Observe why it is called Tabular SARSA: we store the action-value function as a matrix of size <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">|</mo>
<mstyle mathvariant="script"><mi>S</mi>
</mstyle>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">|</mo>
<mo> × </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">|</mo>
<mstyle mathvariant="script"><mi>A</mi>
</mstyle>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">|</mo>
</mrow>
</mrow></math>.</div>
<h3 class="subsection" id="magicparlabel-4923"><span class="subsection_label">13.3</span> Off-Policy Temporal-Difference and Q-Learning</h3>
<div class="standard" id="magicparlabel-4924"><div class="flex_color_box"><div class="definition" id="magicparlabel-4928"><div class="definition_item"><span class="definition_label">Definition 13.1.</span>
<b>On-Policy Learning</b> consists on learning about the behavior policy <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> π </mi>
</mrow></math> from experience, sampeld from <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> π </mi>
</mrow></math>.</div>
<div class="definition_item"><b>Off-Policy Learning</b> consists about learning the target policy <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> π </mi>
</mrow></math> from experiences sampled from <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> μ </mi>
</mrow></math>. It tries to learn counterfactually about other things the agent could do, by asking 'what if...?'.</div>
</div>
</div></div>
<div class="example" id="magicparlabel-4930"><div class="example_item"><span class="example_label">Example 13.1.</span>
What I turned left? <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> → </mo>
</mrow></math> New observations, rewards?</div>
<div class="example_item">What if I played more defensively? <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> → </mo>
</mrow></math> Do I change the win probability?</div>
<div class="example_item">What if I continued to go forward? <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo> → </mo>
</mrow></math> How long until I bump into a wall?</div>
</div>
<div class="standard" id="magicparlabel-4933">The approaches seen so far are on-policy learning, while off-policy learning approaches go as follows:</div>
<ul class="itemize" id="magicparlabel-4934"><li class="itemize_item">Evaluate target policy <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>a</mi><mo>|</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> to compute <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> or <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>.</li>
<li class="itemize_item">Use a behavior policy <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> μ </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>a</mi><mo>|</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> to generate actions.</li>
</ul>
<div class="standard" id="magicparlabel-4936">Now, why is this important? It is important because it enables an agent to learn by observing humans or other agents, and to re-use experience from old policies. In addition, the agent can learn several policies, while only following one. Finally, it learns about the greedy policy while following an exploratory policy.</div>
<div class="standard" id="magicparlabel-4937">A common approach is <b>Q-Learning</b>, which estimates the value of the greedy policy as<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo>
<msub>
<mrow><mi> α </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow>
<mrow><mi>a</mi><mo>'</mo>
</mrow>
</mrow>
</msub>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo><mi>a</mi><mo>'</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>-</mo>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-4938"><div class="flex_color_box"><div class="theorem" id="magicparlabel-4942"><div class="theorem_item"><span class="theorem_label">Theorem 13.3.</span>
Q-Learning control converges to the optimal action-value function, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>q</mi><mo> → </mo>
<msup>
<mrow><mi>q</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup>
</mrow>
</mrow></math>, as long as we take each action in each state infinitely often.</div>
</div>
</div></div>
<div class="remark" id="magicparlabel-4943"><div class="remark_item"><span class="remark_label">Remark 13.1.</span>
Note that Q-Learning achieves convergence without the need for greedy behavior. It works for any policy that eventually selects all actions sufficiently often.</div>
<div class="remark_item">It requires appropriately decaying step sizes, having <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
<msub>
<mrow><mi> α </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi> ∞ </mi>
</mrow>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
<msub>
<mrow><mi> α </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>&lt;</mo><mi> ∞ </mi>
</mrow>
</mrow></math>.</div>
<div class="remark_item">One possibility is using <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi> α </mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow>
<msup>
<mrow><mi>t</mi>
</mrow>
<mrow><mi> ω </mi>
</mrow>
</msup>
</mrow>
</mfrac>
</mrow>
</mrow></math>, with <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> ω </mi><mo> ∈ </mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mn>0.5</mn><mo>,</mo><mn>1</mn>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>.</div>
</div>
<div class="example" id="magicparlabel-4946"><div class="example_item"><span class="example_label">Example 13.2.</span>
In this picture we can observe how Q-Learning is more conservative than SARSA. SARSA manages to find the optimal path, but this is quite close to the cliff, and is therefore dangerous. Q-Learning finds a path that is less efficient, but is far from the cliff, minimizing the danger to fall off.</div>
<div class="standard" id="magicparlabel-4947" style="text-align: center;"><img alt="image: 66_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado48.png" src="66_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado48.png" style="width:60%;"/>
</div>
</div>
<h3 class="subsection" id="magicparlabel-4948"><span class="subsection_label">13.4</span> Overestimation in Q-Learning</h3>
<div class="standard" id="magicparlabel-4949">Classical Q-Learning has potential issues, due to the fact it uses the same values to select, and to evaluate; but these values are approximate, so the maximizing nature of the approach makes it more likely to select overestimated values, and less likely to select understimated values, causing <b>upward bias</b>.</div>
<div class="example" id="magicparlabel-4950"><div class="example_item"><span class="example_label">Example 13.3.</span>
Roulette example</div>
<div class="example_item">There are 171 actions: bet 1€ on one of 170 options, or stop. </div>
<div class="example_item">Stops ends the episode, with 0€. All other actions have high variance reward, with negative expected value, and betting actions do not end the episode, instead enables the agent to bet again.</div>
<div class="example_item">In the following graph, we observe how Q-Learning overestimates the expected profit in this problem:</div>
<div class="standard" id="magicparlabel-4954" style="text-align: center;"><img alt="image: 67_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado49.png" src="67_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado49.png" style="width:60%;"/>
</div>
</div>
<div class="standard" id="magicparlabel-4955">The solution to this problem is to decouple selection from evaluation: <b>double Q-Learning</b>. The idea is to store two action-value functions, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>q</mi>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>q</mi><mo>'</mo>
</mrow>
</mrow></math>, and update the reward estimates using them both:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>'</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo><mo> arg </mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi>a</mi>
</mrow>
</msub>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math><math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mi> γ </mi>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo><mo> arg </mo>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi>a</mi>
</mrow>
</msub>
<msub>
<mrow><mi>q</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>'</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-4956">At each <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>t</mi>
</mrow></math>, we pick <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>q</mi>
</mrow></math> or <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>q</mi><mo>'</mo>
</mrow>
</mrow></math> (randomly or with some design) and update using one of the two previous formulas, depending on which one we chose.</div>
<div class="standard" id="magicparlabel-4957">We can alse use both to act, for example by using the policy <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mfrac>
<mrow>
<mrow><mi>q</mi><mo>+</mo><mi>q</mi><mo>'</mo>
</mrow>
</mrow>
<mrow><mn>2</mn>
</mrow>
</mfrac>
</mrow></math>.</div>
<div class="standard" id="magicparlabel-4958">Double Q-Learning also converges to the optimal policy under the same conditions as Q-Learning, and solves the overestimation problem.</div>
<div class="standard" id="magicparlabel-4959">Moreover, this idea can be generalized to other update approaches, like <b>double SARSA</b>.</div>
<h3 class="subsection" id="magicparlabel-4960"><span class="subsection_label">13.5</span> Importance of Sampling Corrections</h3>
<div class="standard" id="magicparlabel-4961">Off-Policy Learning is basically trying to solve the following problem:</div>
<div class="standard" id="magicparlabel-4962">Given some function <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>f</mi>
</mrow></math>, with random inputs <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>X</mi>
</mrow></math> and a distribution <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>d</mi><mo>'</mo>
</mrow>
</mrow></math>, estimate the expectation of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>X</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> under a different distribution <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>d</mi>
</mrow></math> (the target distribution).</div>
<div class="standard" id="magicparlabel-4963">This can be solved by weighting the data using the ratio <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mfrac>
<mrow><mi>d</mi>
</mrow>
<mrow>
<mrow><mi>d</mi><mo>'</mo>
</mrow>
</mrow>
</mfrac>
</mrow></math>:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>x</mi><mo> ∼ </mo><mi>d</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>=</mo><mo> ∑ </mo><mi>d</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo> ∑ </mo><mi>d</mi><mo>'</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mfrac>
<mrow>
<mrow><mi>d</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
<mrow>
<mrow><mi>d</mi><mo>'</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</mfrac><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>x</mi><mo> ∼ </mo><mi>d</mi><mo>'</mo>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<mfrac>
<mrow>
<mrow><mi>d</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
<mrow>
<mrow><mi>d</mi><mo>'</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</mfrac><mi>f</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>x</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mn>.</mn>
</mrow>
</mrow></math> The intuition is that we scale up events that are rare under <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>d</mi><mo>'</mo>
</mrow>
</mrow></math>, but common under <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>d</mi>
</mrow></math>, and scale down events that are common under <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>d</mi><mo>'</mo>
</mrow>
</mrow></math>, but rare under <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>d</mi>
</mrow></math>.</div>
<div class="example" id="magicparlabel-4964"><div class="example_item"><span class="example_label">Example 13.4.</span>
Estimate one-step reward, with behavior <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> μ </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>a</mi><mo>|</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>. Then<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo> ∼ </mo><mi> π </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>=</mo>
<msub>
<mrow><mo> ∑ </mo>
</mrow>
<mrow><mi>a</mi>
</mrow>
</msub><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>a</mi><mo>|</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mi>r</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mo> ∑ </mo><mi> μ </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>a</mi><mo>|</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mfrac>
<mrow>
<mrow><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>a</mi><mo>|</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
<mrow>
<mrow><mi> μ </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>a</mi><mo>|</mo><mi>s</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</mfrac><mi>r</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow>
<mfrac>
<mrow>
<mrow><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
<mrow>
<mrow><mi> μ </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</mfrac>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo><mi>s</mi><mo>,</mo>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo> ∼ </mo><mi> μ </mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mn>.</mn>
</mrow>
</mrow></math> Therefore, when following policy <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> μ </mi>
</mrow></math>, we can use <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mfrac>
<mrow>
<mrow><mi> π </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
<mrow>
<mrow><mi> μ </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>A</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>|</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow>
</mfrac>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow>
</mrow></math> as unbiased sample.</div>
</div>
<h2 class="section" id="magicparlabel-4965"><span class="section_label">14</span> Value approximation &amp; Deep Reinforcement Learning</h2>
<h3 class="subsection" id="magicparlabel-4966"><span class="subsection_label">14.1</span> Approximate Model-Free Prediction</h3>
<div class="standard" id="magicparlabel-4967">Tabular RL does not scale to large, complex problems, because there are too many states to store in memory, and it is too slow to learn the values of each state separately. Therefore, we need to generalise what we learn across states.</div>
<div class="standard" id="magicparlabel-4968">We can estimate values or policies in an approximate way:</div>
<ol class="enumerate" id="magicparlabel-4969"><li class="enumerate_item">Map states <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>s</mi>
</mrow></math> onto a suitable feature representation <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> φ </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>.</li>
<li class="enumerate_item">Map features to values through a parametrised funciton <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi> φ </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>.</li>
<li class="enumerate_item">Update parameters <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> θ </mi>
</mrow></math> so that <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ∼ </mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi> φ </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>s</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>.</li>
</ol>
<div class="standard" id="magicparlabel-4972">The <b>goal</b> is then to find <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> θ </mi>
</mrow></math> that minimises the difference between <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub>
</mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub>
</mrow></math>,<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>L</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>S</mi><mo> ∼ </mo><mi>d</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>S</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>-</mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>S</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msup>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mo>,</mo>
</mrow>
</mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>d</mi>
</mrow></math> is the state visitation distribution induced by <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi> π </mi>
</mrow></math> and the dynamics <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>p</mi>
</mrow></math>.</div>
<div class="standard" id="magicparlabel-4973">For this, we can use gradient descent to iteratively minimise this objective function:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> Δ </mo><mi> θ </mi><mo>=</mo><mo>-</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>2</mn>
</mrow>
</mfrac><mi> α </mi>
<msub>
<mrow><mi> ∇ </mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mi>L</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi> θ </mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo><mi> α </mi>
<msub>
<mrow>
<mstyle mathvariant="double-struck"><mi>E</mi>
</mstyle>
</mrow>
<mrow>
<mrow><mi>S</mi><mo> ∼ </mo><mi>d</mi>
</mrow>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">[</mo>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> π </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>S</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>-</mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>S</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mi> ∇ </mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>S</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">]</mo>
<mn>.</mn>
</mrow>
</mrow></math> The evaluation of the expectation is going to be a problem, and therefore the solution is to use stochastic gradient descent, sampling the gradient update:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> Δ </mo><mi> θ </mi><mo>=</mo><mi> α </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>-</mo>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mi> ∇ </mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub>
</mrow></math> is a suitable sampled estimate of the return.</div>
<div class="standard" id="magicparlabel-4974">For Monte Carlo Prediction, it is<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>+</mo><mi> γ </mi>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>+</mo>
<msup>
<mrow><mi> γ </mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msup>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>2</mn>
</mrow>
</mrow>
</msub><mo>+</mo><mn>...</mn>
</mrow>
</mrow></math> and for Temporal Difference Prediction it is<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>G</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>=</mo>
<msub>
<mrow><mi>R</mi>
</mrow>
<mrow><mi>t</mi>
</mrow>
</msub><mo>+</mo><mi> γ </mi>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<msub>
<mrow><mi>S</mi>
</mrow>
<mrow>
<mrow><mi>t</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mn>.</mn>
</mrow>
</mrow></math></div>
<div class="standard" id="magicparlabel-4975">At this point, it is pretty evident that we can use Deep Learning in this setting!</div>
<h3 class="subsection" id="magicparlabel-4976"><span class="subsection_label">14.2</span> Deep Reinforcement Learning</h3>
<div class="standard" id="magicparlabel-4977">Before, the feature representation was typically fixed, and the parametrized function was just a linear mapping. We will consider more complicated non-linear mappings <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub>
</mrow></math>.</div>
<div class="standard" id="magicparlabel-4978">A popular choice is to use DNNs to parameterize such mapping. These are known to discover useful feature representation tailored to the specific task, and we can leverage extensive research on architectures and optimisation from DL.</div>
<div class="standard" id="magicparlabel-4979">Therefore, we are going to parameterize <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub>
</mrow></math> using a DNN. For instance, as a MLP:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>S</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<msub>
<mrow><mi>W</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub><mo> ⋅ </mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>W</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo> ⋅ </mo><mi>S</mi><mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>+</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub><mo>,</mo>
</mrow>
</mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> θ </mi><mo>=</mo><mo fence="true" form="prefix" stretchy="true" symmetric="true">{</mo>
<mrow>
<msub>
<mrow><mi>W</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>1</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>W</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>b</mi>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">}</mo>
</mrow>
</mrow></math>.</div>
<div class="standard" id="magicparlabel-4980">When <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub>
</mrow></math> is linear, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi> ∇ </mi>
<msub>
<mrow><mi>v</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub>
</mrow>
</mrow></math> is trivial to compute. However, now we have to use automatic differentiation with back-propagation implemented in DL frameworks.</div>
<div class="standard" id="magicparlabel-4981"><div class="flex_color_box"><div class="plain_layout" id="magicparlabel-4990">DeepMind was able to develop a Deep Reinforcement Learning system that obtained human performance in all classic Atari games. The did it by using function approximation to help scale ip to making decisions in very large domains.</div>
</div></div>
<div class="standard" id="magicparlabel-4991">To achieve such thing, they developed the concept of <b>Deep Q-Networks (DQN)</b> which consists on neural networks that are used to represents the value function, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>Q</mi>
</mrow></math>, policies and models, optimising a loss function by SGD.</div>
<div class="standard" id="magicparlabel-4992">Therefore, we represent the state-action value function by a DQN with weights <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>w</mi>
</mrow></math>:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<mover>
<mrow><mi>Q</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi><mo>;</mo><mi>w</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ≈ </mo><mi>Q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> for which we:</div>
<ol class="enumerate" id="magicparlabel-4993"><li class="enumerate_item">Map the input state, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>s</mi>
</mrow></math>, to a <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>h</mi>
</mrow></math>-dimensional vector.</li>
<li class="enumerate_item">Apply a non-linear transformation, like <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi>
</mrow>
</mrow></math> or <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi>
</mrow>
</mrow></math>.</li>
<li class="enumerate_item">Map the hidden vector to the <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>m</mi>
</mrow></math> possible action values, as a classification problem. </li>
<li class="enumerate_item">The action with greatest approximated <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>Q</mi>
</mrow></math>-value is selected.</li>
</ol>
<div class="standard" id="magicparlabel-4997">It is also possible to pass each action to the network, and output its <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>Q</mi>
</mrow></math>-value. This is useful when there are too many actions or the action space is continuous.</div>
<div class="standard" id="magicparlabel-4998"><div class="flex_color_box"><div class="plain_layout" id="magicparlabel-5007">The performed end-to-end learning of the values <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>Q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> from the states, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>s</mi>
</mrow></math>, represented by the pixels of the game of the last 4 frames of the game.</div>
<div class="plain_layout" id="magicparlabel-5008">The output is the value <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>Q</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> for the 18 possible button positions.</div>
<div class="plain_layout" id="magicparlabel-5009">The reward is a change in the score for that step.</div>
<div class="plain_layout" id="magicparlabel-5010">The network architecture and hyperparameters were the same for all the games of the console!</div>
<div class="plain_layout" id="magicparlabel-5011" style="text-align: center;"><img alt="image: 68_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado67.png" src="68_home_runner_work_BDMA_Notes_BDMA_Notes_Paris___chine-Learning_LectureNotes_source_pegado67.png" style="width:60%;"/>
</div>
</div></div>
<div class="standard" id="magicparlabel-5012">We saw in previous sections that <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>Q</mi>
</mrow></math>-learning converges to the optimal <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msup>
<mrow><mi>Q</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> using table lookup representation. When working with value function approximation, we can minimize the MSE loss by SGD using a target <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>Q</mi>
</mrow></math> estimate instead of the true <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>Q</mi>
</mrow></math>, like in linear value function approximation.</div>
<div class="standard" id="magicparlabel-5013">However, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>Q</mi>
</mrow></math>-learning with value function approximation can diverge from the optimal <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<msup>
<mrow><mi>Q</mi>
</mrow>
<mrow><mo>*</mo>
</mrow>
</msup>
</mrow></math>, due to mainly the correlations between samples, and the existence of non-stationary targets.</div>
<div class="standard" id="magicparlabel-5014">DQN addresses these challenges by leveraging <b>experience replay</b> and <b>fixed <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>Q</mi>
</mrow></math>-targets</b>.</div>
<h4 class="subsubsection" id="magicparlabel-5015"><span class="subsubsection_label">14.2.1</span> Experience Replay</h4>
<div class="standard" id="magicparlabel-5016">To help remove correlations, we can store a dataset, called the <b>replay buffer</b>, <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mstyle mathvariant="script"><mi>D</mi>
</mstyle>
</mrow></math>, obtained from prior experience. In this dataset, we store at each step <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>i</mi>
</mrow></math>, the tuple <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>a</mi>
</mrow>
<mrow><mi>i</mi>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>r</mi>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub><mo>,</mo>
<msub>
<mrow><mi>s</mi>
</mrow>
<mrow>
<mrow><mi>i</mi><mo>+</mo><mn>1</mn>
</mrow>
</mrow>
</msub>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow></math>, that is:</div>
<ul class="itemize" id="magicparlabel-5017"><li class="itemize_item">The initial state.</li>
<li class="itemize_item">The action taken.</li>
<li class="itemize_item">The reward.</li>
<li class="itemize_item">The obtained state.</li>
</ul>
<div class="standard" id="magicparlabel-5021">Once we have the dataset, we would perform experience replay as follows:</div>
<ol class="enumerate" id="magicparlabel-5022"><li class="enumerate_item">Sample an experience tuple from the replay buffer:.<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>s</mi><mo>'</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo> ∼ </mo>
<mstyle mathvariant="script"><mi>D</mi>
</mstyle><mn>.</mn>
</mrow>
</mrow></math></li>
<li class="enumerate_item">Compute the target value for the sampled <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mi>s</mi>
</mrow></math> as <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>r</mi><mo>+</mo><mi> γ </mi>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow>
<mrow><mi>a</mi><mo>'</mo>
</mrow>
</mrow>
</msub>
<mover>
<mrow><mi>Q</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>'</mo><mo>,</mo><mi>a</mi><mo>'</mo><mo>;</mo><mi>w</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>.</li>
<li class="enumerate_item">Use SGD to update the network weights:<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mo> Δ </mo><mi>w</mi><mo>=</mo><mi> α </mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>r</mi><mo>+</mo><mi> γ </mi>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow>
<mrow><mi>a</mi><mo>'</mo>
</mrow>
</mrow>
</msub>
<mover>
<mrow><mi>Q</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>'</mo><mo>,</mo><mi>a</mi><mo>'</mo><mo>;</mo><mi>w</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>-</mo>
<mover>
<mrow><mi>Q</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi><mo>;</mo><mi>w</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<msub>
<mrow><mi> ∇ </mi>
</mrow>
<mrow><mi>w</mi>
</mrow>
</msub>
<mover>
<mrow><mi>Q</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi><mo>;</mo><mi>w</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>,</mo>
</mrow>
</mrow></math> obtained from the loss function<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow><mi>L</mi><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mi>w</mi>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>=</mo>
<mfrac>
<mrow><mn>1</mn>
</mrow>
<mrow><mn>2</mn>
</mrow>
</mfrac>
<msup>
<mrow><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>r</mi><mo>+</mo><mi> γ </mi>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow>
<mrow><mi>a</mi><mo>'</mo>
</mrow>
</mrow>
</msub>
<mover>
<mrow><mi>Q</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>'</mo><mo>,</mo><mi>a</mi><mo>'</mo><mo>;</mo><mi>w</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
<mo>-</mo>
<mover>
<mrow><mi>Q</mi>
</mrow><mo stretchy="true">ˆ</mo>
</mover><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi><mo>;</mo><mi>w</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
<mrow><mn>2</mn>
</mrow>
</msup><mn>.</mn>
</mrow>
</mrow></math></li>
</ol>
<h3 class="subsection" id="magicparlabel-5025"><span class="subsection_label">14.3</span> Deep Learning aware Reinforcement Learning</h3>
<div class="standard" id="magicparlabel-5026">As always, things are not all good, and there exist issues with online deep RL. We know from DL literature that</div>
<ul class="itemize" id="magicparlabel-5027"><li class="itemize_item">SGD assumes gradients are sampled IID.</li>
<li class="itemize_item">Using mini-batches instead of single samples is typically better.</li>
</ul>
<div class="standard" id="magicparlabel-5029">However, in online RL, we perform an update on every new sample, and consecutive updates are strongly correlated.</div>
<div class="standard" id="magicparlabel-5030">We can be smart in the design of the method to improve it. For instance, Experience Replay helps with this, because we can mix online updates with updates on data sampled from the replay buffer. This approach can reduce correlation between consecutive updates, and enables mini-batch updates, instead of vanilla SGD.</div>
<div class="standard" id="magicparlabel-5031">Other ways to address this issue are:</div>
<ul class="itemize" id="magicparlabel-5032"><li class="itemize_item">Use better online algorithms, like eligibility traces.</li>
<li class="itemize_item">Use better optimizers, like with momentum.</li>
<li class="itemize_item">Planning with learnt models, like Dyna-Q.</li>
<li class="itemize_item">Changing the problem setting itself, like using parallel environments.</li>
</ul>
<h4 class="subsubsection" id="magicparlabel-5036"><span class="subsubsection_label">14.3.1</span> The Deadly Triad</h4>
<div class="standard" id="magicparlabel-5037">If we use experience replay, we are combining:</div>
<ol class="enumerate" id="magicparlabel-5038"><li class="enumerate_item">Function approximation: with the mere use of a neural network to fit the action values.</li>
<li class="enumerate_item">Bootstrapping: because we bootstrap on <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi>a</mi>
</mrow>
</msub>
<msub>
<mrow><mi>Q</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math> to construct the target.</li>
<li class="enumerate_item">Off-policy learning: the replay hold data from a mixture of past policies.</li>
</ol>
<div class="standard" id="magicparlabel-5041">When these three elements are combined, they can create a feedback loop where errors in the function approximation are compounded through bootstrapping, and the off-policy nature of the learning algorithm can lead to poor or incorrect updates. This can cause the learning algorithm to diverge, meaning that instead of converging to a solution, it becomes increasingly unstable over time.</div>
<div class="standard" id="magicparlabel-5042">Empirically, it is actually rare to see unbounded divergence. The most common scenarios are value explosions that recover after an initial phase, a phenomenon referred to as <b>soft-divergence</b>.</div>
<div class="standard" id="magicparlabel-5043">Soft-divergence still cause value estimates to be quite poor for extended periods... But we can address this in our RL agents using a separate <b>target network</b>, that is:</div>
<ul class="itemize" id="magicparlabel-5044"><li class="itemize_item">Hold fixed the parameters used to compute the bootstrap targets <math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
<mrow>
<msub>
<mrow><mo> max </mo>
</mrow>
<mrow><mi>a</mi>
</mrow>
</msub>
<msub>
<mrow><mi>Q</mi>
</mrow>
<mrow><mi> θ </mi>
</mrow>
</msub><mo fence="true" form="prefix" stretchy="true" symmetric="true">(</mo>
<mrow><mi>s</mi><mo>,</mo><mi>a</mi>
</mrow>
<mo fence="true" form="postfix" stretchy="true" symmetric="true">)</mo>
</mrow>
</mrow></math>.</li>
<li class="itemize_item">Only update them periodically (every few hundreds or thousands of updates).</li>
</ul>
<div class="standard" id="magicparlabel-5046">This way, we can break the feedback loop that sits at the heart of the deadly triad.</div>
<div class="standard" id="magicparlabel-5047">The general pseudocode of DQN is the following:</div>
<div class="float-listings"><pre class="listings Python">def DQN(C, alpha, gamma):
	D = {} #Replay buffer
	Initialize w #DQN
	w_ = w #Target network
	t = 0
	Get initial state s_0

	while True:
		sample action a_t given eps-greedy policy for current Q'(s_t,a;w)
		Observe reward r_t
		Observe next state s_t+1
		D.append(s_t,a_t,r_t,s_t+1)
		sample random mini-batch of tuples (s_i,a_i,r_i,s_i+1) from D
		
		for j in mini-batch do
			if episode terminated at step i+1 then
				y_i = r_i
			else
				y_i = r_i + gamma * max_a[Q'(s_i,a_i;w_)]
			
			do SGD step on (y_i - Q'(s_i,a_i;w))^2
			update w += dw
		t++
		if t % C == 0:
			w_ = w</pre></div>
<h4 class="subsubsection" id="magicparlabel-5077"><span class="subsubsection_label">14.3.2</span> Deep Double Q-Learning</h4>
<div class="standard" id="magicparlabel-5078"><br/>
</div>
<div class="standard" id="magicparlabel-5079"><h2 class="bibtex">References</h2><div class="bibtex"><div class="bibtexentry" id="LyXCite-Dupuis2023"><span class="bibtexlabel">1</span><span class="bibtexinfo"><span class="bib-fullnames:author">Tom Dupuis</span>, "<span class="bib-title">Machine Learning</span>".</span></div>
<div class="bibtexentry" id="LyXCite-radford2015"><span class="bibtexlabel">2</span><span class="bibtexinfo"><span class="bib-fullnames:author">Alec Radford, Luke Metz, and Soumith Chintala</span>, "<span class="bib-title">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</span>", <i></i>  (<span class="bib-year">2015</span>).</span></div>
</div></div>
</body>
</html>
