#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{footmisc}
\usepackage{listings}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1,0.99,0.94}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,  
    frame=single,
    tabsize=2
}

\lstset{style=mystyle}
\end_preamble
\use_default_options true
\begin_modules
tcolorbox
customHeadersFooters
theorems-ams-bytype
theorems-sec-bytype
algorithm2e
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "linkcolor=blue, urlcolor=blue, citecolor=blue, pdfstartview={FitH}, unicode=true"
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\boxbgcolor #62a0ea
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 3cm
\rightmargin 2cm
\bottommargin 3cm
\headheight 2cm
\headsep 1cm
\footskip 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Exam 2021
\end_layout

\begin_layout Exercise
Ordinal regression refers to a type of supervised learning problem where
 (discrete) taget labels show a natural ordering.
 For example, classifying wines in a 1-10 scale, or predicting customer
 satisfaction into one of excellent, good, average, or bad.
 Given the type of supervised learning problem that you have come accross
 during our course, what modifications or extensions can you think of that
 could solve the ordinal regression problem? You can mention how to alter
 learning algorithms, or cost functions, or anything you can think of.
\end_layout

\begin_deeper
\begin_layout Itemize
Extend Existing Algorithms: Many popular classification methods, such as
 logistic regression, can be extended to handle ordinal data.
 For instance, Ordinal Logistic Regression (also called Ordered Logit model)
 can be used when the response variable is ordinal in nature.
\end_layout

\begin_layout Itemize
Treat as Regression: Another method would be to treat the problem as a regressio
n problem.
 Since the labels are ordered, they can be treated as numerical data.
 However, caution should be taken since this assumes equal intervals between
 the classes which might not be the case in some datasets.
\end_layout

\begin_layout Itemize
Cost Functions: Design a cost function that takes the order of classes into
 account.
 For example, a misclassification from "good" to "bad" might be penalized
 more than a misclassification from "excellent" to "good".
 You may design a new loss function or modify an existing one to reflect
 this.
\end_layout

\begin_layout Itemize
Pairwise Comparison: You can transform the problem into binary classification
 tasks by comparing all pairs of classes.
\end_layout

\begin_layout Itemize
Cumulative Comparison: Another transformation option is to consider the
 problem as a set of binary classification tasks that discriminate between
 lower ranks and higher ranks.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Exercise
A data analyst has received a (small) set of expensive, labelled data and
 wants to build a good predictive classification model.
 She comes up with the following protocol: 
\end_layout

\begin_layout Enumerate
Split all available data into training, validation and test sets using 50/25/25
 proportions at random 
\end_layout

\begin_layout Enumerate
Train SVM model with default parameters and polynomial kernel of degree
 3 on the training set 
\end_layout

\begin_layout Enumerate
Train and optimize Random Forest, optimizing its hyper-parameters using
 OOB on the training set 
\end_layout

\begin_layout Enumerate
Choose best of SVM, and RF models using error on the validation set 
\end_layout

\begin_layout Enumerate
Estimate true error of selected model using test set 
\end_layout

\begin_layout Standard
Please criticize (in a constructive manner) this solution.
\end_layout

\begin_layout Standard
The proposed method could work well, but there are a few potential issues:
\end_layout

\begin_layout Itemize
Data Split: For a small dataset, it might be better to use a method like
 cross-validation rather than a static split.
 This can help the model see all the available data during training and
 validation phase.
\end_layout

\begin_layout Itemize
Parameter Tuning: SVM with a polynomial kernel of degree 3 is used with
 default parameters.
 It would be better to tune these parameters (like the C parameter for regulariz
ation) to ensure that the model is not underfitting or overfitting.
\end_layout

\begin_layout Itemize
Model Selection: Choosing the best model based solely on validation error
 might be a bit simplistic.
 Other metrics (like precision, recall, F1-score) could give a more rounded
 understanding of how the model performs.
\end_layout

\begin_layout Itemize
Hyperparameter Optimization: For Random Forest, optimizing hyperparameters
 using OOB error can be a good choice.
 However, it should be noted that hyperparameters should ideally also be
 validated using the validation set or cross-validation.
 The OOB error is a good rough estimate but it might not fully represent
 the model's ability to generalize.
\end_layout

\begin_layout Itemize
Model Bias: The protocol includes two algorithms only (SVM and RF).
 If the data characteristics favor other types of models (e.g.
 neural networks, gradient boosting machines etc.), they could be overlooked.
 Including a wider array of models for comparison might be beneficial.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Exercise
Explain in your own words the difference between the posterior and the predictiv
e distribution in Bayesian learning.
\end_layout

\begin_layout Exercise
In Bayesian learning, the posterior distribution and the predictive distribution
 play significant roles, but they serve different purposes:
\end_layout

\begin_layout Itemize
Posterior Distribution: The posterior distribution is the probability distributi
on of an unknown quantity, treated as a random variable, conditioned on
 the observed data.
 In Bayesian learning, it's used to represent our updated belief about the
 model parameters after we have seen the data.
 It combines our prior knowledge, expressed as the prior distribution, with
 the evidence provided by the observed data in the form of the likelihood
 function.
\end_layout

\begin_layout Itemize
Predictive Distribution: The predictive distribution is the probability
 distribution of a new, unobserved data point given the observed data.
 It integrates over all possible parameter values, each weighted by their
 posterior probability.
 In other words, it provides a distribution over possible values for new
 data points, taking into account parameter uncertainty.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Exercise
Please mark whether the following statements are true or false;
\end_layout

\begin_layout Itemize
Training error is always lower than test error: T (in general)
\end_layout

\begin_layout Itemize
Lasso and ridge regression both help in reducing overfitting: T
\end_layout

\begin_layout Itemize
Lasso regression is preferable to ridge regression because it produces sparse
 models: F (it produces sparse model, but this is not necessarily preferable)
\end_layout

\begin_layout Itemize
The activation functions of output neurons of a neural network are determined
 mainly by the nature of the target variable one wants to predict: T
\end_layout

\begin_layout Itemize
Linear regression assumes Gaussian input variables: F
\end_layout

\begin_layout Itemize
Naive Bayes assumes Gaussian input variables: F
\end_layout

\begin_layout Itemize
Bayes formula is used in Bayesian learning to obtain posterior distributions:
 T
\end_layout

\begin_layout Itemize
It is not possible to train a neural network for both regression and classificat
ion at the same time: F
\end_layout

\begin_layout Itemize
Bigger training sets help to reduce overfitting: T
\end_layout

\begin_layout Itemize
It is impossible to evaluate the quality of a clustering result because
 we never have the ground truth: F
\end_layout

\begin_layout Itemize
Cross-validation is a resampling method used to select a good model: T
\end_layout

\begin_layout Itemize
Gaussian Naive Bayes assumes Gaussian input variables: T
\end_layout

\begin_layout Itemize
Backprop is an algorithm used in neural network learning to obtain partial
 derivatives of an error function with respect to its weights: T
\end_layout

\begin_layout Itemize
The negative log-likelihood can always be used as an error function in supervise
d learning: F
\end_layout

\begin_layout Itemize
The EM algorithm is particularly suited to learn probabilistic models with
 partially observed data: T
\end_layout

\end_body
\end_document
