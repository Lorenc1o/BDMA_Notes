#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{footmisc}
\usepackage{listings}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1,0.99,0.94}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,  
    frame=single,
    tabsize=2
}

\lstset{style=mystyle}
\end_preamble
\use_default_options true
\begin_modules
tcolorbox
customHeadersFooters
theorems-ams-bytype
theorems-sec-bytype
algorithm2e
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "linkcolor=blue, urlcolor=blue, citecolor=blue, pdfstartview={FitH}, unicode=true"
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\boxbgcolor #62a0ea
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 3cm
\rightmargin 2cm
\bottommargin 3cm
\headheight 2cm
\headsep 1cm
\footskip 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Exam 2017
\end_layout

\begin_layout Standard
True or False:
\end_layout

\begin_layout Enumerate
Based on training data alone, there is no means of choosing which model
 is better: T
\end_layout

\begin_layout Enumerate
Complexity control is only necessary when data is high-dimensional: F
\end_layout

\begin_layout Enumerate
The empirical error in the training set is always smaller (or equal) than
 the empirical error in the test set: F (not guaranteed, but in most of
 the cases it is T)
\end_layout

\begin_layout Enumerate
Using a larger training data set reduces the chances to obtain an overfitted
 model: T
\end_layout

\begin_layout Enumerate
Regularization is intended to penalize models that are less complex than
 needed: F
\end_layout

\begin_layout Enumerate
Cross-validation is mainly used for model selection purposes: T
\end_layout

\begin_layout Enumerate
L2-regularization does not produce sparsity, as opposed to L1-regularization:
 T
\end_layout

\begin_layout Enumerate
Reducing the hypothesis (model) space is a way of controlling complexity:
 T
\end_layout

\begin_layout Enumerate
The VC dimension of a two-class linear classifier is a linear function of
 data dimension: T
\end_layout

\begin_layout Enumerate
The VC dimension of a two-class classifier is always a finite integer: T
\end_layout

\begin_layout Enumerate
The Bayes formula transforms prior distributions into posterior distributions:
 T
\end_layout

\begin_layout Enumerate
The numerator in Bayes formula is enough to perform classification, by taking
 simply the maximum over the classes: T
\end_layout

\begin_layout Enumerate
The Bayes classifier is the best possible classifier when the classes are
 Gaussian: F, unless we know the parameters of the distributions
\end_layout

\begin_layout Enumerate
The Bayes classifier is the best possible classifier when the true priors
 are known: T
\end_layout

\begin_layout Enumerate
For normally distributed classes, Bayesian classifiers turn out to be quadratic
 discriminant functions: T
\end_layout

\begin_layout Enumerate
For normally distributed classes, equal posterior probabilities yield linear
 discriminant functions: T
\end_layout

\begin_layout Enumerate
The Naive-Bayes classifier does not make assumptions about data distribution
 for continuous variables: F
\end_layout

\begin_layout Enumerate
The kNN classifier requires tuning of the number of neighbours, because
 we have a finite data sample: T
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\sum_{a}P\left(a|b\right)P\left(b\right)=P\left(b\right)$
\end_inset

 where A, B are discrete random variables: F
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\sum_{b}P\left(a|b\right)=\sum_{a}P\left(b|a\right)$
\end_inset

 where A, B are discrete random variables: F
\end_layout

\begin_layout Enumerate
The likelihood is a function of the parameters for a given choice of data
 sample: T
\end_layout

\begin_layout Enumerate
Logistic regression does not make assumptions about input data distribution:
 T
\end_layout

\begin_layout Enumerate
Linear regression assumes normally distributed outputs, conditioned on the
 inputs: T
\end_layout

\begin_layout Enumerate
In a GLM, the model tries to predict the expected value of the target using
 a linear function of the predictors and a suitable interface function:
 T
\end_layout

\begin_layout Enumerate
We can obtain an error function as the negative log-likelihood of a problem:
 T
\end_layout

\begin_layout Enumerate
The regression function is the best possible predictor, in the sense that
 it would achieve zero bias: F
\end_layout

\begin_layout Enumerate
The regression function is the best possible predictor, in the sense that
 it would achieve zero variance: F
\end_layout

\begin_layout Enumerate
The regression function is the best possible predictor, in the sense that
 it would achieve zero noise: F
\end_layout

\begin_layout Enumerate
In statistics, bias and variance are related concepts: they represent the
 distribution of errors in the training and test sets, respectively: F
\end_layout

\begin_layout Enumerate
The mean squared error is always preferred for GLM regression, because it
 is the only one that works in practice: F
\end_layout

\begin_layout Enumerate
An MLP needs no regularization, because backpropagation prevents arbitrary
 growth of the weights: F
\end_layout

\begin_layout Enumerate
We can convert a non-linear model into a linear one by giving values to
 the non-linear adaptive parameters: F
\end_layout

\begin_layout Enumerate
The backpropagation algorithm computes the partial derivatives of a given
 differentiable error function with respect to the network weights: T
\end_layout

\begin_layout Enumerate
The backpropagation algorithm must be coupled with an optimization method
 (update rule) to make it a learning algorithm for a neural network: T
\end_layout

\begin_layout Enumerate
The backpropagation algorithm is mainly used to compute the gradient vector
 of the error function at each step: T
\end_layout

\begin_layout Enumerate
The nature of the target variable dictates the activation function for the
 output neurons: T
\end_layout

\begin_layout Enumerate
The activation function for the hidden neurons could be a linear function
 to facilitate learning: F
\end_layout

\begin_layout Enumerate
Both RBF and MLP neural networks can have one or more hidden layers of neurons:
 T
\end_layout

\begin_layout Enumerate
An RBF neural network could in principle be trained with the backpropagation
 algorithm: T
\end_layout

\begin_layout Enumerate
Regularization makes little sense in neural networks, because they are non-linea
r models: F
\end_layout

\begin_layout Enumerate
The E-M algorithm refines a suboptimal solution obtained by k-means until
 a global optimum is found: F
\end_layout

\begin_layout Enumerate
A Gaussian mixture model assumes that the data has been generated by a “big”
 Gaussian that can be decomposed as a finite mixture: F (I think this statement
 is weird and unclear)
\end_layout

\begin_layout Enumerate
The k-means algorithm will discover the true clusters in the data, if given
 enough prototypes: F
\end_layout

\begin_layout Enumerate
Bagging methods are based on the fact that, for unstable learners, variance
 can be greatly reduced with little or no increase in bias: T
\end_layout

\begin_layout Enumerate
A Random Forest is “random” because decision trees are random learners (meaning
 that a single tree changes if we “execute” the algorithm again): F
\end_layout

\begin_layout Enumerate
A Random Forest is “random” because the data used in each decision node
 come from a different bootstrap resample: T
\end_layout

\begin_layout Enumerate
In Machine Learning, the lack of predictive variables can be compensated
 by more training data; in other words, there is no limit on the achievable
 predictive performance of a model, if we can gather enough data: F
\end_layout

\begin_layout Enumerate
We should optimize the number of folds in cross-validation, and separately
 for each modeling technique: F, we should decide this based on the size
 of the dataset at hand
\end_layout

\begin_layout Enumerate
In Machine Learning, better pre-processing can make a large impact on learning,
 and therefore on predictive performance: T
\end_layout

\begin_layout Enumerate
In a noiseless setting, at least theoretically speaking, there is no need
 for regularization: F, because we don't know the real generating function,
 so we can still be choosing a too complex model
\end_layout

\end_body
\end_document
