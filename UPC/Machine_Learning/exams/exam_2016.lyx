#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{footmisc}
\usepackage{listings}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1,0.99,0.94}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,  
    frame=single,
    tabsize=2
}

\lstset{style=mystyle}
\end_preamble
\use_default_options true
\begin_modules
tcolorbox
customHeadersFooters
theorems-ams-bytype
theorems-sec-bytype
algorithm2e
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "linkcolor=blue, urlcolor=blue, citecolor=blue, pdfstartview={FitH}, unicode=true"
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\boxbgcolor #62a0ea
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 3cm
\rightmargin 2cm
\bottommargin 3cm
\headheight 2cm
\headsep 1cm
\footskip 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Exam 2016
\end_layout

\begin_layout Standard
True or False:
\end_layout

\begin_layout Enumerate
Complexity control is necessary in non-linear methods only: F
\end_layout

\begin_layout Enumerate
The empirical error in the training set is always smaller (or equal) than
 the empirical error in the test set: T (in general)
\end_layout

\begin_layout Enumerate
The empirical error in the training set is always smaller (or equal) than
 the empirical error in the validation set: F (it depends, for example in
 LOOCV this does not generally holds)
\end_layout

\begin_layout Enumerate
Using a larger validation data set reduces the chances to select an overfitted
 model: T
\end_layout

\begin_layout Enumerate
Regularization usually penalizes models that are more complex than needed:
 T
\end_layout

\begin_layout Enumerate
Regularization may penalize models that are simpler than needed: F
\end_layout

\begin_layout Enumerate
Cross-validation guarantees that our model does not overfit the data: F
\end_layout

\begin_layout Enumerate
L2-regularization produces sparsity, as opposed to L1-regularization: F
\end_layout

\begin_layout Enumerate
The VC dimension of a two-class classifier is independent of data dimension:
 F
\end_layout

\begin_layout Enumerate
The VC dimension of a two-class classifier is always a finite integer: T
\end_layout

\begin_layout Enumerate
The Bayes formula transforms prior distributions into posterior distributions:
 T
\end_layout

\begin_layout Enumerate
The denominator in Bayes formula is enough to perform classification, by
 taking simply the maximum over the classes: F
\end_layout

\begin_layout Enumerate
The Bayes classifier is the best possible classifier when the prior and
 class-conditional distributions are known: T
\end_layout

\begin_layout Enumerate
For normally distributed classes, Bayesian classifiers turn out to be quadratic
 discriminant functions: T
\end_layout

\begin_layout Enumerate
For normally distributed classes, equal prior probabilities yield linear
 discriminant functions: F
\end_layout

\begin_layout Enumerate
The Naive-Bayes classifier can only be used with discrete random variables,
 because it assumes statistical independence among all variables, given
 the class: F
\end_layout

\begin_layout Enumerate
The kNN classifier needs no tuning of the number of neighbours, because
 in the limit of infinite data it is a Bayesian classifier: F
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\sum_{b}P\left(a|b\right)P\left(b\right)=1$
\end_inset

 where A, B are discrete random variables: F
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\sum_{b}P\left(b|a\right)=1$
\end_inset

 where A, B are discrete random variables: T
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\sum_{b}P\left(a|b\right)=1$
\end_inset

 where A, B are discrete random variables: F
\end_layout

\begin_layout Enumerate
The likelihood is a function of the data sample for a given choice of parameters
: F
\end_layout

\begin_layout Enumerate
The negative log-likelihood sometimes yields different results than the
 likelihood: F
\end_layout

\begin_layout Enumerate
Logistic regression is a linear method that can be used to predict an arbitrary
 numerical quantity: F
\end_layout

\begin_layout Enumerate
Linear regression assumes normally distributed inputs and outputs: F
\end_layout

\begin_layout Enumerate
In a GLM, the model tries to predict the expected value of the target using
 a linear function of the predictors and a suitable interface function:
 T
\end_layout

\begin_layout Enumerate
The solution for a GLM can be found analytically by minimizing the log-likelihoo
d, or iteratively using Newton-Raphson: T
\end_layout

\begin_layout Enumerate
In Poisson regression, we are interested in predicting positive outcomes
 that represent counts: T
\end_layout

\begin_layout Enumerate
The regression function is the best possible predictor, and would achieve
 zero error on the population: T
\end_layout

\begin_layout Enumerate
In statistics, bias and variance are related concepts: increasing one must
 increase the other, and viceversa (decreasing one must decrease the other):
 F
\end_layout

\begin_layout Enumerate
The mean squared error is always preferred for optimization, because it
 is the more theoretically sound: F
\end_layout

\begin_layout Enumerate
Non-linear functions of the data can be estimated by using linear fitting
 techniques: T
\end_layout

\begin_layout Enumerate
A linear combination of non-linear functions with adaptive parameters is
 a linear model: F
\end_layout

\begin_layout Enumerate
The backpropagation algorithm computes the partial derivatives of the given
 error function with respect to the network weights: T
\end_layout

\begin_layout Enumerate
The backpropagation algorithm must be coupled with an optimization method
 (update rule) to make it a learning algorithm for a MLP: T
\end_layout

\begin_layout Enumerate
Even if we fix the initial weights, a MLP is a non-deterministic method:
 F
\end_layout

\begin_layout Enumerate
The activation function for the output neurons is dictated by the nature
 of the target variable: T
\end_layout

\begin_layout Enumerate
A MLP requires the specification of the number of hidden neurons, which
 can be done in a variety of ways: T
\end_layout

\begin_layout Enumerate
RBF and MLP neural networks can be seen as a particular case of the same
 class of neural networks: ?
\end_layout

\begin_layout Enumerate
In a RBF neural network there is no regularization, because they are based
 on Euclidean distances instead of inner products: ?
\end_layout

\begin_layout Enumerate
Regularization does not make sense in neural networks, because they learn
 adaptive regressors (regressors with parameters): F
\end_layout

\begin_layout Enumerate
The k-means algorithm converges to a global optimum as the number of iterations
 goes to infinity: F
\end_layout

\begin_layout Enumerate
A Gaussian mixture model assumes that the data has been generated by some
 finite mixture of Gaussians: T
\end_layout

\begin_layout Enumerate
The k-means algorithm can be used to initialize a Gaussian mixture model:
 T
\end_layout

\begin_layout Enumerate
A Random Forest is “random” partly because the variables used in each decision
 tree are optimized amongst a randomly chosen subset: F
\end_layout

\begin_layout Enumerate
A Random Forest is “random” partly because the variables used in each decision
 node are optimized amongst a randomly chosen subset: T
\end_layout

\begin_layout Enumerate
A Random Forest is “random” partly because the data used in each decision
 tree come from a different bootstrap resample: T
\end_layout

\begin_layout Enumerate
A Random Forest is “random” partly because the data used in each decision
 node come from a different bootstrap resample: F
\end_layout

\begin_layout Enumerate
In Machine Learning, there is no limit on the achievable predictive performance
 of a model, it is just a matter of choosing the correct method, and tuning
 the parameters: F
\end_layout

\begin_layout Enumerate
In Machine Learning, pre-processing can make a large impact on learning,
 and therefore on predictive performance: T
\end_layout

\begin_layout Enumerate
A system (living or not) learns when it uses past experience to improve
 future performance: T
\end_layout

\end_body
\end_document
