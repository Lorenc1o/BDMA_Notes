#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{footmisc}
\usepackage{listings}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1,0.99,0.94}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,  
    frame=single,
    tabsize=2
}

\lstset{style=mystyle}
\end_preamble
\use_default_options true
\begin_modules
tcolorbox
customHeadersFooters
theorems-ams-bytype
theorems-sec-bytype
algorithm2e
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "linkcolor=blue, urlcolor=blue, citecolor=blue, pdfstartview={FitH}, unicode=true"
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\boxbgcolor #62a0ea
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 3cm
\rightmargin 2cm
\bottommargin 3cm
\headheight 2cm
\headsep 1cm
\footskip 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Exam 2015
\end_layout

\begin_layout Enumerate
If we keep complexity low, we do not need to care about training error:
 F
\end_layout

\begin_layout Enumerate
Training error is always smaller (or equal) than test error: T (in general,
 unless very scarce data and lucky shots)
\end_layout

\begin_layout Enumerate
Supplying more training data reduces the chances to obtain an overfitted
 model: T, in general
\end_layout

\begin_layout Enumerate
Regularization penalizes models that are either simpler or more complex
 than needed: F, only more complex
\end_layout

\begin_layout Enumerate
Training error is enough to perform model selection: F
\end_layout

\begin_layout Enumerate
The VC dimension for a two-class classifier penalizes training sample size:
 ?
\end_layout

\begin_layout Enumerate
The VC dimension for a two-class classifier is the maximum number of linear
 separations that the classifier can perform: ?
\end_layout

\begin_layout Enumerate
In order to check that the VC dimension is (at least) some integer k, we
 just need to find k points that can be shattered: ?
\end_layout

\begin_layout Enumerate
Checking that the VC dimension is infinite requires an infinite number of
 checks: ?
\end_layout

\begin_layout Enumerate
A two-class classifier with infinite VC dimension must have an infinite
 (or very large) number of parameters: ?
\end_layout

\begin_layout Enumerate
The Bayes formula converts prior distributions into posterior distributions:
 T
\end_layout

\begin_layout Enumerate
The Bayes formula is of theoretical importance, but can never be used in
 practice: F
\end_layout

\begin_layout Enumerate
The numerator in Bayes formula is enough to perform classification: T
\end_layout

\begin_layout Enumerate
The Bayes classifier is the best possible classifier when the prior and
 posterior distributions are known: T (the prior and the class-conditional
 are needed.
 But this is to compute the posterior, so if you already know the posterior,
 I guess it should also work)
\end_layout

\begin_layout Enumerate
For normally distributed classes, Bayesian classifiers turn out to be quadratic
 discriminant functions (QDA): T
\end_layout

\begin_layout Enumerate
For normally distributed classes, statistical independence among all variables
 yields linear discriminant functions (LDA): F
\end_layout

\begin_layout Enumerate
For normally distributed classes, Bayesian classifiers are minimum-distance
 classifiers: T
\end_layout

\begin_layout Enumerate
The Naive-Bayes classifier assumes statistical independence among all variables:
 T
\end_layout

\begin_layout Enumerate
The kNN classifier can be explained as a Bayesian classifier: F
\end_layout

\begin_layout Enumerate
The kNN classifier works better with more neighbours, although it is computation
ally more costly: F
\end_layout

\begin_layout Enumerate
The likelihood of a sample is its density for a given choice of parameters:
 T (kinda)
\end_layout

\begin_layout Enumerate
The likelihood of a sample is a function of the sample: F
\end_layout

\begin_layout Enumerate
Logistic regression is a generative linear classifier: F (it is discriminative)
\end_layout

\begin_layout Enumerate
Logistic regression assumes normally distributed classes: F
\end_layout

\begin_layout Enumerate
In a Generalized Linear Model, the prediction is the logistic function applied
 to a linear function of the predictors: F
\end_layout

\begin_layout Enumerate
The solution for Logistic regression can be found analytically by minimizing
 the log-likelihood: F
\end_layout

\begin_layout Enumerate
In Poisson regression, we are interested in predicting integer outcomes,
 which are equally likely: ?
\end_layout

\begin_layout Enumerate
In a Generalized Linear Model, we always find the logistic function in one
 way or another (so it is called the link function): F
\end_layout

\begin_layout Enumerate
In statistics, bias and variance are opposite concepts: increasing one must
 decrease the other: F
\end_layout

\begin_layout Enumerate
Variance always decreases with increasing sample size; however, bias can
 increase or stay the same: T
\end_layout

\begin_layout Enumerate
The regression function is the best possible model in regression, and achieves
 zero error on the training data: F
\end_layout

\begin_layout Enumerate
The risk is equal to the sum of the (squared) bias, the variance and the
 noise variance: T
\end_layout

\begin_layout Enumerate
The theoretical MSE does not depend on the regression function: F
\end_layout

\begin_layout Enumerate
Models that are “more complex than needed” will tend to have a large bias
 and large variance: F
\end_layout

\begin_layout Enumerate
Models that are “less complex than needed” will tend to have a small bias
 and small variance: F
\end_layout

\begin_layout Enumerate
A linear combination of non-linear (fixed) functions of the inputs make
 a linear model: F
\end_layout

\begin_layout Enumerate
Ridge regression adds a penalty term to a linear model such that the new
 model is non-linear: F
\end_layout

\begin_layout Enumerate
Non-linear functions of the data can be estimated by using linear fitting
 techniques: T
\end_layout

\begin_layout Enumerate
Both RBFs and MLPs create non-linear models by learning adaptive regressors
 (regressors with parameters): ?
\end_layout

\begin_layout Enumerate
Regularization allows the specification of models that are more complex
 than needed; it also helps numerically: F
\end_layout

\begin_layout Enumerate
The k-means algorithm converges to a global optimum if the number of iterations
 goes to infinity: F
\end_layout

\begin_layout Enumerate
A Gaussian mixture model assumes normality of the training data: F
\end_layout

\begin_layout Enumerate
The k-means algorithm is used to fine-tune a Gaussian mixture model after
 the latter has converged: F
\end_layout

\begin_layout Enumerate
The backpropagation algorithm computes the partial derivatives of the error
 function with respect to the network weights: T
\end_layout

\begin_layout Enumerate
The backpropagation algorithm must be coupled with an optimization method
 (update rule) to make it a learning algorithm for a MLP: T
\end_layout

\begin_layout Enumerate
A MLP requires the specification of the number of hidden neurons; this is
 best done by trial-and-error, monitoring the fitting error of the network:
 T
\end_layout

\begin_layout Enumerate
RBF neural networks are a particular case of MLP networks: ?
\end_layout

\begin_layout Enumerate
In a RBF neural network, regularization does not make sense, because it
 is based on Euclidean distance: ?
\end_layout

\begin_layout Enumerate
Feature selection can never increase the practical performance of a learning
 method, it only reduces learning time: F
\end_layout

\begin_layout Enumerate
Feature selection can be performed after feature extraction, using the extracted
 features as new variables for selection: T
\end_layout

\end_body
\end_document
