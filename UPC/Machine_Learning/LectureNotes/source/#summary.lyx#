#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{footmisc}
\usepackage{listings}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
%\usepackage{epstopdf}
\usepackage{matlab}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1,0.99,0.94}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,  
    frame=single,
    tabsize=2
}

\lstset{style=mystyle}
\end_preamble
\use_default_options true
\begin_modules
tcolorbox
customHeadersFooters
theorems-ams-bytype
theorems-sec-bytype
algorithm2e
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "linkcolor=blue, urlcolor=blue, citecolor=blue, pdfstartview={FitH}, unicode=true"
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\boxbgcolor #62a0ea
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 3cm
\rightmargin 2cm
\bottommargin 3cm
\headheight 2cm
\headsep 1cm
\footskip 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
ML-MDS - Machine Learning
\end_layout

\begin_layout Date
Spring 2023
\end_layout

\begin_layout Author
Jose Antonio Lorencio Abril
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../../../Big_Data_Mng/LectureNotes/source/upc-logo.png
	scale 50

\end_inset


\end_layout

\begin_layout Standard
\align right
Professor: Marta Arias
\end_layout

\begin_layout Standard
\align right
Student e-mail: jose.antonio.lorencio@estudiantat..upc.edu
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Address
This is a summary of the course 
\emph on
Machine Learning
\emph default
 taught at the Universitat Polit√®cnica de Catalunya by Professor Marta Arias
 in the academic year 22/23.
 Most of the content of this document is adapted from the course notes by
 Arias, 
\begin_inset CommandInset citation
LatexCommand cite
key "Arias2022"
literal "false"

\end_inset

, so I won't be citing it all the time.
 Other references will be provided when used.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Linear regression
\end_layout

\begin_layout Subsection
Introduction
\end_layout

\begin_layout Standard
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Matlab's-accidents-dataset"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can observe a dataset of the population of different states plotted
 against the number of fatal accidents in each of the states.
 Here, each blue circle corresponds to a row of our data, and the coordinates
 are the 
\begin_inset Formula $\left(population,\#accidents\right)$
\end_inset

 values in the row.
 The red line is the linear regression model of this data.
 This means it is the line that 'best' approximates the data, where best
 refers to minimizing some kind of error: the squared error between each
 point to its projection on the 
\begin_inset Formula $y$
\end_inset

 axis of the line, in this case.
 This approach is called the 
\series bold
least squares method
\series default
.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado2.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Matlab's-accidents-dataset"

\end_inset

Matlab's 
\color blue
accidents
\color inherit
 dataset and best linear fit.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Least squares method
\end_layout

\begin_layout Subsubsection
Least squares in 2D
\end_layout

\begin_layout Standard
In 2D, we have a dataset 
\begin_inset Formula $\left\{ \left(x_{i},y_{i}\right),i=1,...,n\right\} $
\end_inset

 and we want to find the line that best approximates 
\begin_inset Formula $y$
\end_inset

 as a function of 
\begin_inset Formula $x$
\end_inset

.
 As we want a line, we need to specify its slope, 
\begin_inset Formula $\theta_{1}$
\end_inset

, and its intercept, 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 So, our estimations are:
\begin_inset Formula 
\[
\hat{y}\left(x_{i}\right)=\hat{y}_{i}=\theta_{0}+\theta_{1}x_{i}.
\]

\end_inset

 The least squares linear regression method chooses 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\theta_{1}$
\end_inset

 in such a way that the 
\series bold
error function
\series default

\begin_inset Formula 
\[
J\left(\theta_{0},\theta_{1}\right)=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}=\sum_{i=1}^{n}\left(y_{i}-\theta_{0}-\theta_{1}x_{i}\right)^{2}
\]

\end_inset

 is minimized.
 
\end_layout

\begin_layout Standard
Note that this function only depends on the parameters 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\theta_{1}$
\end_inset

, since the data is assumed to be fixed (they are observations).
\end_layout

\begin_layout Standard
To compute them, we just need to find the minimum of 
\begin_inset Formula $J$
\end_inset

, by taking partial derivatives and setting them to 0.
 Let's do this optimization.
 First, we can develop the square:
\begin_inset Formula 
\[
J\left(\theta_{0},\theta_{1}\right)=\sum_{i=1}^{n}y_{i}^{2}+\theta_{0}^{2}+\theta_{1}^{2}x_{i}^{2}-2\theta_{0}y_{i}-2\theta_{1}x_{i}y_{i}+2\theta_{0}\theta_{1}x_{i}.
\]

\end_inset

 Thus:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial J}{\partial\theta_{0}}=\sum_{i=1}^{n}2\theta_{0}-2y_{i}+2\theta_{1}x_{i}=2n\theta_{0}-2\sum_{i=1}^{n}y_{i}+2\theta_{1}\sum_{i=1}^{n}x_{i}
\]

\end_inset

and 
\begin_inset Formula 
\[
\frac{\partial J}{\partial\theta_{1}}=\sum_{i=1}^{n}2\theta_{1}x_{i}^{2}-2x_{i}y_{i}+2\theta_{0}x_{i}=2\theta_{1}\sum_{i=1}^{n}x_{i}^{2}-2\sum_{i=1}^{n}x_{i}y_{i}+2\theta_{0}\sum_{i=1}^{n}x_{i}.
\]

\end_inset

 We have now to solve the system given by
\begin_inset Formula 
\[
\begin{cases}
2n\theta_{0}-2\sum_{i=1}^{n}y_{i}+2\theta_{1}\sum_{i=1}^{n}x_{i}=0\\
2\theta_{1}\sum_{i=1}^{n}x_{i}^{2}-2\sum_{i=1}^{n}x_{i}y_{i}+2\theta_{0}\sum_{i=1}^{n}x_{i}=0
\end{cases}
\]

\end_inset

 which is equivalent to
\begin_inset Formula 
\[
\begin{cases}
n\theta_{0}-\sum_{i=1}^{n}y_{i}+\theta_{1}\sum_{i=1}^{n}x_{i}=0\\
\theta_{1}\sum_{i=1}^{n}x_{i}^{2}-\sum_{i=1}^{n}x_{i}y_{i}+\theta_{0}\sum_{i=1}^{n}x_{i}=0
\end{cases}.
\]

\end_inset

 We can isolate 
\begin_inset Formula $\theta_{0}$
\end_inset

 from the first equation:
\begin_inset Formula 
\[
\theta_{0}=\frac{\sum_{i=1}^{n}y_{i}-\theta_{1}\sum_{i=1}^{n}x_{i}}{n},
\]

\end_inset

 and substitute it in the second one
\begin_inset Formula 
\[
\theta_{1}\sum_{i=1}^{n}x_{i}^{2}-\sum_{i=1}^{n}x_{i}y_{i}+\frac{\sum_{i=1}^{n}y_{i}-\theta_{1}\sum_{i=1}^{n}x_{i}}{n}\sum_{i=1}^{n}x_{i}=0,
\]

\end_inset

 which is equivalent to
\begin_inset Formula 
\[
\theta_{1}\sum_{i=1}^{n}x_{i}^{2}-\sum_{i=1}^{n}x_{i}y_{i}+\frac{\sum_{i=1}^{n}y_{i}\sum_{i=1}^{n}x_{i}-\theta_{1}\left[\sum_{i=1}^{n}x_{i}\right]^{2}}{n}=0
\]

\end_inset

 or
\begin_inset Formula 
\[
\theta_{1}\left[\sum_{i=1}^{n}x_{i}^{2}-\frac{\left[\sum_{i=1}^{n}x_{i}\right]^{2}}{n}\right]-\sum_{i=1}^{n}x_{i}y_{i}+\frac{\sum_{i=1}^{n}y_{i}\sum_{i=1}^{n}x_{i}}{n}=0.
\]

\end_inset

 At this point, we can divide everything by 
\begin_inset Formula $n$
\end_inset

, yielding:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\theta_{1}\left[\frac{\sum_{i=1}^{n}x_{i}^{2}}{n}-\frac{\left[\sum_{i=1}^{n}x_{i}\right]^{2}}{n^{2}}\right]-\frac{\sum_{i=1}^{n}x_{i}y_{i}}{n}+\frac{\sum_{i=1}^{n}y_{i}\sum_{i=1}^{n}x_{i}}{n^{2}}=0.
\]

\end_inset


\end_layout

\begin_layout Standard
If we now assume that the observations are equiprobable, i.e., 
\begin_inset Formula $P\left(x_{i}\right)=\frac{1}{n}$
\end_inset

, and we call 
\begin_inset Formula $X$
\end_inset

 the random variable from which observations 
\begin_inset Formula $x_{i}$
\end_inset

 are obtained and the same for the observations 
\begin_inset Formula $y_{i}$
\end_inset

, obtained from 
\begin_inset Formula $Y$
\end_inset

, then:
\begin_inset Formula 
\[
\frac{\sum_{i=1}^{n}x_{i}^{2}}{n}=E\left[X^{2}\right],\ \frac{\left[\sum_{i=1}^{n}x_{i}\right]^{2}}{n^{2}}=E\left[X\right]^{2},\ \frac{\sum_{i=1}^{n}x_{i}y_{i}}{n}=E\left[XY\right],\ \frac{\sum_{i=1}^{n}y_{i}\sum_{i=1}^{n}x_{i}}{n^{2}}=E\left[X\right]E\left[Y\right].
\]

\end_inset

 This means that the previous equation can be rewritten as:
\begin_inset Formula 
\[
\theta_{1}\left(E\left[X^{2}\right]-E\left[X\right]^{2}\right)-\left(E\left[XY\right]-E\left[X\right]E\left[Y\right]\right)=0\iff\theta_{1}Var\left[X\right]-Cov\left[X,Y\right]=0
\]

\end_inset

 So
\begin_inset Formula 
\[
\theta_{1}=\frac{Cov\left[X,Y\right]}{Var\left[X\right]},
\]

\end_inset


\begin_inset Formula 
\[
\theta_{0}=E\left[Y\right]-\theta_{1}E\left[X\right].
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Least squares regression: multivariate case
\end_layout

\begin_layout Standard
Now, we can assume that we have 
\begin_inset Formula $m$
\end_inset

 independent variables 
\begin_inset Formula $X_{1},...,X_{m}$
\end_inset

 which we want to use to predict the dependent variable 
\begin_inset Formula $Y$
\end_inset

.
 Again, we have 
\begin_inset Formula $n$
\end_inset

 observations of each variable.
 Now, we want to construct an hyperplane in 
\begin_inset Formula $\mathbb{R}^{m+1}$
\end_inset

, whose predictions would be obtained as
\begin_inset Formula 
\[
\hat{y}\left(X_{i}\right)=\theta_{0}+\theta_{1}x_{i1}+...+\theta_{m}x_{im}=\theta_{0}+\sum_{j=1}^{m}\theta_{j}x_{ij}=\sum_{j=0}^{m}\theta_{j}x_{ij},
\]

\end_inset

 where we define 
\begin_inset Formula $x_{i0}=1,$
\end_inset

 for all 
\begin_inset Formula $i$
\end_inset

.
 We can represent
\begin_inset Formula 
\[
X=\left(x_{ij}\right)_{i=1,...,n;\ j=1,...,m},\ Y=\left[\begin{array}{c}
y_{1}\\
\vdots\\
y_{n}
\end{array}\right],\ \theta=\left[\begin{array}{c}
\theta_{0}\\
\vdots\\
\theta_{m}
\end{array}\right],
\]

\end_inset

 so that we can write
\begin_inset Formula 
\[
\hat{Y}=X\theta.
\]

\end_inset

 The error function is defined as in the simple case
\begin_inset Formula 
\[
J\left(\theta\right)=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2},
\]

\end_inset

 but now we can rewrite this as
\begin_inset Formula 
\[
J\left(\theta\right)=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}=\left(Y-\hat{Y}\right)^{T}\left(Y-\hat{Y}\right)=\left(Y-X\theta\right)^{T}\left(Y-X\theta\right).
\]

\end_inset

 Again, to obtain 
\begin_inset Formula $\theta$
\end_inset

 we need to optimize this function using matrix calculus.
\end_layout

\begin_layout Lemma
If 
\begin_inset Formula $A=\left[\begin{array}{ccc}
a_{11} & ... & a_{1m}\\
\vdots & \ddots & \vdots\\
a_{n1} & ... & a_{nm}
\end{array}\right]\in\mathcal{M}_{n\times m}\left(\mathbb{R}\right)$
\end_inset

 , 
\begin_inset Formula $\theta=\left[\begin{array}{c}
\theta_{1}\\
\vdots\\
\theta_{m}
\end{array}\right]\in\mathbb{R}^{m}$
\end_inset

 and 
\begin_inset Formula $B=\left[\begin{array}{ccc}
b_{11} & ... & b_{1n}\\
\vdots & \ddots & \vdots\\
b_{n1} & ... & b_{nn}
\end{array}\right]\in\mathcal{M}_{m\times m}\left(\mathbb{R}\right)$
\end_inset

 is a symmetric matrix, it holds:
\begin_inset Formula 
\[
\frac{\partial A\theta}{\partial\theta}=A,
\]

\end_inset


\begin_inset Formula 
\[
\frac{\partial\theta^{T}A^{T}}{\partial\theta}=A,
\]

\end_inset

 and
\begin_inset Formula 
\[
\frac{\partial\theta^{T}B\theta}{\partial\theta}=2\theta^{T}B^{T}.
\]

\end_inset


\end_layout

\begin_layout Proof
First, notice that 
\begin_inset Formula $A\theta=\left[\begin{array}{c}
\sum_{j=1}^{m}a_{1j}\theta_{j}\\
\vdots\\
\sum_{j=1}^{m}a_{nj}\theta_{j}
\end{array}\right]\in\mathbb{R}^{n},$
\end_inset

so it is
\begin_inset Formula 
\[
\frac{\partial A\theta}{\partial\theta}=\left[\begin{array}{ccc}
\frac{\partial\sum_{j=1}^{m}a_{1j}\theta_{j}}{\partial\theta_{1}} & ... & \frac{\partial\sum_{j=1}^{m}a_{1j}\theta_{j}}{\partial\theta_{m}}\\
\vdots & \ddots & \vdots\\
\frac{\partial\sum_{j=1}^{m}a_{nj}\theta_{j}}{\partial\theta_{1}} & ... & \frac{\partial\sum_{j=1}^{m}a_{nj}\theta_{j}}{\partial\theta_{m}}
\end{array}\right]=\left[\begin{array}{ccc}
a_{11} & ... & a_{1m}\\
\vdots & \ddots & \vdots\\
a_{n1} & ... & a_{nm}
\end{array}\right]=A.
\]

\end_inset

 For the second result, the procedure is the same.
\end_layout

\begin_layout Proof
Lastly, notice that 
\begin_inset Formula $\theta^{T}B\theta=\sum_{k=1}^{m}\sum_{j=1}^{m}\theta_{k}b_{kj}\theta_{j}\in\mathbb{R}$
\end_inset

, so
\begin_inset Formula 
\[
\frac{\partial\theta^{T}B\theta}{\partial\theta}=\left[\begin{array}{ccc}
\frac{\partial\sum_{k=1}^{m}\sum_{j=1}^{m}\theta_{k}b_{kj}\theta_{j}}{\partial\theta_{1}} & ... & \frac{\partial\sum_{k=1}^{m}\sum_{j=1}^{m}\theta_{k}b_{kj}\theta_{j}}{\partial\theta_{m}}\end{array}\right]=\left[\begin{array}{ccc}
2\sum_{j=1}^{m}b_{1j}\theta_{j} & ... & 2\sum_{j=1}^{m}b_{mj}\theta_{j}\end{array}\right]=2\left[B\theta\right]^{T}=2\theta^{T}B^{T}.
\]

\end_inset


\end_layout

\begin_layout Standard
Now, we can proceed and minimize 
\begin_inset Formula $J$
\end_inset

:
\begin_inset Formula 
\begin{align*}
\frac{\partial J\left(\theta\right)}{\partial\theta}= & \frac{\partial\left(Y-X\theta\right)^{T}\left(Y-X\theta\right)}{\theta}\\
= & \frac{\partial}{\partial\theta}\left[Y^{T}Y-Y^{T}X\theta-\theta^{T}X^{T}Y+\theta^{T}X^{T}X\theta\right]\\
= & 0-Y^{T}X-Y^{T}X+2X^{T}X\theta\\
= & -2Y^{T}X+2\theta^{T}X^{T}X,
\end{align*}

\end_inset

 setting this to be 0, we get
\begin_inset Formula 
\[
\theta^{T}X^{T}X=Y^{T}X\iff X^{T}X\theta=X^{T}Y\iff\theta=\left(X^{T}X\right)^{-1}X^{T}Y.
\]

\end_inset

 Thus, the 'best' linear model is given by
\begin_inset Formula 
\[
\theta_{lse}=\left(X^{T}X\right)^{-1}X^{T}Y.
\]

\end_inset

 Once we have this model, if we have an observation of 
\begin_inset Formula $X$
\end_inset

, 
\begin_inset Formula $x'=\left(x'_{1},...,x'_{m}\right)$
\end_inset

 and we want to make a prediction, we compute
\begin_inset Formula 
\[
y'=x'\theta_{lse}.
\]

\end_inset

 The approach that we have followed here is the 
\series bold
optimization
\series default
 view of learning, which basically consists of the steps:
\end_layout

\begin_layout Enumerate
Set up an error function as a function of some parameters.
\end_layout

\begin_layout Enumerate
Optimize this function to find the suitable values for this parameters,
 assuming the data as given.
\end_layout

\begin_layout Enumerate
Use incoming values to make predictions.
\end_layout

\begin_layout Subsubsection
Computation of least squares solution via the singular values decomposition
 (SVD)
\end_layout

\begin_layout Standard
Inverting 
\begin_inset Formula $X^{T}X$
\end_inset

 can entail numerical problems, so the SVD can be used instead.
\end_layout

\begin_layout Standard
\begin_inset Flex Color Box
status open

\begin_layout Theorem
Any matrix 
\begin_inset Formula $A\in\mathbb{R}^{m\times n}$
\end_inset

, 
\begin_inset Formula $m>n$
\end_inset

, can be expressed as
\begin_inset Formula 
\[
A=U\Sigma V^{T},
\]

\end_inset

 where 
\begin_inset Formula $U\in\mathbb{R}^{m\times n}$
\end_inset

 has orthonormal columns 
\begin_inset Formula $\left(U^{T}U=I\right)$
\end_inset

, 
\begin_inset Formula $\Sigma\in\mathbb{R}^{n\times n}$
\end_inset

 is diagonal and contains the singular values in its diagonal and 
\begin_inset Formula $V\in\mathbb{R}^{n\times n}$
\end_inset

 is orthonormal 
\begin_inset Formula $\left(V^{-1}=V^{T}\right)$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $C=A^{T}A\in\mathbb{R}^{n\times n}$
\end_inset

.
 
\begin_inset Formula $C$
\end_inset

 is square, symmetric and positive semidefinite.
 Therefore, 
\begin_inset Formula $C$
\end_inset

 is diagonalizable, so it can be written as
\begin_inset Formula 
\[
C=V\Lambda V^{T},
\]

\end_inset

 where 
\begin_inset Formula $V=\left(v_{i}\right)_{i=1,...,n}$
\end_inset

 is orthogonal and 
\begin_inset Formula $\Lambda=diag\left(\lambda_{1},...,\lambda_{d}\right)$
\end_inset

 with 
\begin_inset Formula $\lambda_{1}\geq...\geq\lambda_{r}>0=\lambda_{r+1}=...=\lambda_{n}$
\end_inset

 and 
\begin_inset Formula $r$
\end_inset

 is 
\begin_inset Formula $rank\left(A\right)\leq n$
\end_inset

.
 
\end_layout

\begin_layout Proof
Now, define 
\begin_inset Formula $\sigma_{i}=\sqrt{\lambda_{i}}$
\end_inset

, and form the matrix
\begin_inset Formula 
\[
\Sigma=\left[\begin{array}{cc}
diag\left(\sigma_{1},...,\sigma_{r}\right) & 0_{r\times\left(n-r\right)}\\
0_{\left(m-r\right)\times r} & 0_{\left(m-r\right)\times\left(n-r\right)}
\end{array}\right].
\]

\end_inset

 Define also
\begin_inset Formula 
\[
u_{i}=\frac{1}{\sigma_{i}}Xv_{i}\in\mathbb{R}^{m},i=1,...,r.
\]

\end_inset

 Then, this vectors are orthonormal:
\begin_inset Formula 
\[
u_{i}^{T}u_{j}=\left(\frac{1}{\sigma_{i}}Xv_{i}\right)^{T}\left(\frac{1}{\sigma_{j}}Xv_{j}\right)=\frac{1}{\sigma_{i}\sigma_{j}}v_{i}^{T}X^{T}Xv_{j}=\frac{1}{\sigma_{i}\sigma_{j}}v_{i}^{T}Cv_{j}\overset{*}{=}\frac{1}{\sigma_{i}\sigma_{j}}v_{i}^{T}\left(\lambda_{j}v_{j}\right)\overset{\left(\lambda_{j}=\sigma_{j}^{2}\right)}{=}\frac{\sigma_{j}}{\sigma_{i}}v_{i}^{T}v_{j}\overset{**}{=}0,
\]

\end_inset

 where 
\begin_inset Formula $\left(*\right)$
\end_inset

 is because 
\begin_inset Formula $V$
\end_inset

 is formed with the eigenvectors of 
\begin_inset Formula $C$
\end_inset

, and 
\begin_inset Formula $\left(**\right)$
\end_inset

 is because 
\begin_inset Formula $V$
\end_inset

 is orthonormal.
\end_layout

\begin_layout Proof
Now, we can complete the base with 
\begin_inset Formula $u_{r+1},...,u_{n}$
\end_inset

 (using Gram-Schmidt) in such a way that
\begin_inset Formula 
\[
U=\left[u_{1},...,u_{r},u_{r+1},...,u_{n}\right]\in\mathbb{R}^{n\times n}
\]

\end_inset

 is column orthonormal.
\end_layout

\begin_layout Proof
Now, if it is the case that 
\begin_inset Formula $XV=U\Sigma$
\end_inset

, then 
\begin_inset Formula 
\[
X=XVV^{T}=U\Sigma V^{T},
\]

\end_inset

 so it is only left to see that indeed this holds.
 Consider two cases:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $1\leq i\leq r:$
\end_inset

 
\begin_inset Formula $Xv_{i}=u_{i}\Sigma$
\end_inset

 by the definition of 
\begin_inset Formula $u_{i}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $r+1\leq i\leq n:$
\end_inset

 It is 
\begin_inset Formula $Xv_{i}=0,$
\end_inset

 because 
\begin_inset Formula $X^{T}Xv_{i}=Cv_{i}=\lambda_{i}v_{i}\overset{i>r}{=}0$
\end_inset

.
 As 
\begin_inset Formula $X,v_{i}\neq0$
\end_inset

, it must be 
\begin_inset Formula $Xv_{i}=0$
\end_inset

.
 On the other side of the equation we also have 0 because 
\begin_inset Formula $u_{i}\Sigma=u_{i}\sigma_{i}=0$
\end_inset

 as 
\begin_inset Formula $i>r$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
This, added to the fact that if 
\begin_inset Formula $X$
\end_inset

 has full rank, 
\begin_inset Formula $X^{T}X$
\end_inset

 is invertible and all its eigenvalues non null, gives us: 
\begin_inset Formula 
\begin{align*}
\theta_{lse}= & \left(X^{T}X\right)^{-1}X^{T}y\\
= & \left(\left(U\Sigma V^{T}\right)U\Sigma V^{T}\right)^{-1}\left(U\Sigma V^{T}\right)^{T}y\\
= & \left(V\Sigma U^{T}U\Sigma V^{T}\right)^{-1}V\Sigma U^{T}y\\
= & \Sigma^{-2}V\Sigma U^{T}y=V\Sigma^{-1}U^{T}y\\
= & V\cdot diag\left(\frac{1}{\sigma_{1}},...,\frac{1}{\sigma_{n}}\right)\cdot U^{T}y.
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection*
Intuitive interpretation
\end_layout

\begin_layout Standard
The intuition behind the SVD is summarized in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SVD-visual."
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Basically, every linear transformation can be decomposed into a rotation,
 a scaling and a simpler transformation (column orthogonal).
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename SVD.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
SVD visual.
\begin_inset CommandInset label
LatexCommand label
name "fig:SVD-visual."

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The intuition behind SVD lies in the idea of finding a low-rank approximation
 of a given matrix.
 The rank of a matrix is the number of linearly independent rows or columns
 it contains.
 A high-rank matrix has many linearly independent rows or columns, which
 makes it complex and difficult to analyze.
 On the other hand, a low-rank matrix has fewer linearly independent rows
 or columns, which makes it simpler and easier to analyze.
\end_layout

\begin_layout Standard
SVD provides a way to find the best possible low-rank approximation of a
 given matrix by decomposing it into three components.
 The left singular vectors represent the 
\series bold
direction of maximum variance
\series default
 in the data, while the right singular vectors represent the 
\series bold
direction of maximum correlation
\series default
 between the variables.
 The singular values represent the 
\series bold
magnitude of the variance or correlation
\series default
 in each direction.
\end_layout

\begin_layout Standard
By truncating the diagonal matrix of singular values to keep only the top-k
 values, we can obtain a low-rank approximation of the original matrix that
 retains most of the important information.
 This is useful for reducing the dimensionality of data, compressing images,
 and solving linear equations, among other applications.
\end_layout

\begin_layout Example
How to use SVD in Python and Matlab.
\end_layout

\begin_layout Example
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset listings
lstparams "language=Python"
inline false
status open

\begin_layout Plain Layout

import numpy as np
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

U, d, Vt = np.linalg.svd(X, full_matrices=False)
\end_layout

\begin_layout Plain Layout

D = np.diag(1/d)
\end_layout

\begin_layout Plain Layout

theta = Vt.T @ D @ U.T @ y
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
SVD in Python.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Example
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset listings
lstparams "language=Matlab"
inline false
status open

\begin_layout Plain Layout

[U, d, V] = svd(X)
\end_layout

\begin_layout Plain Layout

D = diag(diag(1./d))
\end_layout

\begin_layout Plain Layout

theta = V'*D*U'*y
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
SVD in Matlab.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Things that could go wrong when using linear regression
\end_layout

\begin_layout Subsubsection
Our independent variable is not enough
\end_layout

\begin_layout Standard
It is possible that our variable 
\begin_inset Formula $X$
\end_inset

 does not provide enough information to predict 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado7.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The independent variable does not provide enough information.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
The relationship between the variables is not linear (underfitting)
\begin_inset CommandInset label
LatexCommand label
name "subsec:The-relationship-between"

\end_inset


\end_layout

\begin_layout Standard
It is also possible that the variables are related in non-linear ways.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado6.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The variables are not linearly related.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Outliers affect the fit
\end_layout

\begin_layout Standard
In the presence of outliers, the model obtained can be distorted, leading
 to bad results.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado8.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The outlier distort the fit.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Basis Functions
\end_layout

\begin_layout Standard
In order to fix the second problem (Subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:The-relationship-between"
plural "false"
caps "false"
noprefix "false"

\end_inset

), we can make use of basis functions.
 The idea is to apply different transformations to the data, so that we
 can extend the expressive power of our model.
 
\end_layout

\begin_layout Standard
\begin_inset Flex Color Box
status open

\begin_layout Definition
A 
\series bold
feature mapping
\series default
 is a non-linear transformation of the inputs 
\begin_inset Formula $\phi:\mathbb{R}^{m}\rightarrow\mathbb{R}^{k}$
\end_inset

.
\end_layout

\begin_layout Definition
The resulting 
\series bold
predictive function
\series default
 or model is 
\begin_inset Formula $y=\phi\left(x\right)\theta$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Example
For example, we can consider the 
\series bold
polynomial expansion of degree 
\begin_inset Formula $k$
\end_inset


\series default
, which is a commonly used feature mapping that approximates the relationship
 between the independent variable 
\begin_inset Formula $x$
\end_inset

 and the dependent variable 
\begin_inset Formula $y$
\end_inset

 to be polynomial of degree 
\begin_inset Formula $k$
\end_inset

, i.e.:
\begin_inset Formula 
\[
y=\theta_{0}+\theta_{1}x+\theta_{2}x^{2}+...+\theta_{k}x^{k}.
\]

\end_inset

 The feature mapping is 
\begin_inset Formula $\phi\left(x\right)=\left(\begin{array}{ccccc}
1 & x & x^{2} & ... & x^{k}\end{array}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Note that the idea is to transform the data so that the fit is still linear,
 even if the relationship is not.
 Of course, this requires to apply the same transformation whenever we receive
 an input for which we want to make predictions.
 Also, the resulting model is more complex, so 
\series bold
complexity control
\series default
 is necessary to avoid overfitting.
\end_layout

\begin_layout Example
A MATLAB example
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

% This LaTeX was auto-generated from MATLAB code.
\end_layout

\begin_layout Plain Layout

% To make changes, update the MATLAB code and export to LaTeX again.
\end_layout

\begin_layout Plain Layout


\backslash
sloppy
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
matlabtitle{Example 2.3}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

First, we define the dataset.
 In this case $y=e^x$.
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

x=linspace(1,5,15)'
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

x = 15x1    
\end_layout

\begin_layout Plain Layout

    1.0000
\end_layout

\begin_layout Plain Layout

    1.2857
\end_layout

\begin_layout Plain Layout

    1.5714
\end_layout

\begin_layout Plain Layout

    1.8571
\end_layout

\begin_layout Plain Layout

    2.1429
\end_layout

\begin_layout Plain Layout

    2.4286
\end_layout

\begin_layout Plain Layout

    2.7143
\end_layout

\begin_layout Plain Layout

    3.0000
\end_layout

\begin_layout Plain Layout

    3.2857
\end_layout

\begin_layout Plain Layout

    3.5714
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

X = [ones(length(x),1) x]
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

X = 15x2    
\end_layout

\begin_layout Plain Layout

    1.0000    1.0000
\end_layout

\begin_layout Plain Layout

    1.0000    1.2857
\end_layout

\begin_layout Plain Layout

    1.0000    1.5714
\end_layout

\begin_layout Plain Layout

    1.0000    1.8571
\end_layout

\begin_layout Plain Layout

    1.0000    2.1429
\end_layout

\begin_layout Plain Layout

    1.0000    2.4286
\end_layout

\begin_layout Plain Layout

    1.0000    2.7143
\end_layout

\begin_layout Plain Layout

    1.0000    3.0000
\end_layout

\begin_layout Plain Layout

    1.0000    3.2857
\end_layout

\begin_layout Plain Layout

    1.0000    3.5714
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

y=exp(x);
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

Now, we first see what happens with linear regression:
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

b1 = X
\backslash
y;
\end_layout

\begin_layout Plain Layout

yCalc1 = X*b1;
\end_layout

\begin_layout Plain Layout

figure;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

scatter(x,y,'filled')
\end_layout

\begin_layout Plain Layout

hold on 
\end_layout

\begin_layout Plain Layout

plot(x,yCalc1,'LineWidth',2)
\end_layout

\begin_layout Plain Layout

xlabel('X')
\end_layout

\begin_layout Plain Layout

ylabel('Y')
\end_layout

\begin_layout Plain Layout

title('Linear Regression Relation Between X & Y')
\end_layout

\begin_layout Plain Layout

legend('Data','Linear regression')
\end_layout

\begin_layout Plain Layout

grid on
\end_layout

\begin_layout Plain Layout

hold off
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename scripts/example_2_3_images/figure_0.eps

\end_inset


\end_layout

\end_deeper
\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

As we can see, the model does not fit the data addecuately.
 We can use a feature mapping and repeat the process with the transformed
 input:
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

X = feat_map(x)
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

X = 15x3    
\end_layout

\begin_layout Plain Layout

    1.0000    1.0000    2.7183
\end_layout

\begin_layout Plain Layout

    1.0000    1.2857    3.6173
\end_layout

\begin_layout Plain Layout

    1.0000    1.5714    4.8135
\end_layout

\begin_layout Plain Layout

    1.0000    1.8571    6.4054
\end_layout

\begin_layout Plain Layout

    1.0000    2.1429    8.5238
\end_layout

\begin_layout Plain Layout

    1.0000    2.4286   11.3427
\end_layout

\begin_layout Plain Layout

    1.0000    2.7143   15.0938
\end_layout

\begin_layout Plain Layout

    1.0000    3.0000   20.0855
\end_layout

\begin_layout Plain Layout

    1.0000    3.2857   26.7281
\end_layout

\begin_layout Plain Layout

    1.0000    3.5714   35.5674
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

b2 = X
\backslash
y;
\end_layout

\begin_layout Plain Layout

yCalc2 = X*b2;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

scatter(x,y,'filled')
\end_layout

\begin_layout Plain Layout

hold on 
\end_layout

\begin_layout Plain Layout

plot(x,yCalc2,'LineWidth',2)
\end_layout

\begin_layout Plain Layout

xlabel('X')
\end_layout

\begin_layout Plain Layout

ylabel('Y')
\end_layout

\begin_layout Plain Layout

title('Linear Regression Relation Between exp(X) & Y')
\end_layout

\begin_layout Plain Layout

legend('Data','Linear regression','Location','north')
\end_layout

\begin_layout Plain Layout

grid on
\end_layout

\begin_layout Plain Layout

hold off
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename scripts/example_2_3_images/figure_1.eps

\end_inset


\end_layout

\end_deeper
\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

As we can see, the model is now perfectly fitting the data!
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

function X=feat_map(x)
\end_layout

\begin_layout Plain Layout

    X = [ones(size(x)) x exp(x)];
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "upc"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
