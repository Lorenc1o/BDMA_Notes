#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{footmisc}
\usepackage[most]{tcolorbox}
\tcbuselibrary{listings}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
%\usepackage{epstopdf}
\usepackage{matlab}
\tcbuselibrary{listings}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1,0.99,0.94}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,  
    frame=single,
    tabsize=2,
	morekeywords={Algorithm, begin, repeat, until, end},
	escapeinside={<@}{@>}
}

\lstset{
  style=mystyle
}

\newtcblisting{pseudocode}{
  listing only,
  breakable,
  colback=backcolour,
  boxrule=0.1pt,
  arc=0pt,
  outer arc=0pt,
  colframe=black,
  listing options={
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    frame=single,
    tabsize=2,
    morekeywords={Algorithm, begin, repeat, until, end, Initialize},
    escapeinside={<@}{@>}
  }
}
\end_preamble
\use_default_options true
\begin_modules
tcolorbox
customHeadersFooters
theorems-ams-bytype
theorems-sec-bytype
algorithm2e
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "linkcolor=blue, urlcolor=blue, citecolor=blue, pdfstartview={FitH}, unicode=true"
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\boxbgcolor #62a0ea
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 3cm
\rightmargin 2cm
\bottommargin 3cm
\headheight 2cm
\headsep 1cm
\footskip 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
ML-MDS - Machine Learning
\end_layout

\begin_layout Date
Spring 2023
\end_layout

\begin_layout Author
Jose Antonio Lorencio Abril
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../../../Big_Data_Mng/LectureNotes/source/upc-logo.png
	scale 50

\end_inset


\end_layout

\begin_layout Standard
\align right
Professor: Marta Arias
\end_layout

\begin_layout Standard
\align right
Student e-mail: jose.antonio.lorencio@estudiantat..upc.edu
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Address
This is a summary of the course 
\emph on
Machine Learning
\emph default
 taught at the Universitat Polit√®cnica de Catalunya by Professor Marta Arias
 in the academic year 22/23.
 Most of the content of this document is adapted from the course notes by
 Arias, 
\begin_inset CommandInset citation
LatexCommand cite
key "Arias2022"
literal "false"

\end_inset

, so I won't be citing it all the time.
 Other references will be provided when used.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList figure

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList algorithm

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Linear regression
\end_layout

\begin_layout Subsection
Introduction
\end_layout

\begin_layout Standard
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Matlab's-accidents-dataset"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can observe a dataset of the population of different states plotted
 against the number of fatal accidents in each of the states.
 Here, each blue circle corresponds to a row of our data, and the coordinates
 are the 
\begin_inset Formula $\left(population,\#accidents\right)$
\end_inset

 values in the row.
 The red line is the linear regression model of this data.
 This means it is the line that 'best' approximates the data, where best
 refers to minimizing some kind of error: the squared error between each
 point to its projection on the 
\begin_inset Formula $y$
\end_inset

 axis of the line, in this case.
 This approach is called the 
\series bold
least squares method
\series default
.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado2.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Matlab's-accidents-dataset"

\end_inset

Matlab's 
\color blue
accidents
\color inherit
 dataset and best linear fit.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Least squares method
\end_layout

\begin_layout Subsubsection
Least squares in 2D
\end_layout

\begin_layout Standard
In 2D, we have a dataset 
\begin_inset Formula $\left\{ \left(x_{i},y_{i}\right),i=1,...,n\right\} $
\end_inset

 and we want to find the line that best approximates 
\begin_inset Formula $y$
\end_inset

 as a function of 
\begin_inset Formula $x$
\end_inset

.
 As we want a line, we need to specify its slope, 
\begin_inset Formula $\theta_{1}$
\end_inset

, and its intercept, 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 So, our estimations are:
\begin_inset Formula 
\[
\hat{y}\left(x_{i}\right)=\hat{y}_{i}=\theta_{0}+\theta_{1}x_{i}.
\]

\end_inset

 The least squares linear regression method chooses 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\theta_{1}$
\end_inset

 in such a way that the 
\series bold
error function
\series default

\begin_inset Formula 
\[
J\left(\theta_{0},\theta_{1}\right)=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}=\sum_{i=1}^{n}\left(y_{i}-\theta_{0}-\theta_{1}x_{i}\right)^{2}
\]

\end_inset

 is minimized.
 
\end_layout

\begin_layout Standard
Note that this function only depends on the parameters 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\theta_{1}$
\end_inset

, since the data is assumed to be fixed (they are observations).
\end_layout

\begin_layout Standard
To compute them, we just need to find the minimum of 
\begin_inset Formula $J$
\end_inset

, by taking partial derivatives and setting them to 0.
 Let's do this optimization.
 First, we can develop the square:
\begin_inset Formula 
\[
J\left(\theta_{0},\theta_{1}\right)=\sum_{i=1}^{n}y_{i}^{2}+\theta_{0}^{2}+\theta_{1}^{2}x_{i}^{2}-2\theta_{0}y_{i}-2\theta_{1}x_{i}y_{i}+2\theta_{0}\theta_{1}x_{i}.
\]

\end_inset

 Thus:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial J}{\partial\theta_{0}}=\sum_{i=1}^{n}2\theta_{0}-2y_{i}+2\theta_{1}x_{i}=2n\theta_{0}-2\sum_{i=1}^{n}y_{i}+2\theta_{1}\sum_{i=1}^{n}x_{i}
\]

\end_inset

and 
\begin_inset Formula 
\[
\frac{\partial J}{\partial\theta_{1}}=\sum_{i=1}^{n}2\theta_{1}x_{i}^{2}-2x_{i}y_{i}+2\theta_{0}x_{i}=2\theta_{1}\sum_{i=1}^{n}x_{i}^{2}-2\sum_{i=1}^{n}x_{i}y_{i}+2\theta_{0}\sum_{i=1}^{n}x_{i}.
\]

\end_inset

 We have now to solve the system given by
\begin_inset Formula 
\[
\begin{cases}
2n\theta_{0}-2\sum_{i=1}^{n}y_{i}+2\theta_{1}\sum_{i=1}^{n}x_{i}=0\\
2\theta_{1}\sum_{i=1}^{n}x_{i}^{2}-2\sum_{i=1}^{n}x_{i}y_{i}+2\theta_{0}\sum_{i=1}^{n}x_{i}=0
\end{cases}
\]

\end_inset

 which is equivalent to
\begin_inset Formula 
\[
\begin{cases}
n\theta_{0}-\sum_{i=1}^{n}y_{i}+\theta_{1}\sum_{i=1}^{n}x_{i}=0\\
\theta_{1}\sum_{i=1}^{n}x_{i}^{2}-\sum_{i=1}^{n}x_{i}y_{i}+\theta_{0}\sum_{i=1}^{n}x_{i}=0
\end{cases}.
\]

\end_inset

 We can isolate 
\begin_inset Formula $\theta_{0}$
\end_inset

 from the first equation:
\begin_inset Formula 
\[
\theta_{0}=\frac{\sum_{i=1}^{n}y_{i}-\theta_{1}\sum_{i=1}^{n}x_{i}}{n},
\]

\end_inset

 and substitute it in the second one
\begin_inset Formula 
\[
\theta_{1}\sum_{i=1}^{n}x_{i}^{2}-\sum_{i=1}^{n}x_{i}y_{i}+\frac{\sum_{i=1}^{n}y_{i}-\theta_{1}\sum_{i=1}^{n}x_{i}}{n}\sum_{i=1}^{n}x_{i}=0,
\]

\end_inset

 which is equivalent to
\begin_inset Formula 
\[
\theta_{1}\sum_{i=1}^{n}x_{i}^{2}-\sum_{i=1}^{n}x_{i}y_{i}+\frac{\sum_{i=1}^{n}y_{i}\sum_{i=1}^{n}x_{i}-\theta_{1}\left[\sum_{i=1}^{n}x_{i}\right]^{2}}{n}=0
\]

\end_inset

 or
\begin_inset Formula 
\[
\theta_{1}\left[\sum_{i=1}^{n}x_{i}^{2}-\frac{\left[\sum_{i=1}^{n}x_{i}\right]^{2}}{n}\right]-\sum_{i=1}^{n}x_{i}y_{i}+\frac{\sum_{i=1}^{n}y_{i}\sum_{i=1}^{n}x_{i}}{n}=0.
\]

\end_inset

 At this point, we can divide everything by 
\begin_inset Formula $n$
\end_inset

, yielding:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\theta_{1}\left[\frac{\sum_{i=1}^{n}x_{i}^{2}}{n}-\frac{\left[\sum_{i=1}^{n}x_{i}\right]^{2}}{n^{2}}\right]-\frac{\sum_{i=1}^{n}x_{i}y_{i}}{n}+\frac{\sum_{i=1}^{n}y_{i}\sum_{i=1}^{n}x_{i}}{n^{2}}=0.
\]

\end_inset


\end_layout

\begin_layout Standard
If we now assume that the observations are equiprobable, i.e., 
\begin_inset Formula $P\left(x_{i}\right)=\frac{1}{n}$
\end_inset

, and we call 
\begin_inset Formula $X$
\end_inset

 the random variable from which observations 
\begin_inset Formula $x_{i}$
\end_inset

 are obtained and the same for the observations 
\begin_inset Formula $y_{i}$
\end_inset

, obtained from 
\begin_inset Formula $Y$
\end_inset

, then:
\begin_inset Formula 
\[
\frac{\sum_{i=1}^{n}x_{i}^{2}}{n}=E\left[X^{2}\right],\ \frac{\left[\sum_{i=1}^{n}x_{i}\right]^{2}}{n^{2}}=E\left[X\right]^{2},\ \frac{\sum_{i=1}^{n}x_{i}y_{i}}{n}=E\left[XY\right],\ \frac{\sum_{i=1}^{n}y_{i}\sum_{i=1}^{n}x_{i}}{n^{2}}=E\left[X\right]E\left[Y\right].
\]

\end_inset

 This means that the previous equation can be rewritten as:
\begin_inset Formula 
\[
\theta_{1}\left(E\left[X^{2}\right]-E\left[X\right]^{2}\right)-\left(E\left[XY\right]-E\left[X\right]E\left[Y\right]\right)=0\iff\theta_{1}Var\left[X\right]-Cov\left[X,Y\right]=0
\]

\end_inset

 So
\begin_inset Formula 
\[
\theta_{1}=\frac{Cov\left[X,Y\right]}{Var\left[X\right]},
\]

\end_inset


\begin_inset Formula 
\[
\theta_{0}=E\left[Y\right]-\theta_{1}E\left[X\right].
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Least squares regression: multivariate case
\end_layout

\begin_layout Standard
Now, we can assume that we have 
\begin_inset Formula $m$
\end_inset

 independent variables 
\begin_inset Formula $X_{1},...,X_{m}$
\end_inset

 which we want to use to predict the dependent variable 
\begin_inset Formula $Y$
\end_inset

.
 Again, we have 
\begin_inset Formula $n$
\end_inset

 observations of each variable.
 Now, we want to construct an hyperplane in 
\begin_inset Formula $\mathbb{R}^{m+1}$
\end_inset

, whose predictions would be obtained as
\begin_inset Formula 
\[
\hat{y}\left(X_{i}\right)=\theta_{0}+\theta_{1}x_{i1}+...+\theta_{m}x_{im}=\theta_{0}+\sum_{j=1}^{m}\theta_{j}x_{ij}=\sum_{j=0}^{m}\theta_{j}x_{ij},
\]

\end_inset

 where we define 
\begin_inset Formula $x_{i0}=1,$
\end_inset

 for all 
\begin_inset Formula $i$
\end_inset

.
 We can represent
\begin_inset Formula 
\[
X=\left(x_{ij}\right)_{i=1,...,n;\ j=1,...,m},\ Y=\left[\begin{array}{c}
y_{1}\\
\vdots\\
y_{n}
\end{array}\right],\ \theta=\left[\begin{array}{c}
\theta_{0}\\
\vdots\\
\theta_{m}
\end{array}\right],
\]

\end_inset

 so that we can write
\begin_inset Formula 
\[
\hat{Y}=X\theta.
\]

\end_inset

 The error function is defined as in the simple case
\begin_inset Formula 
\[
J\left(\theta\right)=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2},
\]

\end_inset

 but now we can rewrite this as
\begin_inset Formula 
\[
J\left(\theta\right)=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}=\left(Y-\hat{Y}\right)^{T}\left(Y-\hat{Y}\right)=\left(Y-X\theta\right)^{T}\left(Y-X\theta\right).
\]

\end_inset

 Again, to obtain 
\begin_inset Formula $\theta$
\end_inset

 we need to optimize this function using matrix calculus.
\end_layout

\begin_layout Lemma
If 
\begin_inset Formula $A=\left[\begin{array}{ccc}
a_{11} & ... & a_{1m}\\
\vdots & \ddots & \vdots\\
a_{n1} & ... & a_{nm}
\end{array}\right]\in\mathcal{M}_{n\times m}\left(\mathbb{R}\right)$
\end_inset

 , 
\begin_inset Formula $\theta=\left[\begin{array}{c}
\theta_{1}\\
\vdots\\
\theta_{m}
\end{array}\right]\in\mathbb{R}^{m}$
\end_inset

 and 
\begin_inset Formula $B=\left[\begin{array}{ccc}
b_{11} & ... & b_{1n}\\
\vdots & \ddots & \vdots\\
b_{n1} & ... & b_{nn}
\end{array}\right]\in\mathcal{M}_{m\times m}\left(\mathbb{R}\right)$
\end_inset

 is a symmetric matrix, it holds:
\begin_inset Formula 
\[
\frac{\partial A\theta}{\partial\theta}=A,
\]

\end_inset


\begin_inset Formula 
\[
\frac{\partial\theta^{T}A^{T}}{\partial\theta}=A,
\]

\end_inset

 and
\begin_inset Formula 
\[
\frac{\partial\theta^{T}B\theta}{\partial\theta}=2\theta^{T}B^{T}.
\]

\end_inset


\end_layout

\begin_layout Proof
First, notice that 
\begin_inset Formula $A\theta=\left[\begin{array}{c}
\sum_{j=1}^{m}a_{1j}\theta_{j}\\
\vdots\\
\sum_{j=1}^{m}a_{nj}\theta_{j}
\end{array}\right]\in\mathbb{R}^{n},$
\end_inset

so it is
\begin_inset Formula 
\[
\frac{\partial A\theta}{\partial\theta}=\left[\begin{array}{ccc}
\frac{\partial\sum_{j=1}^{m}a_{1j}\theta_{j}}{\partial\theta_{1}} & ... & \frac{\partial\sum_{j=1}^{m}a_{1j}\theta_{j}}{\partial\theta_{m}}\\
\vdots & \ddots & \vdots\\
\frac{\partial\sum_{j=1}^{m}a_{nj}\theta_{j}}{\partial\theta_{1}} & ... & \frac{\partial\sum_{j=1}^{m}a_{nj}\theta_{j}}{\partial\theta_{m}}
\end{array}\right]=\left[\begin{array}{ccc}
a_{11} & ... & a_{1m}\\
\vdots & \ddots & \vdots\\
a_{n1} & ... & a_{nm}
\end{array}\right]=A.
\]

\end_inset

 For the second result, the procedure is the same.
\end_layout

\begin_layout Proof
Lastly, notice that 
\begin_inset Formula $\theta^{T}B\theta=\sum_{k=1}^{m}\sum_{j=1}^{m}\theta_{k}b_{kj}\theta_{j}\in\mathbb{R}$
\end_inset

, so
\begin_inset Formula 
\[
\frac{\partial\theta^{T}B\theta}{\partial\theta}=\left[\begin{array}{ccc}
\frac{\partial\sum_{k=1}^{m}\sum_{j=1}^{m}\theta_{k}b_{kj}\theta_{j}}{\partial\theta_{1}} & ... & \frac{\partial\sum_{k=1}^{m}\sum_{j=1}^{m}\theta_{k}b_{kj}\theta_{j}}{\partial\theta_{m}}\end{array}\right]=\left[\begin{array}{ccc}
2\sum_{j=1}^{m}b_{1j}\theta_{j} & ... & 2\sum_{j=1}^{m}b_{mj}\theta_{j}\end{array}\right]=2\left[B\theta\right]^{T}=2\theta^{T}B^{T}.
\]

\end_inset


\end_layout

\begin_layout Standard
Now, we can proceed and minimize 
\begin_inset Formula $J$
\end_inset

:
\begin_inset Formula 
\begin{align*}
\frac{\partial J\left(\theta\right)}{\partial\theta}= & \frac{\partial\left(Y-X\theta\right)^{T}\left(Y-X\theta\right)}{\theta}\\
= & \frac{\partial}{\partial\theta}\left[Y^{T}Y-Y^{T}X\theta-\theta^{T}X^{T}Y+\theta^{T}X^{T}X\theta\right]\\
= & 0-Y^{T}X-Y^{T}X+2X^{T}X\theta\\
= & -2Y^{T}X+2\theta^{T}X^{T}X,
\end{align*}

\end_inset

 setting this to be 0, we get
\begin_inset Formula 
\[
\theta^{T}X^{T}X=Y^{T}X\iff X^{T}X\theta=X^{T}Y\iff\theta=\left(X^{T}X\right)^{-1}X^{T}Y.
\]

\end_inset

 Thus, the 'best' linear model is given by
\begin_inset Formula 
\[
\theta_{lse}=\left(X^{T}X\right)^{-1}X^{T}Y.
\]

\end_inset

 Once we have this model, if we have an observation of 
\begin_inset Formula $X$
\end_inset

, 
\begin_inset Formula $x'=\left(x'_{1},...,x'_{m}\right)$
\end_inset

 and we want to make a prediction, we compute
\begin_inset Formula 
\[
y'=x'\theta_{lse}.
\]

\end_inset

 The approach that we have followed here is the 
\series bold
optimization
\series default
 view of learning, which basically consists of the steps:
\end_layout

\begin_layout Enumerate
Set up an error function as a function of some parameters.
\end_layout

\begin_layout Enumerate
Optimize this function to find the suitable values for this parameters,
 assuming the data as given.
\end_layout

\begin_layout Enumerate
Use incoming values to make predictions.
\end_layout

\begin_layout Subsubsection
Computation of least squares solution via the singular values decomposition
 (SVD)
\end_layout

\begin_layout Standard
Inverting 
\begin_inset Formula $X^{T}X$
\end_inset

 can entail numerical problems, so the SVD can be used instead.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tcolorbox}
\end_layout

\end_inset


\end_layout

\begin_layout Theorem
Any matrix 
\begin_inset Formula $A\in\mathbb{R}^{m\times n}$
\end_inset

, 
\begin_inset Formula $m>n$
\end_inset

, can be expressed as
\begin_inset Formula 
\[
A=U\Sigma V^{T},
\]

\end_inset

 where 
\begin_inset Formula $U\in\mathbb{R}^{m\times n}$
\end_inset

 has orthonormal columns 
\begin_inset Formula $\left(U^{T}U=I\right)$
\end_inset

, 
\begin_inset Formula $\Sigma\in\mathbb{R}^{n\times n}$
\end_inset

 is diagonal and contains the singular values in its diagonal and 
\begin_inset Formula $V\in\mathbb{R}^{n\times n}$
\end_inset

 is orthonormal 
\begin_inset Formula $\left(V^{-1}=V^{T}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{tcolorbox}
\end_layout

\end_inset


\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $C=A^{T}A\in\mathbb{R}^{n\times n}$
\end_inset

.
 
\begin_inset Formula $C$
\end_inset

 is square, symmetric and positive semidefinite.
 Therefore, 
\begin_inset Formula $C$
\end_inset

 is diagonalizable, so it can be written as
\begin_inset Formula 
\[
C=V\Lambda V^{T},
\]

\end_inset

 where 
\begin_inset Formula $V=\left(v_{i}\right)_{i=1,...,n}$
\end_inset

 is orthogonal and 
\begin_inset Formula $\Lambda=diag\left(\lambda_{1},...,\lambda_{d}\right)$
\end_inset

 with 
\begin_inset Formula $\lambda_{1}\geq...\geq\lambda_{r}>0=\lambda_{r+1}=...=\lambda_{n}$
\end_inset

 and 
\begin_inset Formula $r$
\end_inset

 is 
\begin_inset Formula $rank\left(A\right)\leq n$
\end_inset

.
 
\end_layout

\begin_layout Proof
Now, define 
\begin_inset Formula $\sigma_{i}=\sqrt{\lambda_{i}}$
\end_inset

, and form the matrix
\begin_inset Formula 
\[
\Sigma=\left[\begin{array}{cc}
diag\left(\sigma_{1},...,\sigma_{r}\right) & 0_{r\times\left(n-r\right)}\\
0_{\left(m-r\right)\times r} & 0_{\left(m-r\right)\times\left(n-r\right)}
\end{array}\right].
\]

\end_inset

 Define also
\begin_inset Formula 
\[
u_{i}=\frac{1}{\sigma_{i}}Xv_{i}\in\mathbb{R}^{m},i=1,...,r.
\]

\end_inset

 Then, this vectors are orthonormal:
\begin_inset Formula 
\[
u_{i}^{T}u_{j}=\left(\frac{1}{\sigma_{i}}Xv_{i}\right)^{T}\left(\frac{1}{\sigma_{j}}Xv_{j}\right)=\frac{1}{\sigma_{i}\sigma_{j}}v_{i}^{T}X^{T}Xv_{j}=\frac{1}{\sigma_{i}\sigma_{j}}v_{i}^{T}Cv_{j}\overset{*}{=}\frac{1}{\sigma_{i}\sigma_{j}}v_{i}^{T}\left(\lambda_{j}v_{j}\right)\overset{\left(\lambda_{j}=\sigma_{j}^{2}\right)}{=}\frac{\sigma_{j}}{\sigma_{i}}v_{i}^{T}v_{j}\overset{**}{=}0,
\]

\end_inset

 where 
\begin_inset Formula $\left(*\right)$
\end_inset

 is because 
\begin_inset Formula $V$
\end_inset

 is formed with the eigenvectors of 
\begin_inset Formula $C$
\end_inset

, and 
\begin_inset Formula $\left(**\right)$
\end_inset

 is because 
\begin_inset Formula $V$
\end_inset

 is orthonormal.
\end_layout

\begin_layout Proof
Now, we can complete the base with 
\begin_inset Formula $u_{r+1},...,u_{n}$
\end_inset

 (using Gram-Schmidt) in such a way that
\begin_inset Formula 
\[
U=\left[u_{1},...,u_{r},u_{r+1},...,u_{n}\right]\in\mathbb{R}^{n\times n}
\]

\end_inset

 is column orthonormal.
\end_layout

\begin_layout Proof
Now, if it is the case that 
\begin_inset Formula $XV=U\Sigma$
\end_inset

, then 
\begin_inset Formula 
\[
X=XVV^{T}=U\Sigma V^{T},
\]

\end_inset

 so it is only left to see that indeed this holds.
 Consider two cases:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $1\leq i\leq r:$
\end_inset

 
\begin_inset Formula $Xv_{i}=u_{i}\Sigma$
\end_inset

 by the definition of 
\begin_inset Formula $u_{i}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $r+1\leq i\leq n:$
\end_inset

 It is 
\begin_inset Formula $Xv_{i}=0,$
\end_inset

 because 
\begin_inset Formula $X^{T}Xv_{i}=Cv_{i}=\lambda_{i}v_{i}\overset{i>r}{=}0$
\end_inset

.
 As 
\begin_inset Formula $X,v_{i}\neq0$
\end_inset

, it must be 
\begin_inset Formula $Xv_{i}=0$
\end_inset

.
 On the other side of the equation we also have 0 because 
\begin_inset Formula $u_{i}\Sigma=u_{i}\sigma_{i}=0$
\end_inset

 as 
\begin_inset Formula $i>r$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
This, added to the fact that if 
\begin_inset Formula $X$
\end_inset

 has full rank, 
\begin_inset Formula $X^{T}X$
\end_inset

 is invertible and all its eigenvalues non null, gives us: 
\begin_inset Formula 
\begin{align*}
\theta_{lse}= & \left(X^{T}X\right)^{-1}X^{T}y\\
= & \left(\left(U\Sigma V^{T}\right)U\Sigma V^{T}\right)^{-1}\left(U\Sigma V^{T}\right)^{T}y\\
= & \left(V\Sigma U^{T}U\Sigma V^{T}\right)^{-1}V\Sigma U^{T}y\\
= & \Sigma^{-2}V\Sigma U^{T}y=V\Sigma^{-1}U^{T}y\\
= & V\cdot diag\left(\frac{1}{\sigma_{1}},...,\frac{1}{\sigma_{n}}\right)\cdot U^{T}y.
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection*
Intuitive interpretation
\end_layout

\begin_layout Standard
The intuition behind the SVD is summarized in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SVD-visual."
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Basically, every linear transformation can be decomposed into a rotation,
 a scaling and a simpler transformation (column orthogonal).
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename SVD.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
SVD visual.
\begin_inset CommandInset label
LatexCommand label
name "fig:SVD-visual."

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The intuition behind SVD lies in the idea of finding a low-rank approximation
 of a given matrix.
 The rank of a matrix is the number of linearly independent rows or columns
 it contains.
 A high-rank matrix has many linearly independent rows or columns, which
 makes it complex and difficult to analyze.
 On the other hand, a low-rank matrix has fewer linearly independent rows
 or columns, which makes it simpler and easier to analyze.
\end_layout

\begin_layout Standard
SVD provides a way to find the best possible low-rank approximation of a
 given matrix by decomposing it into three components.
 The left singular vectors represent the 
\series bold
direction of maximum variance
\series default
 in the data, while the right singular vectors represent the 
\series bold
direction of maximum correlation
\series default
 between the variables.
 The singular values represent the 
\series bold
magnitude of the variance or correlation
\series default
 in each direction.
\end_layout

\begin_layout Standard
By truncating the diagonal matrix of singular values to keep only the top-k
 values, we can obtain a low-rank approximation of the original matrix that
 retains most of the important information.
 This is useful for reducing the dimensionality of data, compressing images,
 and solving linear equations, among other applications.
\end_layout

\begin_layout Example
How to use SVD in Python and Matlab.
\end_layout

\begin_layout Example
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset listings
lstparams "language=Python"
inline false
status open

\begin_layout Plain Layout

import numpy as np
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

U, d, Vt = np.linalg.svd(X, full_matrices=False)
\end_layout

\begin_layout Plain Layout

D = np.diag(1/d)
\end_layout

\begin_layout Plain Layout

theta = Vt.T @ D @ U.T @ y
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
SVD in Python.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Example
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset listings
lstparams "language=Matlab"
inline false
status open

\begin_layout Plain Layout

[U, d, V] = svd(X)
\end_layout

\begin_layout Plain Layout

D = diag(diag(1./d))
\end_layout

\begin_layout Plain Layout

theta = V'*D*U'*y
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
SVD in Matlab.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Things that could go wrong when using linear regression
\end_layout

\begin_layout Subsubsection
Our independent variable is not enough
\end_layout

\begin_layout Standard
It is possible that our variable 
\begin_inset Formula $X$
\end_inset

 does not provide enough information to predict 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado7.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The independent variable does not provide enough information.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
The relationship between the variables is not linear (underfitting)
\begin_inset CommandInset label
LatexCommand label
name "subsec:The-relationship-between"

\end_inset


\end_layout

\begin_layout Standard
It is also possible that the variables are related in non-linear ways.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado6.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The variables are not linearly related.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Outliers affect the fit
\end_layout

\begin_layout Standard
In the presence of outliers, the model obtained can be distorted, leading
 to bad results.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado8.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The outlier distort the fit.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Basis Functions
\end_layout

\begin_layout Standard
In order to fix the second problem (Subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:The-relationship-between"
plural "false"
caps "false"
noprefix "false"

\end_inset

), we can make use of basis functions.
 The idea is to apply different transformations to the data, so that we
 can extend the expressive power of our model.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tcolorbox}
\end_layout

\end_inset


\end_layout

\begin_layout Definition
A 
\series bold
feature mapping
\series default
 is a non-linear transformation of the inputs 
\begin_inset Formula $\phi:\mathbb{R}^{m}\rightarrow\mathbb{R}^{k}$
\end_inset

.
\end_layout

\begin_layout Definition
The resulting 
\series bold
predictive function
\series default
 or model is 
\begin_inset Formula $y=\phi\left(x\right)\theta$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{tcolorbox}
\end_layout

\end_inset


\end_layout

\begin_layout Example
For example, we can consider the 
\series bold
polynomial expansion of degree 
\begin_inset Formula $k$
\end_inset


\series default
, which is a commonly used feature mapping that approximates the relationship
 between the independent variable 
\begin_inset Formula $x$
\end_inset

 and the dependent variable 
\begin_inset Formula $y$
\end_inset

 to be polynomial of degree 
\begin_inset Formula $k$
\end_inset

, i.e.:
\begin_inset Formula 
\[
y=\theta_{0}+\theta_{1}x+\theta_{2}x^{2}+...+\theta_{k}x^{k}.
\]

\end_inset

 The feature mapping is 
\begin_inset Formula $\phi\left(x\right)=\left(\begin{array}{ccccc}
1 & x & x^{2} & ... & x^{k}\end{array}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Note that the idea is to transform the data so that the fit is still linear,
 even if the relationship is not.
 Of course, this requires to apply the same transformation whenever we receive
 an input for which we want to make predictions.
 Also, the resulting model is more complex, so 
\series bold
complexity control
\series default
 is necessary to avoid overfitting.
\end_layout

\begin_layout Standard
When we apply 
\begin_inset Formula $\phi$
\end_inset

 to the input matrix 
\begin_inset Formula $X$
\end_inset

, we get a new input matrix, given by
\begin_inset Formula 
\[
\Phi=\left(\begin{array}{c}
\phi\left(x_{1}\right)\\
\phi\left(x_{2}\right)\\
\vdots\\
\phi\left(x_{n}\right)
\end{array}\right)=\left(\begin{array}{cccc}
\phi_{1}\left(x_{1}\right) & \phi_{2}\left(x_{1}\right) & \dots & \phi_{m}\left(x_{1}\right)\\
\phi_{1}\left(x_{2}\right) & \phi_{2}\left(x_{2}\right) & \dots & \phi_{m}\left(x_{2}\right)\\
\vdots & \vdots & \ddots & \vdots\\
\phi_{1}\left(x_{n}\right) & \phi_{2}\left(x_{n}\right) & \dots & \phi_{m}\left(x_{n}\right)
\end{array}\right),
\]

\end_inset

 and we obtain the optimal solution as before:
\begin_inset Formula 
\[
\theta_{min}=\arg\min_{\theta}\left(y-\Phi\theta\right)^{T}\left(y-\Phi\theta\right)=\left(\Phi^{T}\Phi\right)^{-1}\Phi^{T}y.
\]

\end_inset


\end_layout

\begin_layout Example
A MATLAB example
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

% This LaTeX was auto-generated from MATLAB code.
\end_layout

\begin_layout Plain Layout

% To make changes, update the MATLAB code and export to LaTeX again.
\end_layout

\begin_layout Plain Layout


\backslash
sloppy
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
matlabtitle{Example 2.3}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

First, we define the dataset.
 In this case $y=e^x$.
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

x=linspace(1,5,15)';
\end_layout

\begin_layout Plain Layout

y=exp(x);
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

Now, we first see what happens with linear regression:
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

b1 = X
\backslash
y;
\end_layout

\begin_layout Plain Layout

yCalc1 = X*b1;
\end_layout

\begin_layout Plain Layout

figure;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

scatter(x,y,'filled')
\end_layout

\begin_layout Plain Layout

hold on 
\end_layout

\begin_layout Plain Layout

plot(x,yCalc1,'LineWidth',2)
\end_layout

\begin_layout Plain Layout

xlabel('X')
\end_layout

\begin_layout Plain Layout

ylabel('Y')
\end_layout

\begin_layout Plain Layout

title('Linear Regression Relation Between X & Y')
\end_layout

\begin_layout Plain Layout

legend('Data','Linear regression')
\end_layout

\begin_layout Plain Layout

grid on
\end_layout

\begin_layout Plain Layout

hold off
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename scripts/example_2_3_images/figure_0.eps
	scale 50

\end_inset


\end_layout

\end_deeper
\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

As we can see, the model does not fit the data addecuately.
 We can use a feature mapping and repeat the process with the transformed
 input:
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

X = feat_map(x)
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

X = 15x3    
\end_layout

\begin_layout Plain Layout

    1.0000    1.0000    2.7183
\end_layout

\begin_layout Plain Layout

    1.0000    1.2857    3.6173
\end_layout

\begin_layout Plain Layout

    1.0000    1.5714    4.8135
\end_layout

\begin_layout Plain Layout

    1.0000    1.8571    6.4054
\end_layout

\begin_layout Plain Layout

    1.0000    2.1429    8.5238
\end_layout

\begin_layout Plain Layout

    1.0000    2.4286   11.3427
\end_layout

\begin_layout Plain Layout

    1.0000    2.7143   15.0938
\end_layout

\begin_layout Plain Layout

    1.0000    3.0000   20.0855
\end_layout

\begin_layout Plain Layout

    1.0000    3.2857   26.7281
\end_layout

\begin_layout Plain Layout

    1.0000    3.5714   35.5674
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

b2 = X
\backslash
y;
\end_layout

\begin_layout Plain Layout

yCalc2 = X*b2;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

scatter(x,y,'filled')
\end_layout

\begin_layout Plain Layout

hold on 
\end_layout

\begin_layout Plain Layout

plot(x,yCalc2,'LineWidth',2)
\end_layout

\begin_layout Plain Layout

xlabel('X')
\end_layout

\begin_layout Plain Layout

ylabel('Y')
\end_layout

\begin_layout Plain Layout

title('Linear Regression Relation Between exp(X) & Y')
\end_layout

\begin_layout Plain Layout

legend('Data','Linear regression','Location','north')
\end_layout

\begin_layout Plain Layout

grid on
\end_layout

\begin_layout Plain Layout

hold off
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename scripts/example_2_3_images/figure_1.eps
	scale 50

\end_inset


\end_layout

\end_deeper
\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

As we can see, the model is now perfectly fitting the data!
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

function X=feat_map(x)
\end_layout

\begin_layout Plain Layout

    X = [ones(size(x)) x exp(x)];
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Probabilistic approach
\end_layout

\begin_layout Standard
A review on probability theory, Bayes theorem and Bayesian Learning is in
 Appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Notes-on-probability"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Least squares regression from a probabilistic perspective
\end_layout

\begin_layout Standard
We are noe going to derive the linear regression estimates using the principle
 of maximum likelihood and the univariate Gaussian distribution, whose probabili
ty density function is given by
\begin_inset Formula 
\[
p\left(x\right)=\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{1}{2\sigma^{2}}\left(x-\mu\right)^{2}}.
\]

\end_inset

 Given a sample 
\begin_inset Formula $D=\left\{ x_{1},...,x_{n}\right\} $
\end_inset

, where 
\begin_inset Formula $x_{i}\sim\mathcal{N}\left(\mu,\sigma\right)$
\end_inset

, we define its likelihood as the function
\begin_inset Formula 
\[
\mathcal{L}\left(\mu,\sigma,D\right)=P\left(D;\mu,\sigma\right)=\prod_{i}p\left(x_{i};\mu,\sigma\right).
\]

\end_inset

 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Likelihood-example."
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can see how the likelihood relates to '
\emph on
how likely it is that our points have been created froma certain distribution
\emph default
', because the red outcomes are more likely to appear from the blue distribution
 than the green ones.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado9.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Likelihood-example."

\end_inset

Likelihood example.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Now, this can be used to select the distribution that best matches our data.
 As an easy approach, suppose we want to decide between two distributions
 
\begin_inset Formula $F_{1}$
\end_inset

 and 
\begin_inset Formula $F_{2}$
\end_inset

, and we have a dataset 
\begin_inset Formula $D$
\end_inset

.
 To decide, we can compute 
\begin_inset Formula $\mathcal{L}\left(F_{1},D\right)$
\end_inset

 and 
\begin_inset Formula $\mathcal{L}\left(F_{2},D\right)$
\end_inset

 and select the distribution whose likelihood is greater.
 This is visually exemplified in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Using-likelihood-to"
plural "false"
caps "false"
noprefix "false"

\end_inset

, where we can see that given the three red outcomes and the two distributions
 (blue and green), the blue one should be preferred, because it mazimizes
 the likelihood.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado10.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Using likelihood to select the distribution.
\begin_inset CommandInset label
LatexCommand label
name "fig:Using-likelihood-to"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
This way, we can think of the likelihood of a function of the unknown parameters
 of the distribution, with the dataset fixed, and we can maximize this function
 to obtain the parameters that best describe our data.
\end_layout

\begin_layout Standard
In the probabilistic setting of linear regression, we assume that each label
 
\begin_inset Formula $y_{i}$
\end_inset

 we observe is normally distributed, with mean 
\begin_inset Formula $\mu=x_{i}\theta$
\end_inset

 and variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

:
\begin_inset Formula 
\[
y_{i}=x_{i}\theta+\varepsilon_{i},
\]

\end_inset

 where 
\begin_inset Formula $\varepsilon_{i}\sim\mathcal{N}\left(0,\sigma^{2}\right)$
\end_inset

.
 This way, we seek to obtain 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}$
\end_inset

 that best describe our data.
 Remember that
\begin_inset Formula 
\[
y=\left(\begin{array}{c}
y_{1}\\
\vdots\\
y_{n}
\end{array}\right),\ X=\left(\begin{array}{cccc}
1 & x_{11} & ... & x_{1d}\\
\vdots & \vdots & \ddots & \vdots\\
1 & x_{n1} & ... & x_{nd}
\end{array}\right),
\]

\end_inset

 so that the likelihood of the parameter vector 
\begin_inset Formula $\theta$
\end_inset

 is given by
\begin_inset Formula 
\[
\mathcal{L}\left(\theta,\sigma\right)=P\left(y|X;\theta,\sigma^{2}\right)=\prod_{i=1}^{n}p\left(y_{i}|x_{i};\theta,\sigma^{2}\right).
\]

\end_inset

 It is usual to maximize the log-likelihood instead, basically because the
 likelihood tends to give values too close to zero (we may be multiplying
 thousands of small values), so numerical problems may arise.
 Thus:
\begin_inset Formula 
\begin{align*}
l\left(\theta,\sigma^{2}\right)=\log\mathcal{L}\left(\theta,\sigma^{2}\right)= & \log\prod_{i=1}^{n}p\left(y_{i}|x_{i};\theta,\sigma^{2}\right)\\
= & \sum_{i=1}^{n}\log p\left(y_{i}|x_{i};\theta,\sigma^{2}\right)\\
= & \sum_{i=1}^{n}\log\left[\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{1}{2\sigma^{2}}\left(y_{i}-x_{i}\theta\right)^{2}}\right]\\
= & \sum_{i=1}^{n}\left(\log\frac{1}{\sqrt{2\pi\sigma^{2}}}-\frac{1}{2\sigma^{2}}\left(y_{i}-x_{i}\theta\right)^{2}\right)\\
= & -\frac{n}{2}\log\left(2\pi\sigma^{2}\right)-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(y_{i}-x_{i}\theta\right)^{2}\\
= & -\frac{n}{2}\log\left(2\pi\sigma^{2}\right)-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(y-X\theta\right)^{T}\left(y-X\theta\right).
\end{align*}

\end_inset

 At this point, we differentiate and set equal to 0:
\begin_inset Formula 
\[
\frac{\partial}{\partial\theta}l\left(\theta,\sigma^{2}\right)=-\frac{1}{2\sigma^{2}}\left(-2X^{T}y+2X^{T}X\theta\right)=0
\]

\end_inset


\begin_inset Formula 
\[
\frac{\partial}{\partial\sigma^{2}}l\left(\theta,\sigma^{2}\right)=-\frac{n}{2\sigma^{2}}+\frac{1}{2\sigma^{4}}\left(y-X\theta\right)^{T}\left(y-X\theta\right)=0,
\]

\end_inset

 obtaining
\begin_inset Formula 
\[
\theta_{ML}=\left(X^{T}X\right)^{-1}X^{T}y
\]

\end_inset

 and
\begin_inset Formula 
\[
\sigma_{ML}^{2}=\frac{1}{n}\left(y-X\theta\right)^{T}\left(y-X\theta\right)=\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}-x_{i}\theta_{ML}\right)^{2}=MSE.
\]

\end_inset

 It is noticeable that the maximum likelihood estimates concide with the
 estimates we found minimizing the squared error.
 This is a consequence of assuming gaussian noise, and other types of distributi
on would give us the same estimates as minimizing a different error function.
\end_layout

\begin_layout Subsection
Bias-Variance decomposition
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $f:\mathbb{R}^{d}\rightarrow\mathbb{R}$
\end_inset

 be the true function that we are trying to approximate and 
\begin_inset Formula $D=\left\{ \left(x_{1},y_{1}\right),...,\left(x_{n},y_{n}\right)\right\} $
\end_inset

 a finite training dataset, where 
\begin_inset Formula $y_{i}=f\left(x_{i}\right)+\varepsilon_{i}$
\end_inset

 and 
\begin_inset Formula $\varepsilon_{i}\sim\mathcal{N}\left(0,\sigma^{2}\right)$
\end_inset

.
 Let 
\begin_inset Formula $x\in\mathbb{R}^{d}$
\end_inset

 be a test data point.
 The setup is using 
\begin_inset Formula $D$
\end_inset

 to train a model 
\begin_inset Formula $\hat{f}$
\end_inset

 that we want to use to make predictions
\begin_inset Formula 
\[
\hat{y}_{D}=\hat{f}\left(x\right).
\]

\end_inset

 We are going to see how the expected squared error 
\begin_inset Formula $\left(y-\hat{y}_{D}\right)^{2}$
\end_inset

, where 
\begin_inset Formula $y$
\end_inset

 is the real value, can be decomosed as a sum of the following components:
\end_layout

\begin_layout Itemize

\series bold
Irreducible error
\series default
: given by 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize

\series bold
Bias
\series default
: the systematic limitation that the modelling assumptions impose.
 For example, if we choose linear approximations, we will never be able
 to model non-linear data well enough.
\end_layout

\begin_layout Itemize

\series bold
Variance
\series default
: refers to the sensitivity of the model to the training set 
\begin_inset Formula $D$
\end_inset

.
 The more the model varies when the training set is changed, the higher
 variance it has.
\end_layout

\begin_layout Standard
Let's do this:
\begin_inset Formula 
\begin{align*}
E\left[\left(y-\hat{y}_{D}\right)^{2}\right]= & E\left[\left(f\left(x\right)+\varepsilon-\hat{y}_{D}\right)^{2}\right]=E\left[\left(f\left(x\right)+\varepsilon-\hat{y}_{D}+E\left[\hat{y}_{D}\right]-E\left[\hat{y}_{D}\right]\right)^{2}\right]\\
= & E\left[\left({\color{red}\left({\color{red}f\left(x\right)-E\left[\hat{y}_{D}\right]}\right)}+{\color{teal}\varepsilon}+{\color{blue}\left(E\left[\hat{y}_{D}\right]-\hat{y}_{D}\right)}\right)^{2}\right]\\
= & E\left[{\color{red}\left({\color{red}f\left(x\right)-E\left[\hat{y}_{D}\right]}\right)}^{2}\right]+E\left[{\color{teal}\varepsilon}^{2}\right]+E\left[{\color{blue}\left(E\left[\hat{y}_{D}\right]-\hat{y}_{D}\right)}^{2}\right]\\
 & +2E\left[{\color{red}\left({\color{red}f\left(x\right)-E\left[\hat{y}_{D}\right]}\right)}{\color{teal}\varepsilon}\right]+2E\left[{\color{red}\left({\color{red}f\left(x\right)-E\left[\hat{y}_{D}\right]}\right)}{\color{blue}\left(E\left[\hat{y}_{D}\right]-\hat{y}_{D}\right)}\right]\\
 & +2E\left[{\color{teal}\varepsilon}{\color{blue}\left(E\left[\hat{y}_{D}\right]-\hat{y}_{D}\right)}\right]\\
= & E\left[{\color{red}\left({\color{red}f\left(x\right)-E\left[\hat{y}_{D}\right]}\right)}^{2}\right]+\sigma^{2}+E\left[{\color{blue}\left(E\left[\hat{y}_{D}\right]-\hat{y}_{D}\right)}^{2}\right]\\
 & +2E\left[\left({\color{red}f\left(x\right)-E\left[\hat{y}_{D}\right]}\right)\right]\cancelto{0}{E\left[{\color{teal}\varepsilon}\right]}+2E\left[{\color{red}\left({\color{red}f\left(x\right)-E\left[\hat{y}_{D}\right]}\right)}\right]\cancelto{0}{E\left[{\color{blue}\left(E\left[\hat{y}_{D}\right]-\hat{y}_{D}\right)}\right]}\\
 & +2E\left[{\color{blue}\left(E\left[\hat{y}_{D}\right]-\hat{y}_{D}\right)}\right]\cancelto{0}{E\left[{\color{teal}\varepsilon}\right]}\\
= & E\left[{\color{red}\left({\color{red}f\left(x\right)-E\left[\hat{y}_{D}\right]}\right)}^{2}\right]+\sigma^{2}+E\left[{\color{blue}\left(E\left[\hat{y}_{D}\right]-\hat{y}_{D}\right)}^{2}\right].
\end{align*}

\end_inset

 And we now define
\begin_inset Formula 
\[
Bias\left[\hat{y}_{D}\right]=E\left[{\color{red}\left({\color{red}f\left(x\right)-E\left[\hat{y}_{D}\right]}\right)}^{2}\right],
\]

\end_inset


\begin_inset Formula 
\[
Variance\left[\hat{y}_{D}\right]=E\left[{\color{blue}\left(E\left[\hat{y}_{D}\right]-\hat{y}_{D}\right)}^{2}\right].
\]

\end_inset

 The Bias reflects the expected difference between our assumed model and
 the real function, while the variance reflects the difference between the
 assumed model and the obtained model.
 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Visual-representation-of"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can see:
\end_layout

\begin_layout Itemize
The linear model has high bias and low variance.
\end_layout

\begin_layout Itemize
The polynomial of degree 3 has low bias and moderate variance.
\end_layout

\begin_layout Itemize
The polynomial of degree 8 has low bias but high variance.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado14.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Visual representation of the Bias-Variance errors.
\begin_inset CommandInset label
LatexCommand label
name "fig:Visual-representation-of"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
A summary of commonly used errors used for regression is shown in Table
 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Common-error-functions."
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Name
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Abbreviation
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Formula
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
mean squared error
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
MSE
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}-x_{i}\theta\right)^{2}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
root mean squared error
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
RMSE
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sqrt{MSE}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
normalized root mean squared error
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
NRMSE
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sqrt{\frac{MSE}{Var\left(y\right)}}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
coefficient of determination
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $R^{2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1-NRMSE
\begin_inset Formula $^{2}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
mean absolute error
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
MAE
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}\left|y_{i}-x_{i}\theta\right|$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Common error functions.
\begin_inset CommandInset label
LatexCommand label
name "tab:Common-error-functions."

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Ridge Regression from Gaussian prior
\end_layout

\begin_layout Standard
We are going to consider MAP estimates in this section, which are explained
 in the Annex 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Notes-on-probability"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
Assume isotropic Gaussian prior on 
\begin_inset Formula $d$
\end_inset

-dimensional 
\begin_inset Formula $\theta$
\end_inset

, i.e., 
\begin_inset Formula $\theta\sim\mathcal{N}\left(\mu=0,\Sigma=\tau^{2}I\right)$
\end_inset

, so that 
\begin_inset Formula $\Sigma^{-1}=\frac{1}{\tau^{2}}I$
\end_inset

 and 
\begin_inset Formula $\det\Sigma=\tau^{2d}$
\end_inset

.
 Then, on one hand, we have
\begin_inset Formula 
\begin{align*}
P\left(\theta;\mu=0,\Sigma=\tau^{2}I\right)= & \frac{1}{\det\left(\Sigma\right)^{\frac{1}{2}}\left(2\pi\right)^{\frac{d}{2}}}\exp\left\{ -\frac{1}{2}\left(y-\mu\right)^{T}\Sigma^{-1}\left(y-\mu\right)\right\} \\
= & \frac{1}{\left(2\pi\tau^{2}\right)^{\frac{d}{2}}}\exp\left\{ -\frac{1}{2\tau^{2}}\theta^{T}\theta\right\} \\
= & \frac{1}{\left(2\pi\tau^{2}\right)^{\frac{d}{2}}}\exp\left\{ -\frac{\left\Vert \theta\right\Vert ^{2}}{2\tau^{2}}\right\} .
\end{align*}

\end_inset

 On the other hand, it is
\begin_inset Formula 
\begin{align*}
P\left(\theta|y,X\right)\propto & P\left(y|X,\theta\right)P\left(\theta\right)\\
\propto & \exp\left\{ -\frac{1}{2\sigma^{2}}\left(y-X\theta\right)^{T}\left(y-X\theta\right)\right\} \exp\left\{ -\frac{\left\Vert \theta\right\Vert ^{2}}{2\tau^{2}}\right\} \\
= & \exp\left\{ -\frac{1}{2\sigma^{2}}\left(y-X\theta\right)^{T}\left(y-X\theta\right)-\frac{\left\Vert \theta\right\Vert ^{2}}{2\tau^{2}}\right\} .
\end{align*}

\end_inset

 To obtain the MAP, we maximize the log of this expression:
\begin_inset Formula 
\begin{align*}
\theta_{MAP}= & \arg\max_{\theta}\log\left[P\left(y|X,\theta\right)P\left(\theta\right)\right]\\
= & \arg\max_{\theta}\left[-\frac{1}{2\sigma^{2}}\left(y-X\theta\right)^{T}\left(y-X\theta\right)-\frac{\left\Vert \theta\right\Vert ^{2}}{2\tau^{2}}\right]\\
= & \arg\min_{\theta}\left[\left(y-X\theta\right)^{T}\left(y-X\theta\right)+\frac{\sigma^{2}}{\tau^{2}}\left\Vert \theta\right\Vert ^{2}\right]\\
= & \arg\min_{\theta}\left[\left(y-X\theta\right)^{T}\left(y-X\theta\right)+\lambda\left\Vert \theta\right\Vert ^{2}\right]
\end{align*}

\end_inset

 which is the 
\series bold
ridge regression 
\series default
estimate with 
\begin_inset Formula $\lambda=\frac{\sigma^{2}}{\tau^{2}}$
\end_inset

.
 We can now differenciate this expression to find its minimum:
\begin_inset Formula 
\begin{align*}
\frac{\partial}{\partial\theta}\left[\left\Vert y-X\theta\right\Vert ^{2}+\lambda\left\Vert \theta\right\Vert ^{2}\right]= & -y^{T}X-X^{T}y+2X^{T}X\theta+2\lambda\theta=-2X^{T}y+2X^{T}X\theta+2\lambda\theta=0\\
\iff & \left(2X^{T}X+2\lambda I\right)\theta=2X^{T}y\\
\iff & \theta_{MAP}=\theta_{ridge}=\left(X^{T}X+\lambda I\right)^{-1}X^{T}y.
\end{align*}

\end_inset

 
\end_layout

\begin_layout Remark
There are a few remarks here:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\lambda$
\end_inset

 controls the complexity of the solution 
\begin_inset Formula $\theta$
\end_inset

, the bigger 
\begin_inset Formula $\lambda$
\end_inset

 is, the smaller 
\begin_inset Formula $\theta$
\end_inset

 tends to be, leading to simpler solutions.
\end_layout

\begin_layout Itemize
\begin_inset Formula $X^{T}X+\lambda I$
\end_inset

 is guaranteed to be non-singular and behaves better numerically than 
\begin_inset Formula $X^{T}X$
\end_inset

, specially if there is high correlation in the columns of 
\begin_inset Formula $X$
\end_inset

, or if there are few observations relative to the amount of variables.
\end_layout

\begin_layout Itemize
A general approach when we have a regularized objective function is to use
 models that are potentially more complex than needed, and then adjust 
\begin_inset Formula $\lambda$
\end_inset

 until obtaining good result and simpler models.
\end_layout

\end_deeper
\begin_layout Subsubsection
Tuning 
\begin_inset Formula $\lambda$
\end_inset


\end_layout

\begin_layout Subsubsection*
Cross-validation
\end_layout

\begin_layout Standard
Given a dataset 
\begin_inset Formula $D$
\end_inset

, the 
\begin_inset Formula $k$
\end_inset

-cross-validation approach starts by separating 
\begin_inset Formula $D$
\end_inset

 into two subsets:
\end_layout

\begin_layout Itemize
The 
\series bold
training dataset
\series default
, 
\begin_inset Formula $D_{train}$
\end_inset

.
\end_layout

\begin_layout Itemize
The 
\series bold
test dataset
\series default
, 
\begin_inset Formula $D_{test}$
\end_inset

.
\end_layout

\begin_layout Standard
These are obtained in such a way that:
\end_layout

\begin_layout Enumerate
\begin_inset Formula $D_{train}\cap D_{test}=\emptyset$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $D_{train}\cup D_{test}=D$
\end_inset

.
\end_layout

\begin_layout Standard
Now, we divide 
\begin_inset Formula $D_{train}$
\end_inset

 into 
\begin_inset Formula $k$
\end_inset

 subsets of equal size, 
\begin_inset Formula $\left\{ D_{train,i}\right\} _{i=1}^{k}$
\end_inset

, called 
\series bold
folds
\series default
, and imagine we want to decide on a set of values for our model's parameter
 
\begin_inset Formula $\lambda\in\Lambda=\left\{ \lambda_{1},...,\lambda_{l}\right\} $
\end_inset

.
\end_layout

\begin_layout Enumerate
For each 
\begin_inset Formula $\lambda\in\Lambda$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Enumerate
For each 
\begin_inset Formula $i=1,...,k$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Enumerate
Train the model in 
\begin_inset Formula $D_{train}\setminus D_{train,i}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Evaluate the model in 
\begin_inset Formula $D_{train,i}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
Average the 
\begin_inset Formula $k$
\end_inset

 evaluations to obtain an estimation of the performance of the model.
\end_layout

\end_deeper
\begin_layout Enumerate
Select 
\begin_inset Formula $\lambda$
\end_inset

 that gives us the best estimation.
\end_layout

\begin_layout Subsubsection*
Leave-one-out cross-validation (LOOCV)
\end_layout

\begin_layout Standard
Is a special case of cross-validation, in which 
\begin_inset Formula $k=n$
\end_inset

, i.e., each data point is a fold.
\end_layout

\begin_layout Subsubsection
LOOCV for Ridge regression
\end_layout

\begin_layout Standard
As a particularity of linear and ridge regression, for a given value of
 
\begin_inset Formula $\lambda$
\end_inset

, only one training is necessary for LOOCV, so we can proceed as follows:
\end_layout

\begin_layout Enumerate
For each 
\begin_inset Formula $\lambda\in\Lambda$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Enumerate
Compute the optimal solution
\begin_inset Formula 
\[
\hat{\theta}_{\lambda}=\left(X^{T}X+\lambda I\right)^{-1}X^{T}y.
\]

\end_inset


\end_layout

\begin_layout Enumerate
Compute the 
\emph on
hat matrix 
\emph default
or smoothing matrix
\begin_inset Formula 
\[
H_{\lambda}=\left(h_{ij}\right)_{ij}=X\left(X^{T}X+\lambda I\right)^{-1}X^{T}.
\]

\end_inset


\end_layout

\begin_layout Enumerate
Compute LOOCV directly for each 
\begin_inset Formula $\lambda$
\end_inset

, without folding
\begin_inset Formula 
\[
LOOCV\left(\lambda\right)=\frac{1}{n}\sum_{i=1}^{n}\left(\frac{y_{i}-f\left(x_{i}\right)\theta_{\lambda}}{1-h_{ii}}\right)^{2}.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Return 
\begin_inset Formula $\lambda$
\end_inset

 with minimum LOOCV.
\end_layout

\begin_layout Subsubsection
Generalized Cross-Validation (GCV)
\end_layout

\begin_layout Standard
Generalized Cross-Validation (GCV) is a model selection technique used to
 estimate the performance of a model in terms of prediction accuracy.
 GCV is particularly useful for choosing the optimal parameters in regularizatio
n or smoothing methods, where the goal is to balance model complexity and
 goodness of fit.
 Examples of such methods include ridge regression, LASSO, and smoothing
 splines.
\end_layout

\begin_layout Standard
The main idea of GCV is to approximate the leave-one-out cross-validation
 (LOOCV) error without actually performing the computationally expensive
 process of fitting the model to all but one data point multiple times.
 Also, it is more computationally stable than the previous approach.
\end_layout

\begin_layout Standard
The GCV score is defined as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
GCV\left(\lambda\right)=\frac{1}{n}\sum_{i=1}^{n}\left(\frac{y_{i}-f\left(x_{i}\right)\theta_{\lambda}}{1-\frac{Tr\left(H_{\lambda}\right)}{n}}\right)^{2}=\frac{MSE_{\lambda}}{\left(1-\frac{Tr\left(H_{\lambda}\right)}{n}\right)},
\]

\end_inset

 where 
\begin_inset Formula $Tr\left(H_{\lambda}\right)$
\end_inset

 is the trace of 
\begin_inset Formula $H_{\lambda}$
\end_inset

.
\end_layout

\begin_layout Subsection
LASSO regression
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tcolorbox}
\end_layout

\end_inset


\end_layout

\begin_layout Definition
The 
\series bold

\begin_inset Formula $\boldsymbol{Lp-}$
\end_inset

norm
\series default
 of a vector 
\begin_inset Formula $\theta$
\end_inset

 is
\begin_inset Formula 
\[
\left\Vert \theta\right\Vert _{p}=\left(\sum_{i=1}^{n}\left|\theta_{d}\right|^{p}\right)^{\frac{1}{p}}.
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{tcolorbox}
\end_layout

\end_inset


\end_layout

\begin_layout Remark
As we have seen, assuming an isotropic Gaussian prior on the parameters
 leads to ridge regression, which minimizes the L2-norm of 
\begin_inset Formula $\theta$
\end_inset

 and the squared error.
\end_layout

\begin_layout Standard
Another very common choice is 
\begin_inset Formula $p=1$
\end_inset

, which leads to 
\series bold
LASSO regression
\series default
.
 Thus, LASSO regression minimizes the L1-norm of parameters and squared
 error:
\begin_inset Formula 
\[
\theta_{LASSO}=\arg\min_{\theta}\left[\left\Vert y-X\theta\right\Vert _{2}^{2}+\lambda\left\Vert \theta\right\Vert _{1}\right].
\]

\end_inset

 In fact, LASSO regression arises assuming a Lapace distribution prior over
 the parameters.
\end_layout

\begin_layout Standard
Some characteristics:
\end_layout

\begin_layout Itemize
LASSO regularized cost function is not quadratic anymore, and it has no
 close solution, so an approximation procedure is used: the 
\series bold
least angle regression
\series default
, which provides an efficient way to compute the solutions for a list of
 possible values for 
\begin_inset Formula $\lambda>0$
\end_inset

, giving the 
\series bold
regularization path
\series default
.
\end_layout

\begin_layout Itemize
LASSO regression gives sparse solutions, in which many coefficients/coordinates
 of 
\begin_inset Formula $\theta$
\end_inset

 might be 0.
 This means that LASSO performs feature selection automatically.
\end_layout

\begin_layout Subsection
The full-Bayesian perspective
\end_layout

\begin_layout Standard
Both maximum likelihood (ML) and maximum a priori (MAP) produce point estimates
 of the parameters, while in Bayesian Learning
\begin_inset Foot
status open

\begin_layout Plain Layout
Refer to Appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Notes-on-probability"
plural "false"
caps "false"
noprefix "false"

\end_inset

 for more details.
\end_layout

\end_inset

 we want the full posterior distribution of the parameters.
 The idea is that if we know the posterior distribution of the parameters,
 then we can use all the information provided by this distribution for our
 predictions, instead of just a single point-estimate that summarizes it.
 For instance, if the probability function of the posterior is 
\begin_inset Formula $p\left(\theta|D\right)$
\end_inset

 and we receive a new input point 
\begin_inset Formula $x$
\end_inset

, then we can compute the probability of 
\begin_inset Formula $f\left(x\right)=y$
\end_inset

 by
\begin_inset Formula 
\[
p\left(y|x,D\right)=\int_{\Theta}p\left(y|x,\theta,D\right)p\left(\theta|D\right)d\theta.
\]

\end_inset

Now, when we do this for all possible values of 
\begin_inset Formula $y$
\end_inset

, we obtain the full distribution of the predictions, instead of just one
 estimation.
 Nonetheless, computing this integral is usually too hard, so it needs to
 be approximated, but in the context of linear regression all these expressions
 have close-form formulas
\begin_inset Foot
status open

\begin_layout Plain Layout
For computational speed we may use approximations as well.
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Technically, ML and MAP assume that 
\begin_inset Formula $Y\sim\mathcal{N}\left(x^{T}\theta,\sigma^{2}\right),$
\end_inset

 so a prediction for a new test point 
\begin_inset Formula $\overline{x}$
\end_inset

 is going to have a distribution 
\begin_inset Formula $\mathcal{N}\left(\overline{x}^{T}\hat{\theta},\sigma^{2}\right).$
\end_inset

 Note now the lack of flexibility of this approach, since the width of the
 normal distribution is going to be the same for any new test point, which
 may be a dangerous assumption.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $D=\left\{ \left(x_{i},y_{i}\right)\right\} _{i=1}^{n}$
\end_inset

 be our dataset, with 
\begin_inset Formula $x_{i}\in\mathbb{R}^{d}$
\end_inset

 and 
\begin_inset Formula $y_{i}\in\mathbb{R}$
\end_inset

, and assume:
\end_layout

\begin_layout Itemize
\begin_inset Formula $Y_{i},...,Y_{n}$
\end_inset

 are independent given 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $Y_{i}\sim\mathcal{N}\left(x_{i}^{T}\theta,\frac{1}{a}\right)$
\end_inset

, with 
\begin_inset Formula $a>0$
\end_inset

 being the prevision of the noise in the observations 
\begin_inset Formula $\left(a=\frac{1}{\sigma^{2}}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
A spherical or isotropic Gaussian for the parameter's prior, 
\begin_inset Formula $p\left(\theta\right)\sim\mathcal{N}\left(0,b^{-1}I\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 are known.
\end_layout

\begin_layout Itemize
The only parameter variables are the coefficients 
\begin_inset Formula $\theta=\left(\theta_{0},...,\theta_{d}\right)^{T}$
\end_inset

.
\end_layout

\begin_layout Standard
Then, the likelihood function is
\begin_inset Formula 
\[
p\left(D|\theta\right)\propto\exp\left\{ -\frac{a}{2}\left(y-X\theta\right)^{T}\left(y-X\theta\right)\right\} .
\]

\end_inset

 As usual, using Bayes, we can derive the posterior distribution
\begin_inset Formula 
\begin{align*}
p\left(\theta|D\right)\propto & p\left(D|\theta\right)p\left(\theta\right)\\
\propto & \exp\left\{ -\frac{a}{2}\left(y-X\theta\right)^{T}\left(y-X\theta\right)\right\} \exp\left\{ -\frac{b}{2}\theta^{T}\theta\right\} \\
\propto & \exp\left\{ -\frac{a}{2}\left(y-X\theta\right)^{T}\left(y-X\theta\right)-\frac{b}{2}\theta^{T}\theta\right\} .
\end{align*}

\end_inset

 Notice here that the exponent of this expression is quadratic on 
\begin_inset Formula $\theta$
\end_inset

, so it is going to be a multivariate Gaussian
\begin_inset Foot
status open

\begin_layout Plain Layout
The idea is that the product of two Gaussians is also Gaussian.
 Think of it as if you are measuring height and IQ: these variables can
 be assumed independent, and have a normal distribution.
 You can multiply them and obtain a combined normal distribution in 2-D.
 The idea here is basically the same.
\end_layout

\end_inset

.
 We need to turn the exponent into something resembling
\begin_inset Formula 
\[
\left(\theta-\mu\right)^{T}Q\left(\theta-\mu\right),
\]

\end_inset

 so that we can derive what the mean 
\begin_inset Formula $\mu$
\end_inset

 and the precision 
\begin_inset Formula $Q$
\end_inset

 of the posterior density is.
 For this, we are going to complete squares so that we can 'match the terms'
 between 
\begin_inset Formula $\left(\theta-\mu\right)^{T}Q\left(\theta-\mu\right)$
\end_inset

 and 
\begin_inset Formula $a\left(y-X\theta\right)^{T}\left(y-X\theta\right)+b\theta^{T}\theta$
\end_inset

.
 On one hand:
\begin_inset Formula 
\begin{align*}
\left(\theta-\mu\right)^{T}Q\left(\theta-\mu\right)= & \theta^{T}Q\theta-\theta Q\mu-\mu^{T}Q\theta+\mu^{T}Q\mu\\
= & \theta^{T}Q\theta-2\theta^{T}Q\mu+const.
\end{align*}

\end_inset

 We don't mind about constant terms with respect to 
\begin_inset Formula $\theta$
\end_inset

, since we only care about proportionality.
 On the other hand, we have:
\begin_inset Formula 
\begin{align*}
a\left(y-X\theta\right)^{T}\left(y-X\theta\right)+b\theta^{T}\theta= & ay^{T}y-ay^{T}X\theta-a\theta^{T}X^{T}y+a\theta^{T}X^{T}X\theta+b\theta^{T}\theta\\
= & ay^{T}y-2a\theta^{T}X^{T}y+\theta^{T}\left(aX^{T}X+bI\right)\theta.
\end{align*}

\end_inset

 From here, we obtain that
\begin_inset Formula 
\[
Q=aX^{T}X+bI,
\]

\end_inset

 and we now match 
\begin_inset Formula $-2a\theta^{T}X^{T}y$
\end_inset

 with 
\begin_inset Formula $-2\theta^{T}Q\mu$
\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Formula $Q$
\end_inset

 is invertible because it is positive definite, thanks to the 
\begin_inset Formula $+bI$
\end_inset

, with 
\begin_inset Formula $b>0$
\end_inset

.
\end_layout

\end_inset

:
\begin_inset Formula 
\[
aX^{T}y=Q\mu\iff\mu=aQ^{-1}X^{T}y.
\]

\end_inset

 Thus, the posterior probability is 
\begin_inset Formula $p\left(\theta|D\right)\sim\mathcal{N}\left(\mu,Q^{-1}\right)$
\end_inset

 with
\begin_inset Formula 
\[
Q=aX^{T}X+bI,\ \mu=aQ^{-1}X^{T}y.
\]

\end_inset

 The MAP estimate can be directly obtained from here, since the maximum
 density is obtained at the mean in any Gaussian distribution.
 Additionally, in ridge regression we let 
\begin_inset Formula $\lambda=\frac{b}{a}$
\end_inset

 and turn it into a parameter that we can tune to control complexity against
 training error.
\end_layout

\begin_layout Subsubsection
Using the posterior distribution for predictions
\end_layout

\begin_layout Standard
Let's now see how to compute the 
\series bold
predictive distribution
\series default
, i.e., 
\begin_inset Formula 
\[
p\left(y|x,D\right)=\int_{\Theta}p\left(y|x,\theta,D\right)p\left(\theta|D\right)d\theta.
\]

\end_inset

 For this, we substitute the densities
\begin_inset Formula 
\begin{align*}
p\left(y|x,D\right)= & \int_{\Theta}\mathcal{N}\left(y|x^{T}\theta,\frac{1}{a}\right)\mathcal{N}\left(\theta|Q^{-1}\right)d\theta\\
\overset{w.r.t.\ y}{\propto} & \int_{\Theta}\exp\left\{ -\frac{a}{2}\left(y-x^{T}\theta\right)^{2}\right\} \exp\left\{ -\frac{1}{2}\left(\theta-\mu\right)^{T}Q\left(\theta-\mu\right)\right\} d\theta\\
= & \int_{\Theta}\exp\left\{ -\frac{a}{2}\left(y^{2}-2\left(x^{T}\theta\right)y+\left(x^{T}\theta\right)^{2}\right)-\frac{1}{2}\left(\theta^{T}Q\theta-2\theta^{T}Q\mu+\cancel{\mu^{T}Q\mu}\right)\right\} d\theta\\
\propto & \int_{\Theta}\exp\left\{ -\frac{a}{2}\left(y^{2}-2\left(x^{T}\theta\right)y+\left(x^{T}\theta\right)^{2}\right)-\frac{1}{2}\left(\theta^{T}Q\theta-2\theta^{T}Q\mu\right)\right\} d\theta.
\end{align*}

\end_inset

 Now, our objective is to set this integral to equate (or be proportional
 to) another of the form 
\begin_inset Formula 
\[
\int_{\Theta}\mathcal{N}\left(\theta|...\right)g\left(y\right)d\theta=g\left(y\right)\int_{\Theta}\mathcal{N}\left(\theta|...\right)d\theta=g\left(y\right),
\]

\end_inset

 and finally to see that 
\begin_inset Formula $g\left(y\right)\propto\mathcal{N}\left(y|...\right)$
\end_inset

.
 We then have
\begin_inset Formula 
\begin{align*}
p\left(y|x,D\right)\propto & \int_{\Theta}\exp\left\{ -\frac{a}{2}\left(y^{2}-2\left(x^{T}\theta\right)y+\left(x^{T}\theta\right)^{2}\right)-\frac{1}{2}\left(\theta^{T}Q\theta-2\theta^{T}Q\mu\right)\right\} d\theta\\
= & \int_{\Theta}\exp\left\{ -\frac{1}{2}\left[ay^{2}-2ax^{T}\theta y+a\theta^{T}xx^{T}\theta+\theta^{T}Q\theta-2\theta^{T}Q\mu\right]\right\} d\theta\\
= & \int_{\Theta}\exp\left\{ -\frac{1}{2}\left[\theta^{T}\left(axx^{T}+Q\right)\theta-2\theta^{T}\left(xya+Q\mu\right)+ay^{2}\right]\right\} d\theta.
\end{align*}

\end_inset

 Again, we want to match to something of the form 
\begin_inset Formula $\left(\theta-m\right)^{T}L\left(\theta-m\right)=\theta^{T}L\theta-2\theta^{T}Lm+m^{T}Lm,$
\end_inset

 so
\begin_inset Formula 
\[
L=axx^{T}+Q
\]

\end_inset

 and 
\begin_inset Formula 
\[
Lm=xya+Q\mu\iff m=L^{-1}\left(xya+Q\mu\right),
\]

\end_inset

 assuming 
\begin_inset Formula $L^{-1}$
\end_inset

 exists for now.
 Then:
\begin_inset Formula 
\begin{align*}
p\left(y|x,D\right)\propto & \int_{\Theta}\exp\left\{ -\frac{1}{2}\left[\theta^{T}\left(axx^{T}+Q\right)\theta-2\theta^{T}\left(xya+Q\mu\right)+ay^{2}\right]\right\} d\theta\\
= & \int_{\Theta}\exp\left\{ -\frac{1}{2}\left[\theta^{T}L\theta-2\theta^{T}Lm+m^{T}Lm-m^{T}Lm+ay^{2}\right]\right\} d\theta\\
= & \int_{\Theta}\exp\left\{ -\frac{1}{2}\left[\left(\theta-m\right)^{T}L\left(\theta-m\right)-m^{T}Lm+ay^{2}\right]\right\} d\theta.
\end{align*}

\end_inset

 Notice here that 
\begin_inset Formula $m$
\end_inset

 is independent of 
\begin_inset Formula $\theta$
\end_inset

 so that we have
\begin_inset Formula 
\begin{align*}
p\left(y|x,D\right)\propto & \int_{\Theta}\exp\left\{ -\frac{1}{2}\left[\left(\theta-m\right)^{T}L\left(\theta-m\right)-m^{T}Lm+ay^{2}\right]\right\} d\theta\\
= & \int_{\Theta}\exp\left\{ -\frac{1}{2}\left[\left(\theta-m\right)^{T}L\left(\theta-m\right)\right]\right\} \exp\left\{ -\frac{1}{2}\left\{ ay^{2}-m^{T}Lm\right\} \right\} d\theta\\
= & \int_{\Theta}\exp\left\{ -\frac{1}{2}\left[\left(\theta-m\right)^{T}L\left(\theta-m\right)\right]\right\} g\left(y\right)d\theta\\
= & g\left(y\right)\int_{\Theta}\exp\left\{ -\frac{1}{2}\left[\left(\theta-m\right)^{T}L\left(\theta-m\right)\right]\right\} d\theta\\
\propto & g\left(y\right)=\exp\left\{ -\frac{1}{2}\left\{ ay^{2}-m^{T}Lm\right\} \right\} .
\end{align*}

\end_inset

 And now...
 we complete squares again :D 
\begin_inset Formula 
\begin{align*}
m^{T}Lm= & \left(ayx+Q\mu\right)^{T}L^{-1}\cancel{LL^{-1}}\left(ayx+Q\mu\right)\\
= & ayx^{T}L^{-1}ayx+2ayx^{T}L^{-1}Q\mu+\cancelto{indep\ of\ y}{\mu^{T}Q^{T}L^{-1}Q\mu}\\
= & \left(a^{2}x^{T}L^{-1}x\right)y^{2}+2\left(ax^{T}L^{-1}Q\mu\right)y+const.
\end{align*}

\end_inset

So
\begin_inset Formula 
\[
ay^{2}-m^{T}Lm=\left(a-a^{2}x^{T}L^{-1}x\right)y^{2}-2\left(ax^{T}L^{-1}Q\mu\right)y+const
\]

\end_inset

 If 
\begin_inset Formula $g\left(y\right)$
\end_inset

 is a Gaussian, then this should look something like
\begin_inset Formula 
\[
\lambda\left(y-u\right)^{2}=\lambda y^{2}-2\lambda uy+\lambda u^{2},
\]

\end_inset

 so
\begin_inset Formula 
\[
\lambda=a-a^{2}x^{T}L^{-1}x
\]

\end_inset

 and
\begin_inset Formula 
\[
\lambda u=ax^{T}L^{-1}Q\mu,
\]

\end_inset

 so
\begin_inset Formula 
\[
u=\frac{1}{\lambda}ax^{T}L^{-1}Q\mu.
\]

\end_inset

 And then, we have
\begin_inset Formula 
\begin{align*}
p\left(y|x,D\right)\propto & g\left(y\right)\\
\propto & \exp\left\{ -\frac{\lambda}{2}\left(y-u\right)^{2}\right\} ,
\end{align*}

\end_inset

 so we have that 
\begin_inset Formula $p\left(y|x,D\right)=\mathcal{N}\left(y|u,\frac{1}{\lambda}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Not only this, but
\begin_inset Foot
status open

\begin_layout Plain Layout
The derivation for these values is a bit involved.
 It can be consulted in 
\begin_inset CommandInset href
LatexCommand href
name "https://www.youtube.com/watch?v=LCISTY9S6SQ&t=287s"
target "https://www.youtube.com/watch?v=LCISTY9S6SQ&t=287s"
literal "false"

\end_inset

.
\end_layout

\end_inset


\begin_inset Formula 
\[
u=\mu^{T}x
\]

\end_inset

 and
\begin_inset Formula 
\[
\frac{1}{\lambda}=\frac{1}{a}+x^{T}Q^{-1}x.
\]

\end_inset


\end_layout

\begin_layout Standard
We note now that the predictive distribution's mean prediction equals the
 point-prediction of the MAP.
 However, the variance of the prediction does depend on 
\begin_inset Formula $x$
\end_inset

, which is good, since the unvertainty of out predictions depends on how
 far observed samples are:
\end_layout

\begin_layout Itemize
If observed samples are near from our new inputs, then we should be more
 certain.
\end_layout

\begin_layout Itemize
If they are far, then we should be less certain.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Clustering
\end_layout

\begin_layout Standard
The goal of clustering is to partition a dataset into groups called 
\series bold
clusters
\series default
, in such a way that observations in the same cluster tend to be more similar
 than observations in different clusters.
 The input data is embedded in a 
\begin_inset Formula $d$
\end_inset

-dimensional space with a similarity/dissimilarity function defined among
 elements in the space, which should capture relatedness among elements
 in this space.
 Two elements are understood to be related when they are close in the space.
 Thus, a cluster is a compact group that is separated from other groups
 or elements outside the cluster.
\end_layout

\begin_layout Standard
There is a large variety of clustering algorithms, such as hierarchical
 bottom-up or top-down clustering, probabilistic clustering, possibilistic
 clustering, algorithmic clustering, sprectral clustering or density-based
 clustering.
\end_layout

\begin_layout Standard
The problem of clustering is quite complex, as if we have 
\begin_inset Formula $N$
\end_inset

 data points which we want to separate into 
\begin_inset Formula $K$
\end_inset

 clusters, then there are
\begin_inset Formula 
\[
S\left(N,K\right)=\frac{1}{K!}\sum_{i=1}^{K}\left(-1\right)^{i}\binom{K}{i}\left(K-i\right)^{N}
\]

\end_inset

 possibilities.
 This is the 
\series bold
stirling number of the second king
\series default
.
\end_layout

\begin_layout Standard
If in addition we don't know how many clusters we want to use, we have to
 add all possible 
\begin_inset Formula $K=1,...,N$
\end_inset

, summing up to
\begin_inset Formula 
\[
B\left(N\right)=\sum_{K=1}^{N}S\left(N,K\right)
\]

\end_inset

 possibilites.
 This number is the 
\series bold
Bell number
\series default
, which is really gigantic
\begin_inset Foot
status open

\begin_layout Plain Layout
For example, 
\begin_inset Formula $B\left(71\right)\approx4\times10^{71}$
\end_inset

.
\end_layout

\end_inset

.
\end_layout

\begin_layout Subsection
k-Means
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $k$
\end_inset

-Means clustering algorithm takes a dataset 
\begin_inset Formula $D=\left\{ x_{1},...,x_{n}\right\} $
\end_inset

 and an integer 
\begin_inset Formula $k>1$
\end_inset

 as input, and separates 
\begin_inset Formula $D$
\end_inset

 into 
\begin_inset Formula $k$
\end_inset

 disjoint clusters.
 It is a 
\series bold
representative-based clustering
\series default
, meaning that each cluster is represented by one single point.
 In the case of 
\begin_inset Formula $k$
\end_inset

-means, the representative is the 
\series bold
cluster center
\series default
, 
\begin_inset Formula $\mu_{k}\in\mathbb{R}^{d},k=1,...,K$
\end_inset

, i.e., the average of all the points in that cluster.
 Each point in thus assigned to its closest representative point.
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $C_{k}$
\end_inset

 is the 
\begin_inset Formula $k$
\end_inset

-th cluster, then we consider it a better cluster when the value
\begin_inset Formula 
\[
\sum_{x\in C_{k}}\left\Vert x-\mu_{k}\right\Vert ^{2}
\]

\end_inset

 is smaller.
\end_layout

\begin_layout Standard
Now, let's formalize all this.
 First, we introduce an indicator variable
\begin_inset Formula 
\[
r_{ik}=\begin{cases}
1 & if\ x_{i}\in C_{k}\\
0 & otherwise
\end{cases},
\]

\end_inset

 and the objective function
\begin_inset Formula 
\[
\mathcal{J}\left(\mu,r\right)=\sum_{k=1}^{K}\sum_{i=1}^{n}r_{ij}\left\Vert x_{i}-\mu_{k}\right\Vert ^{2},
\]

\end_inset

 which we aim to minimize by selecting appropriate 
\begin_inset Formula $\left\{ \mu_{k}\right\} _{k}$
\end_inset

 and 
\begin_inset Formula $\left\{ r_{ik}\right\} _{ik}$
\end_inset

.
 The issue is that this problem is NP-hard, so we use an heuristic method
 that is only guaranteed to find local minima.
 This method relies in two facts:
\end_layout

\begin_layout Enumerate
For fixed cluster centers 
\begin_inset Formula $\mu_{k}$
\end_inset

, it is easy to optimize cluster assignments 
\begin_inset Formula $r_{ik}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Proof
Assume fixed 
\begin_inset Formula $\left\{ \mu_{k}\right\} _{k}$
\end_inset

, then we assign 
\begin_inset Formula $x_{i}$
\end_inset

 to the closest 
\begin_inset Formula $\mu_{k}$
\end_inset

, because if we assign it to a different center, 
\begin_inset Formula $\mu_{j}$
\end_inset

, then we can minimize the sum as
\begin_inset Formula 
\[
\left\Vert x_{i}-\mu_{j}\right\Vert ^{2}>\left\Vert x_{i}-\mu_{k}\right\Vert ^{2}.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
For fixed cluster assignments 
\begin_inset Formula $r_{ik}$
\end_inset

, it is easy to optimize cluster centers 
\begin_inset Formula $\mu_{k}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Proof
Assume fixed 
\begin_inset Formula $\left\{ r_{ij}\right\} _{ij}$
\end_inset

, then
\begin_inset Formula 
\begin{align*}
\frac{\partial}{\partial\mu_{j}}\mathcal{J}\left(\mu_{1},...,\mu_{K}\right)= & \sum_{k=1}^{K}\sum_{i=1}^{n}\frac{\partial}{\partial\mu_{j}}r_{ij}\left\Vert x_{i}-\mu_{k}\right\Vert ^{2}\\
\overset{\frac{\partial}{\partial\mu_{j}}\mu_{k}=0,\forall k\neq j}{=} & \sum_{i=1}^{n}r_{ij}\frac{\partial}{\partial\mu_{j}}\left(x_{i}-\mu_{j}\right)^{T}\left(x_{i}-\mu_{j}\right)\\
= & \sum_{i=1}^{n}r_{ij}\frac{\partial}{\partial\mu_{j}}\left(x_{i}^{T}x_{i}-2\mu_{j}^{T}x_{i}+\mu_{j}^{T}\mu_{j}\right)\\
= & \sum_{i=1}^{n}r_{ij}\left(-2x_{i}+2\mu_{j}\right)\\
= & -2\sum_{i=1}^{n}r_{ij}x_{i}+2\mu_{j}\sum_{i=1}^{n}r_{ij}.
\end{align*}

\end_inset

 Thus, the minimum is obtained at
\begin_inset Formula 
\[
\mu_{j}=\frac{\sum_{i=1}^{n}r_{ij}x_{i}}{\sum_{i=1}^{n}r_{ij}}=\frac{\sum_{x\in C_{j}}x}{card\left(C_{j}\right)}.
\]

\end_inset

 This is the average of the points of the cluster.
\end_layout

\end_deeper
\begin_layout Standard
The pseudocode is illustrated in Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:k-Means."
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement !h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{pseudocode}
\end_layout

\begin_layout Plain Layout

Initialize cluster centers <@$
\backslash
mu_1,...,
\backslash
mu_K$@>
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

repeat until convergence
\end_layout

\begin_layout Plain Layout

	assign each point to the cluster with closest center
\end_layout

\begin_layout Plain Layout

		<@
\backslash
[r_{ik}=
\backslash
begin{cases}1 & if
\backslash
 k=
\backslash
arg
\backslash
min_j ||x_i-
\backslash
mu_j||^2
\backslash

\backslash
0 & otherwise
\backslash
end{cases}
\backslash
]@>
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	recompute cluster centers for all k=1,...,K
\end_layout

\begin_layout Plain Layout

		<@
\backslash
[
\backslash
mu_k=
\backslash
frac{
\backslash
sum_{x
\backslash
in C_{j}}x}{card
\backslash
left(C_{j}
\backslash
right)}
\backslash
]@>
\end_layout

\begin_layout Plain Layout


\backslash
end{pseudocode}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
k-Means.
\begin_inset CommandInset label
LatexCommand label
name "alg:k-Means."

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The characteristics of 
\begin_inset Formula $k$
\end_inset

-Means are:
\end_layout

\begin_layout Enumerate
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
Advantages
\end_layout

\end_inset

:
\end_layout

\begin_deeper
\begin_layout Enumerate
Easy implementation.
\end_layout

\begin_layout Enumerate
Fast, even for large datasets.
\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
Limitations
\end_layout

\end_inset

:
\end_layout

\begin_deeper
\begin_layout Enumerate
Can converge to local minimum.
\end_layout

\begin_layout Enumerate
Needs the number of clusters, 
\begin_inset Formula $K$
\end_inset

, as input.
\end_layout

\begin_layout Enumerate
Hard cluster assignments: meaning that each point corresponds to a single
 cluster, which may not always be what we want.
\end_layout

\begin_layout Enumerate
Sensitive to ourliers and clusters of different sizes and densities.
\end_layout

\begin_layout Enumerate
Sensitive to initialization, so it is usual to run it many times and keep
 the best run.
\end_layout

\begin_layout Enumerate
Biased towards rounded clusters, because it uses the Euclidean distance.
\end_layout

\end_deeper
\begin_layout Subsection
k-Means++
\end_layout

\begin_layout Standard
\begin_inset Formula $k$
\end_inset

-Means++ is a variant of 
\begin_inset Formula $k$
\end_inset

-Means that uses a heuristic for initializing cluster centers as in Algorithm
 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:k-Means++."
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement !h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset listings
lstparams "morekeywords={choose,for,do,next,proceed,from}"
inline false
status open

\begin_layout Plain Layout

choose first center <@$
\backslash
mu_1$@> uniformly at random from all available examples
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

for k=2,...,K do
\end_layout

\begin_layout Plain Layout

	choose next center <@$
\backslash
mu_k$ at random, with probability proportional to $||x_i-
\backslash
mu_l||$@>
\end_layout

\begin_layout Plain Layout

	here, <@$
\backslash
mu_l$@> is the closest center picked so far
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

proceed with standard k-Means
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
k-Means++.
\begin_inset CommandInset label
LatexCommand label
name "alg:k-Means++."

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Choosing the number of cluster 
\begin_inset Formula $K$
\end_inset


\end_layout

\begin_layout Standard
The number of clusters is a hyper-parameter that has to be set by the user,
 and there is no obvious way to choose an optimal 
\begin_inset Formula $K$
\end_inset

, since it may not even exist.
 This is due to the fact that there is no such thing as a true clustering
 against to compare.
 Nevertheless, there are reasonable cluster quality criteria, that can be
 used to select 
\begin_inset Formula $K$
\end_inset

.
 These criteria measure a balance between separation of clusters and their
 compactness.
 Depending on the problem, one criterion or another should be chosen.
\end_layout

\begin_layout Subsubsection
Calinski-Harabasz index
\end_layout

\begin_layout Standard
The 
\series bold
CH index
\series default
 uses Euclidean distances to measure cluster quality, so it is usually used
 with 
\begin_inset Formula $k$
\end_inset

-means.
 It measures the ratio between:
\end_layout

\begin_layout Itemize
Separation of cluster centers: sum of distances of cluster centers to the
 overall mean.
\end_layout

\begin_layout Itemize
Cluster compactness: sum of distances from each point to its assigned cluster
 center.
\end_layout

\begin_layout Standard
Thus, it is:
\begin_inset Formula 
\[
CH=\frac{N-K}{K-1}\frac{\sum_{k=1}^{K}n_{k}\left\Vert \mu_{k}-\overline{x}\right\Vert ^{2}}{\sum_{k=1}^{K}\sum_{i=1}^{n}r_{ik}\left\Vert x_{i}-\mu_{k}\right\Vert ^{2}},
\]

\end_inset

 where 
\begin_inset Formula $\overline{x}=\frac{\sum x_{i}}{n}$
\end_inset

.
 Notice that the quantities are normalized by 
\begin_inset Formula $\frac{N-K}{K-1}$
\end_inset

 to avoid larger 
\begin_inset Formula $K$
\end_inset

 having better values.
\end_layout

\begin_layout Standard
The usual approach is run 
\begin_inset Formula $k$
\end_inset

-Means with different values of 
\begin_inset Formula $K$
\end_inset

, and then select the 
\begin_inset Formula $K$
\end_inset

 that maximizes the index.
\end_layout

\begin_layout Example
A 
\begin_inset Formula $k$
\end_inset

-Means example in Matlab.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
matlabtitle{k-Means algorithm example}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

Introduce the data
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

X1 = [2,10];
\end_layout

\begin_layout Plain Layout

X2 = [2,5];
\end_layout

\begin_layout Plain Layout

X3 = [8,4];
\end_layout

\begin_layout Plain Layout

X4 = [5,8];
\end_layout

\begin_layout Plain Layout

X5 = [7,5];
\end_layout

\begin_layout Plain Layout

X6 = [6,4];
\end_layout

\begin_layout Plain Layout

X7 = [1,2];
\end_layout

\begin_layout Plain Layout

X8 = [4,9];
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

D = [X1;X2;X3;X4;X5;X6;X7;X8];
\end_layout

\begin_layout Plain Layout

Y = [X5; X6; X8];
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

Compute the distance between all points
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

dist = zeros(3);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

for j=1:3
\end_layout

\begin_layout Plain Layout

    for i=1:8
\end_layout

\begin_layout Plain Layout

        dist(i,j)=distance(D(i,:),Y(j,:),2);
\end_layout

\begin_layout Plain Layout

    end
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

Select, for each point, the point that minimizes the distance
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

[v, idx] = min(dist, [], 2);
\end_layout

\begin_layout Plain Layout

D = [D,idx];
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

Plot the clusters:
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

c1 = D(:,3) == 1;
\end_layout

\begin_layout Plain Layout

c2 = D(:,3) == 2;
\end_layout

\begin_layout Plain Layout

c3 = D(:,3) == 3;
\end_layout

\begin_layout Plain Layout

scatter(D(c1,1),D(c1,2), 'MarkerFaceColor', 'b');
\end_layout

\begin_layout Plain Layout

hold on
\end_layout

\begin_layout Plain Layout

scatter(D(c2,1),D(c2,2), 'MarkerFaceColor', 'g');
\end_layout

\begin_layout Plain Layout

scatter(D(c3,1),D(c3,2), 'MarkerFaceColor', 'r');
\end_layout

\begin_layout Plain Layout

hold off
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename scripts/kmeans_images/figure_0.eps
	scale 60

\end_inset


\end_layout

\end_deeper
\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

Compute the new 
\backslash
texttt{Y:}
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

for i=1:3
\end_layout

\begin_layout Plain Layout

    ci = D(:,3) == i;
\end_layout

\begin_layout Plain Layout

    x = mean(D(ci,1));
\end_layout

\begin_layout Plain Layout

    y = mean(D(ci,2));
\end_layout

\begin_layout Plain Layout

    Y(i,:)=[x,y];
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout

Y
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

Y = 3x2    
\end_layout

\begin_layout Plain Layout

    7.5000    4.5000
\end_layout

\begin_layout Plain Layout

    3.0000    3.6667
\end_layout

\begin_layout Plain Layout

    3.6667    9.0000
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

Repeat the process:
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

%Distances
\end_layout

\begin_layout Plain Layout

for j=1:3
\end_layout

\begin_layout Plain Layout

    for i=1:8
\end_layout

\begin_layout Plain Layout

        dist(i,j)=distance(D(i,1:2),Y(j,:),2);
\end_layout

\begin_layout Plain Layout

    end
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout

%Assign
\end_layout

\begin_layout Plain Layout

[v, idx] = min(dist, [], 2);
\end_layout

\begin_layout Plain Layout

D(:,3) = idx;
\end_layout

\begin_layout Plain Layout

%Plot
\end_layout

\begin_layout Plain Layout

c1 = D(:,3) == 1;
\end_layout

\begin_layout Plain Layout

c2 = D(:,3) == 2;
\end_layout

\begin_layout Plain Layout

c3 = D(:,3) == 3;
\end_layout

\begin_layout Plain Layout

scatter(D(c1,1),D(c1,2), 'MarkerFaceColor', 'b');
\end_layout

\begin_layout Plain Layout

hold on
\end_layout

\begin_layout Plain Layout

scatter(Y(1,1),Y(1,2), 'MarkerFaceColor', 'b');
\end_layout

\begin_layout Plain Layout

scatter(D(c2,1),D(c2,2), 'MarkerFaceColor', 'g');
\end_layout

\begin_layout Plain Layout

scatter(Y(2,1),Y(2,2), 'MarkerFaceColor', 'g');
\end_layout

\begin_layout Plain Layout

scatter(D(c3,1),D(c3,2), 'MarkerFaceColor', 'r');
\end_layout

\begin_layout Plain Layout

scatter(Y(3,1),Y(3,2), 'MarkerFaceColor', 'r');
\end_layout

\begin_layout Plain Layout

hold off
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename scripts/kmeans_images/figure_1.eps
	scale 60

\end_inset


\end_layout

\end_deeper
\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

%Optimize
\end_layout

\begin_layout Plain Layout

for i=1:3
\end_layout

\begin_layout Plain Layout

    ci = D(:,3) == i;
\end_layout

\begin_layout Plain Layout

    x = mean(D(ci,1));
\end_layout

\begin_layout Plain Layout

    y = mean(D(ci,2));
\end_layout

\begin_layout Plain Layout

    Y(i,:)=[x,y];
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout

Y
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

Y = 3x2    
\end_layout

\begin_layout Plain Layout

    7.0000    4.3333
\end_layout

\begin_layout Plain Layout

    1.5000    3.5000
\end_layout

\begin_layout Plain Layout

    3.6667    9.0000
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

Again:
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

%Distances
\end_layout

\begin_layout Plain Layout

for j=1:3
\end_layout

\begin_layout Plain Layout

    for i=1:8
\end_layout

\begin_layout Plain Layout

        dist(i,j)=distance(D(i,1:2),Y(j,:),2);
\end_layout

\begin_layout Plain Layout

    end
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout

%Assign
\end_layout

\begin_layout Plain Layout

[v, idx] = min(dist, [], 2);
\end_layout

\begin_layout Plain Layout

D(:,3) = idx;
\end_layout

\begin_layout Plain Layout

%Plot
\end_layout

\begin_layout Plain Layout

c1 = D(:,3) == 1;
\end_layout

\begin_layout Plain Layout

c2 = D(:,3) == 2;
\end_layout

\begin_layout Plain Layout

c3 = D(:,3) == 3;
\end_layout

\begin_layout Plain Layout

scatter(D(c1,1),D(c1,2), 'MarkerFaceColor', 'b');
\end_layout

\begin_layout Plain Layout

hold on
\end_layout

\begin_layout Plain Layout

scatter(Y(1,1),Y(1,2), 'MarkerFaceColor', 'b');
\end_layout

\begin_layout Plain Layout

scatter(D(c2,1),D(c2,2), 'MarkerFaceColor', 'g');
\end_layout

\begin_layout Plain Layout

scatter(Y(2,1),Y(2,2), 'MarkerFaceColor', 'g');
\end_layout

\begin_layout Plain Layout

scatter(D(c3,1),D(c3,2), 'MarkerFaceColor', 'r');
\end_layout

\begin_layout Plain Layout

scatter(Y(3,1),Y(3,2), 'MarkerFaceColor', 'r');
\end_layout

\begin_layout Plain Layout

hold off
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename scripts/kmeans_images/figure_2.eps
	scale 60

\end_inset


\end_layout

\end_deeper
\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

%Optimize
\end_layout

\begin_layout Plain Layout

for i=1:3
\end_layout

\begin_layout Plain Layout

    ci = D(:,3) == i;
\end_layout

\begin_layout Plain Layout

    x = mean(D(ci,1));
\end_layout

\begin_layout Plain Layout

    y = mean(D(ci,2));
\end_layout

\begin_layout Plain Layout

    Y(i,:)=[x,y];
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout

Y
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

Y = 3x2    
\end_layout

\begin_layout Plain Layout

    7.0000    4.3333
\end_layout

\begin_layout Plain Layout

    1.5000    3.5000
\end_layout

\begin_layout Plain Layout

    3.6667    9.0000
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

At this point we see how the clusters are the ones that we see naturally
 with our eyes.
 If we execute it again, we can see that the changes are very slight now:
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

%Distances
\end_layout

\begin_layout Plain Layout

for j=1:3
\end_layout

\begin_layout Plain Layout

    for i=1:8
\end_layout

\begin_layout Plain Layout

        dist(i,j)=distance(D(i,1:2),Y(j,:),2);
\end_layout

\begin_layout Plain Layout

    end
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout

%Assign
\end_layout

\begin_layout Plain Layout

[v, idx] = min(dist, [], 2);
\end_layout

\begin_layout Plain Layout

D(:,3) = idx;
\end_layout

\begin_layout Plain Layout

%Plot
\end_layout

\begin_layout Plain Layout

c1 = D(:,3) == 1;
\end_layout

\begin_layout Plain Layout

c2 = D(:,3) == 2;
\end_layout

\begin_layout Plain Layout

c3 = D(:,3) == 3;
\end_layout

\begin_layout Plain Layout

scatter(D(c1,1),D(c1,2), 'MarkerFaceColor', 'b');
\end_layout

\begin_layout Plain Layout

hold on
\end_layout

\begin_layout Plain Layout

scatter(Y(1,1),Y(1,2), 'MarkerFaceColor', 'b');
\end_layout

\begin_layout Plain Layout

scatter(D(c2,1),D(c2,2), 'MarkerFaceColor', 'g');
\end_layout

\begin_layout Plain Layout

scatter(Y(2,1),Y(2,2), 'MarkerFaceColor', 'g');
\end_layout

\begin_layout Plain Layout

scatter(D(c3,1),D(c3,2), 'MarkerFaceColor', 'r');
\end_layout

\begin_layout Plain Layout

scatter(Y(3,1),Y(3,2), 'MarkerFaceColor', 'r');
\end_layout

\begin_layout Plain Layout

hold off
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename scripts/kmeans_images/figure_3.eps
	scale 60

\end_inset


\end_layout

\end_deeper
\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

%Optimize
\end_layout

\begin_layout Plain Layout

for i=1:3
\end_layout

\begin_layout Plain Layout

    ci = D(:,3) == i;
\end_layout

\begin_layout Plain Layout

    x = mean(D(ci,1));
\end_layout

\begin_layout Plain Layout

    y = mean(D(ci,2));
\end_layout

\begin_layout Plain Layout

    Y(i,:)=[x,y];
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout

Y
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

Y = 3x2    
\end_layout

\begin_layout Plain Layout

    7.0000    4.3333
\end_layout

\begin_layout Plain Layout

    1.5000    3.5000
\end_layout

\begin_layout Plain Layout

    3.6667    9.0000
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

In fact, Y is not changing anymore!
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
matlabheadingthree{Functions}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
hfill 
\backslash
break
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

function d = distance(X, Y, m)
\end_layout

\begin_layout Plain Layout

    n = length(X);
\end_layout

\begin_layout Plain Layout

    d = 0;
\end_layout

\begin_layout Plain Layout

    for i=1:n
\end_layout

\begin_layout Plain Layout

        d = d + (X(i)-Y(i))^m;
\end_layout

\begin_layout Plain Layout

    end
\end_layout

\begin_layout Plain Layout

    d = sqrt(d);
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Gaussian Mixtures
\end_layout

\begin_layout Standard
A 
\series bold
mixture of Gaussians
\series default
 is a distributions that is built using a convex sum of Gaussians, making
 it more flexible than a single Gaussian distribution.
 If the 
\series bold
components
\series default
 of the mixture are 
\begin_inset Formula $\mathcal{N}\left(\mu_{k},\Sigma_{k}\right),k=1,...,K$
\end_inset

 and 
\begin_inset Formula $\pi_{k},k=1,...,K$
\end_inset

 are the 
\series bold
mixing coefficients
\series default
, with 
\begin_inset Formula $0\leq\pi_{k}\leq1,\sum_{k}\pi_{k}=1$
\end_inset

, then, the density function of the mixture is given by
\begin_inset Formula 
\[
p\left(x|\theta\right)=\sum_{k=1}^{K}\pi_{k}\mathcal{N}\left(x;\mu_{k},\Sigma_{k}\right),
\]

\end_inset

 where 
\begin_inset Formula $\theta=\left\{ \pi_{k},\mu_{k},\Sigma_{k}\right\} _{k=1,...,K}$
\end_inset

 represents the parameters of the distribution.
\end_layout

\begin_layout Standard
The 
\color blue
key assumption
\color inherit
 is that each data point has been generated from only one component, we
 just don't know which one.
\end_layout

\begin_layout Standard
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Different-Gaussian-mixtures"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can see different Gaussian Mixtures that arise from the same components,
 choosing different mixing coefficients.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado15.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Different-Gaussian-mixtures"

\end_inset

Different Gaussian mixtures with the same components.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Clustering with a Gaussian mixture
\end_layout

\begin_layout Standard
The idea is to identify each component with a cluster, so we want to determine
 the component from which each point in the dataset is more likely to have
 been created, as well as the parameters of each of the distribution.
\end_layout

\begin_layout Standard
Thus, to cluster a dataset 
\begin_inset Formula $D=\left\{ x_{1},...,x_{n}\right\} $
\end_inset

 into 
\begin_inset Formula $K$
\end_inset

 clusters, the approach is the following:
\end_layout

\begin_layout Enumerate
Use Expectation-Maximization (EM) to estimate the mixture, obtaining approximati
ons 
\begin_inset Formula $\hat{\pi}_{k},\hat{\mu}_{k}$
\end_inset

 and 
\begin_inset Formula $\hat{\Sigma}_{k}$
\end_inset

 for each 
\begin_inset Formula $k=1,...,K$
\end_inset

.
\end_layout

\begin_layout Enumerate
Find assignments for each 
\begin_inset Formula $x_{i}$
\end_inset

 to the clusters.
\end_layout

\begin_layout Standard
In this case, the clustering is 
\series bold
soft
\series default
, in opposition to the hard clustering of 
\begin_inset Formula $k$
\end_inset

-Means.
 This means that we will obtain the probability for each point belonging
 to each cluster.
\end_layout

\begin_layout Subsubsection
A generative mixture of Gaussians
\end_layout

\begin_layout Standard
To sample from a mixture of Gaussians, we use a 
\series bold
generative model 
\series default
that uses a latent variable 
\begin_inset Formula $z=\left(z_{1},...,z_{K}\right)$
\end_inset

 whose components are all 0, except one which denotes the component from
 which we sample, and we do:
\end_layout

\begin_layout Enumerate
Pick component 
\begin_inset Formula $k$
\end_inset

 with probability 
\begin_inset Formula $\pi_{k}$
\end_inset

.
 This means that we set 
\begin_inset Formula $z_{k}=1$
\end_inset

 with probability 
\begin_inset Formula $\pi_{k}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Generate a sample 
\begin_inset Formula $x$
\end_inset

 according to 
\begin_inset Formula $\mathcal{N}\left(\mu_{k},\Sigma_{k}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
The probability of generating a sample 
\begin_inset Formula $x$
\end_inset

 using this generative model is
\begin_inset Formula 
\[
p\left(x\right)=\sum_{z}p\left(x,z\right)=\sum_{k}p\left(x,z_{k}=1\right)=\sum_{k}p\left(x|z_{k}=1\right)p\left(z_{k}=1\right)=\sum_{k}\pi_{k}\mathcal{N}\left(x;\mu_{k},\Sigma_{k}\right),
\]

\end_inset

 the joint distribution of 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $z$
\end_inset

 is given by
\begin_inset Formula 
\[
p\left(x,z\right)=\prod_{k}\pi_{k}^{z_{k}}\mathcal{N}\left(x;\mu_{k},\Sigma_{k}\right)^{z_{k}},
\]

\end_inset

and the marginal distribution over 
\begin_inset Formula $x$
\end_inset

 is
\begin_inset Formula 
\begin{align*}
p\left(x\right)= & \sum_{k}\pi_{k}\mathcal{N}\left(x;\mu_{k},\Sigma_{k}\right)\\
= & \sum_{z}p\left(x,z\right)=\sum_{z}\prod_{k'}\pi_{k'}^{z_{k'}}\mathcal{N}\left(x;\mu_{k'},\Sigma_{k'}\right)^{z_{k'}}.
\end{align*}

\end_inset

 Therefore, we can use Bayes to compute the conditional distribution of
 
\begin_inset Formula $z$
\end_inset

 given 
\begin_inset Formula $x$
\end_inset

:
\begin_inset Formula 
\begin{align*}
p\left(z_{k}=1|x\right)= & \frac{p\left(x|z_{k}=1\right)p\left(z_{k}=1\right)}{p\left(x\right)}\\
= & \frac{p\left(x|z_{k}=1\right)p\left(z_{k}=1\right)}{\sum_{k'}\pi_{k'}\mathcal{N}\left(x;\mu_{k'},\Sigma_{k'}\right)}\\
= & \frac{\pi_{k}\mathcal{N}\left(x;\mu_{k},\Sigma_{k}\right)}{\sum_{k'}\pi_{k'}\mathcal{N}\left(x;\mu_{k'},\Sigma_{k'}\right)} & =\gamma_{k}\left(x\right).
\end{align*}

\end_inset

 The quantity 
\begin_inset Formula $\gamma_{k}\left(x\right)$
\end_inset

 indicates how probable it is that a particular data point 
\begin_inset Formula $x$
\end_inset

 has been generated by the mixture component 
\begin_inset Formula $k$
\end_inset

.
 Or, in the context of clustering: 
\emph on
how probable it is that 
\begin_inset Formula $x$
\end_inset

 belongs to cluster 
\begin_inset Formula $k$
\end_inset

.
 
\emph default
We use these quantities as the 
\series bold
soft membership
\series default
 to each cluster.
 If a hard membserhip is needed, then we assign 
\begin_inset Formula $x$
\end_inset

 to cluster 
\begin_inset Formula $j$
\end_inset

, where 
\begin_inset Formula $j=\arg\max_{k'}\gamma_{k'}\left(x\right)$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Learning Gaussian mixtures with Expectation-Maximization
\end_layout

\begin_layout Standard
We have a dataset of unlabelled observations 
\begin_inset Formula $D=\left\{ x_{1},...,x_{n}\right\} $
\end_inset

 and we want to model it as a Gaussian mixture, with unknown parameters
 
\begin_inset Formula $\theta=\left\{ \pi_{k},\mu_{k},\Sigma_{k}\right\} _{k=1,...,K}$
\end_inset

, with a fixed 
\begin_inset Formula $K$
\end_inset

.
 For this we use the maximum likelihood approach.
 First, we compute the loglikelihood of 
\begin_inset Formula $\theta$
\end_inset

:
\begin_inset Formula 
\begin{align*}
l\left(\theta\right)=\log\mathcal{L}\left(\theta\right)= & \log\prod_{i=1}^{n}p\left(x_{i};\theta\right)\\
= & \log\prod_{i}\sum_{k}\pi_{k}\mathcal{N}\left(x;\mu_{k},\Sigma_{k}\right)\\
= & \sum_{i}\log\sum_{k}\pi_{k}\mathcal{N}\left(x;\mu_{k},\Sigma_{k}\right).
\end{align*}

\end_inset

 This is hard to optimize...
 so we use the Expectation-Maximization approach.
 First, we can differenciate it to see what conditions must hold for local
 maxima, with 
\begin_inset Formula $\frac{\partial}{\partial\mu_{k}}l\left(\theta\right)=0$
\end_inset

 leading to
\begin_inset Formula 
\[
\hat{\mu}_{k}=\frac{\sum_{i}\gamma_{k}\left(x_{i}\right)x_{i}}{\sum_{i}\gamma_{k}\left(x_{i}\right)}=\frac{\sum_{i}p\left(z_{k}=1|x_{i}\right)x_{i}}{\sum_{i}p\left(z_{k}=1|x_{i}\right)},
\]

\end_inset

 which is a weighted average of the points in our data, with weights being
 the soft assignments of each point to cluster 
\begin_inset Formula $k$
\end_inset

.
 
\end_layout

\begin_layout Standard
The 
\color red
problem
\color inherit
 now, is we cannot know 
\begin_inset Formula $\gamma_{k}\left(x\right)$
\end_inset

 without 
\begin_inset Formula $\mu_{k},\Sigma_{k}$
\end_inset

 and 
\begin_inset Formula $\pi_{k}$
\end_inset

.
 Now, 
\begin_inset Formula $\frac{\partial}{\partial\Sigma_{k}}l\left(\theta\right)=0$
\end_inset

 gives us
\begin_inset Formula 
\[
\hat{\Sigma}_{k}=\frac{\sum_{i}\gamma_{k}\left(x_{i}\right)\left(x_{i}-\hat{\mu}_{k}\right)\left(x_{i}-\hat{\mu}_{k}\right)^{T}}{\sum_{i}\gamma_{k}\left(x_{i}\right)}=\frac{\sum_{i}p\left(z_{k}=1|x_{i}\right)\left(x_{i}-\hat{\mu}_{k}\right)\left(x_{i}-\hat{\mu}_{k}\right)^{T}}{\sum_{i}p\left(z_{k}=1|x_{i}\right)},
\]

\end_inset

 which is the sample covariance matrix of all 
\begin_inset Formula $x_{i}$
\end_inset

, weigthed by the soft assignments of each point to cluster 
\begin_inset Formula $k$
\end_inset

.
 We have the same 
\color red
problem
\color inherit
!
\end_layout

\begin_layout Standard
Since we have the constraint 
\begin_inset Formula $\sum_{k}\pi_{k}=1$
\end_inset

, we now maximize the Lagrangian
\begin_inset Formula 
\[
\mathcal{L}=l\left(\theta\right)-\lambda\left(\sum_{k}\pi_{k}-1\right),
\]

\end_inset

 obtaining
\begin_inset Formula 
\[
\hat{\pi}_{k}=\frac{1}{n}\sum_{i}\gamma_{k}\left(x_{i}\right),
\]

\end_inset

 which is the average of all soft assignments for each point 
\begin_inset Formula $x$
\end_inset

.
 Again the same 
\color red
problem
\color inherit
 is present.
\end_layout

\begin_layout Standard
Therefore, we are in a situation in which we can estimate 
\begin_inset Formula $\pi_{k},\Sigma_{k}$
\end_inset

 and 
\begin_inset Formula $\mu_{k}$
\end_inset

 if we know 
\begin_inset Formula $\gamma_{k}$
\end_inset

, and we can compute 
\begin_inset Formula $\gamma_{k}$
\end_inset

 from the estimates 
\begin_inset Formula $\hat{\pi}_{k},\hat{\Sigma}_{k}$
\end_inset

 and 
\begin_inset Formula $\hat{\mu}_{k}$
\end_inset

; and we can use this in our benefit by following the pseudocode depicted
 in Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:EM"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Commonly, the initializations are done using the result of 
\begin_inset Formula $k$
\end_inset

-Means in the following manner:
\end_layout

\begin_layout Itemize
Run 
\begin_inset Formula $k$
\end_inset

-Means with 
\begin_inset Formula $k=K$
\end_inset

.
\end_layout

\begin_layout Itemize
Set 
\begin_inset Formula $\hat{\mu}_{k}$
\end_inset

 to the mean of cluster 
\begin_inset Formula $k$
\end_inset

.
\end_layout

\begin_layout Itemize
Set 
\begin_inset Formula $\hat{\Sigma}_{k}$
\end_inset

 to the sample covariance of cluster 
\begin_inset Formula $k$
\end_inset

.
\end_layout

\begin_layout Itemize
Set 
\begin_inset Formula $\hat{\pi}_{k}$
\end_inset

 as the fraction of examples assigned to cluster 
\begin_inset Formula $k$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement !h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{pseudocode}
\end_layout

\begin_layout Plain Layout

Initialize parameters <@$
\backslash
hat{
\backslash
pi}_k,
\backslash
hat{
\backslash
Sigma}_k,
\backslash
hat{
\backslash
mu}_k$@>
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

repeat until convergence
\end_layout

\begin_layout Plain Layout

	E-step: recompute soft assignments <@$
\backslash
gamma_k(x_i)$@>
\end_layout

\begin_layout Plain Layout

		<@
\backslash
[
\backslash
gamma_{k}
\backslash
left(x_{i}
\backslash
right)=
\backslash
frac{
\backslash
pi_{k}
\backslash
mathcal{N}
\backslash
left(x_{i};
\backslash
mu_{k},
\backslash
Sigma_{k}
\backslash
right)}{
\backslash
sum_{k'}
\backslash
pi_{k'}
\backslash
mathcal{N}
\backslash
left(x_{i};
\backslash
mu_{k'},
\backslash
Sigma_{k'}
\backslash
right)}
\backslash
]@>
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	M-step: recompute ML estimates
\end_layout

\begin_layout Plain Layout

					<@
\backslash
[
\backslash
hat{
\backslash
mu}_{k}=
\backslash
frac{
\backslash
sum_{i}
\backslash
gamma_{k}
\backslash
left(x_{i}
\backslash
right)x_{i}}{
\backslash
sum_{i}
\backslash
gamma_{k}
\backslash
left(x_{i}
\backslash
right)}
\backslash
]@>
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<@
\backslash
[
\backslash
hat{
\backslash
Sigma}_{k}=
\backslash
frac{
\backslash
sum_{i}
\backslash
gamma_{k}
\backslash
left(x_{i}
\backslash
right)
\backslash
left(x_{i}-
\backslash
hat{
\backslash
mu}_{k}
\backslash
right)
\backslash
left(x_{i}-
\backslash
hat{
\backslash
mu}_{k}
\backslash
right)^{T}}{
\backslash
sum_{i}
\backslash
gamma_{k}
\backslash
left(x_{i}
\backslash
right)}
\backslash
]@>
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<@
\backslash
[
\backslash
hat{
\backslash
pi}_{k}=
\backslash
frac{1}{n}
\backslash
sum_{i}
\backslash
gamma_{k}
\backslash
left(x_{i}
\backslash
right)
\backslash
]@>
\end_layout

\begin_layout Plain Layout


\backslash
end{pseudocode}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
EM algorithm.
\begin_inset CommandInset label
LatexCommand label
name "alg:EM"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Example
An example of the EM algorithm using Matlab
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
matlabtitle{Gaussian Mixture: EM algorithm}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

1- Create the mixture
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

mu = [1; 9];
\end_layout

\begin_layout Plain Layout

sigma = zeros(1,1,2);
\end_layout

\begin_layout Plain Layout

sigma(1,1,1) = 0.5;
\end_layout

\begin_layout Plain Layout

sigma(1,1,2) = 0.5;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

x = linspace(-2,15,300)';
\end_layout

\begin_layout Plain Layout

p1 = [0.3,0.7];
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

M1 = gmdistribution(mu,sigma,p1);
\end_layout

\begin_layout Plain Layout

MM1 = M1.pdf(x);
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

2- Sample the mixture
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

rng(3)
\end_layout

\begin_layout Plain Layout

Y = random(M1,50)'
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

Y = 1x50    
\end_layout

\begin_layout Plain Layout

    8.2075    9.8171    0.1442    8.8333    9.4084    9.0987    0.7269    1.0454
   -0.1108    9.4905    0.9664    8.9080    9.1382    0.7465    7.3875    8.3064
    1.4610    8.5940    1.5123    7.3018    1.5885    7.9239    7.9331    1.3259
    9.0606    8.6797   10.1803    0.5893    9.3200    9.0754    9.4638    8.5299
   10.3348    9.7996    9.7767    1.3619    8.9398    9.6193    9.7211    0.6352
    9.5654   10.2435    1.2301    0.6117    9.2196    8.5399    9.1209    0.9323
    0.2577    8.0402
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

3- See what we got
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

figure(1)
\end_layout

\begin_layout Plain Layout

plot(x,MM1,'LineWidth',2,'Color',[1 0.2 0.2 1])
\end_layout

\begin_layout Plain Layout

hold on
\end_layout

\begin_layout Plain Layout

ylim([-0.1 0.3])
\end_layout

\begin_layout Plain Layout

scatter(Y,zeros(size(Y)), 'xb')
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename scripts/EM_example_images/figure_0.eps
	scale 60

\end_inset


\end_layout

\end_deeper
\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

4- Initialize variables.
 
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

muhat = [4; 6];
\end_layout

\begin_layout Plain Layout

sigmahat = zeros(1,1,2);
\end_layout

\begin_layout Plain Layout

sigmahat(1,1,1) = 1;
\end_layout

\begin_layout Plain Layout

sigmahat(1,1,2) = 1;
\end_layout

\begin_layout Plain Layout

phat = [0.5 0.5]
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

phat = 1x2    
\end_layout

\begin_layout Plain Layout

    0.5000    0.5000
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

5- Iterate the E-step and M-step (see functions below)
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

niter = 6;
\end_layout

\begin_layout Plain Layout

rem = 3;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

figure(2)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

subplot(niter/rem + 1, 1, 1)
\end_layout

\begin_layout Plain Layout

Mhat = gmdistribution(muhat,sigmahat,phat);
\end_layout

\begin_layout Plain Layout

MMhat = Mhat.pdf(x);
\end_layout

\begin_layout Plain Layout

plot(x,MM1,'LineWidth',2,'Color',[1 0.8 0.9 1])
\end_layout

\begin_layout Plain Layout

hold on
\end_layout

\begin_layout Plain Layout

plot(x,MMhat, 'LineWidth', 2, 'Color', [0.8 0.8 1 1])
\end_layout

\begin_layout Plain Layout

legend('Real mixture','Estimated mixture')
\end_layout

\begin_layout Plain Layout

hold off
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

for i=1:niter
\end_layout

\begin_layout Plain Layout

    g = e_step(Y,muhat,sigmahat,phat);
\end_layout

\begin_layout Plain Layout

    [muhat,sigmahat,phat] = m_step(g,Y);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    Mhat = gmdistribution(muhat,sigmahat,phat);
\end_layout

\begin_layout Plain Layout

    MMhat = Mhat.pdf(x);
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

    if mod(i,rem) == 0
\end_layout

\begin_layout Plain Layout

        subplot(niter/rem+1,1,i/rem+1)
\end_layout

\begin_layout Plain Layout

        plot(x,MM1,'LineWidth',2,'Color',[1 0.8 0.9 1])
\end_layout

\begin_layout Plain Layout

        hold on
\end_layout

\begin_layout Plain Layout

        plot(x,MMhat, 'LineWidth', 2, 'Color', [0.8 0.8 1 1])
\end_layout

\begin_layout Plain Layout

        legend('Real mixture','Estimated mixture')
\end_layout

\begin_layout Plain Layout

        hold off
\end_layout

\begin_layout Plain Layout

    end
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename scripts/EM_example_images/figure_1.eps
	scale 60

\end_inset


\end_layout

\end_deeper
\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

muhat
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

muhat = 2x1    
\end_layout

\begin_layout Plain Layout

    0.8838
\end_layout

\begin_layout Plain Layout

    9.0175
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

sigmahat(1,1,:)
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

ans = 
\end_layout

\begin_layout Plain Layout

ans(:,:,1) =
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    0.2352
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

ans(:,:,2) =
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    0.5825
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

phat
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

phat = 1x2    
\end_layout

\begin_layout Plain Layout

    0.3400    0.6600
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

function g = e_step(Y,muhat,sigmahat,pihat)
\end_layout

\begin_layout Plain Layout

    g = zeros(length(Y),length(muhat));
\end_layout

\begin_layout Plain Layout

    denom = zeros(length(Y),1);
\end_layout

\begin_layout Plain Layout

    for k=1:length(muhat)
\end_layout

\begin_layout Plain Layout

        g(:,k) = pihat(k)*normpdf(Y(:),muhat(k),sigmahat(1,1,k));
\end_layout

\begin_layout Plain Layout

        denom = denom + pihat(k)*normpdf(Y(:),muhat(k),sigmahat(1,1,k));
\end_layout

\begin_layout Plain Layout

    end
\end_layout

\begin_layout Plain Layout

    g = g./denom;
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

function [muhat,sigmahat,pihat] = m_step(g,Y)
\end_layout

\begin_layout Plain Layout

    s = size(g);
\end_layout

\begin_layout Plain Layout

    muhat = zeros(s(2),1);
\end_layout

\begin_layout Plain Layout

    for k=1:s(2)
\end_layout

\begin_layout Plain Layout

        muhat(k) = Y*g(:,k) / sum(g(:,k));
\end_layout

\begin_layout Plain Layout

    end
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    sigmahat = zeros(1,1,s(2));
\end_layout

\begin_layout Plain Layout

    for k=1:s(2)
\end_layout

\begin_layout Plain Layout

        sigmahat(1,1,k) = (Y-muhat(k)).^2*g(:,k) / sum(g(:,k));
\end_layout

\begin_layout Plain Layout

    end
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    pihat = zeros(1,s(2));
\end_layout

\begin_layout Plain Layout

    for k=1:s(2)
\end_layout

\begin_layout Plain Layout

        pihat(k) = sum(g(:,k)) / length(Y);
\end_layout

\begin_layout Plain Layout

    end
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Special cases
\end_layout

\begin_layout Standard
The shape of the Gaussians can be restricted, obtaining special cases of
 mixtures:
\end_layout

\begin_layout Itemize
No restrictions on 
\begin_inset Formula $\Sigma_{k}$
\end_inset

: the general case, in which each cluster can have general Gaussian shape.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\Sigma_{k}$
\end_inset

 are diagonal: each Gaussian component is forced to have no correlation
 among input dimensions.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\Sigma_{k}=\sigma^{2}I$
\end_inset

 are isotropic or spherical: each Gaussian component is forced to be spherical,
 so no correlation among input variables and same scaling across each input
 variable.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Linear Classifiers
\end_layout

\begin_layout Standard
In classification, we have a 
\series bold
labelled
\series default
 dataset, 
\begin_inset Formula $D=\left\{ \left(x_{1},y_{1}\right),...,\left(x_{n},y_{n}\right)\right\} $
\end_inset

 where 
\begin_inset Formula $x_{i}\in\mathbb{R}^{d}$
\end_inset

 and 
\begin_inset Formula $y_{i}\in\mathcal{Y}=\left\{ l_{1},...,l_{K}\right\} $
\end_inset

 are 
\series bold
labels
\series default
.
 Thus, the tuple 
\begin_inset Formula $\left(x_{i},y_{i}\right)$
\end_inset

 means that the vector 
\begin_inset Formula $x_{i}$
\end_inset

 is associated to the label 
\begin_inset Formula $y_{i}$
\end_inset

.
 With this setup, we aim at producing a 
\series bold
classification model
\series default
, meaning a function that, given a new input 
\begin_inset Formula $x'$
\end_inset

, predicts the label 
\begin_inset Formula $y'$
\end_inset

 that it should be associated with.
 Usually, we distinguish between two kinds of classification, attending
 to the number of labels considered:
\end_layout

\begin_layout Itemize
In 
\series bold
binary classification
\series default
, there are only two possible labels, 
\begin_inset Formula $\left|\mathcal{Y}\right|=2$
\end_inset

.
\end_layout

\begin_layout Itemize
In 
\series bold
multi-class classification
\series default
, there are more than two labels, 
\begin_inset Formula $\left|\mathcal{Y}\right|>2$
\end_inset

.
\end_layout

\begin_layout Standard
Now, we introduce some useful terms:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tcolorbox}
\end_layout

\end_inset


\end_layout

\begin_layout Definition
The 
\series bold
decision regions
\series default
 are a partition of the feature space, 
\begin_inset Formula $\left\{ P_{1},...,P_{K}\right\} \subset\mathbb{R}^{d}$
\end_inset

, such that 
\begin_inset Formula $\bigcap_{j=1}^{K}P_{j}=\emptyset$
\end_inset

, 
\begin_inset Formula $\bigcup_{j=1}^{K}P_{j}=\mathbb{R}^{d}$
\end_inset

.
 Intuitively, they are the regions in which we divide the feature space,
 so that all elements inside a region have the same label.
\end_layout

\begin_layout Definition
The 
\series bold
decision boundaries
\series default
 are the points in the frontier between decision regions.
\end_layout

\begin_layout Definition
A 
\series bold
classifier
\series default
 can then be understood as the creation of the decision regions.
\end_layout

\begin_layout Definition
A 
\series bold
linear classifier
\series default
 is a classifier in which the decision boundaries are 
\begin_inset Formula $d-1$
\end_inset

-dimensional hyperplanes.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{tcolorbox}
\end_layout

\end_inset


\end_layout

\begin_layout Example
A visualization.
\end_layout

\begin_layout Example
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Different-classifiers.-Linear"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can see different classification models and the regions they generate
 for the iris dataset.
 The left model correspond to a linear classifier, and we can see how the
 decision boundaries are linear.
 The other two models are not linear.
\end_layout

\begin_layout Example
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado16.png
	scale 40

\end_inset


\begin_inset Graphics
	filename pegado17.png
	scale 40

\end_inset


\begin_inset Graphics
	filename pegado18.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Different classifiers.
 Linear (left).
 Quadratic (middle).
 knn (right).
\begin_inset CommandInset label
LatexCommand label
name "fig:Different-classifiers.-Linear"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Many useful classifiers don't just predict the input's class, but also the
 probabilities of the input belonging to each class.
 This is desirable, as it enables us to express uncertainty about the prediction
 that we make.
 For example, in binary classification it is a common approach to map the
 target labels to be 0 or 1, and then make predictions as a continuous value
 in 
\begin_inset Formula $\left[0,1\right]$
\end_inset

.
 More explicitly, we can have labels 
\begin_inset Formula $\mathcal{Y}=\left\{ 'sick','healthy'\right\} $
\end_inset

 and encode 
\begin_inset Formula $'sick'=1$
\end_inset

, 
\begin_inset Formula $'healthy'=0$
\end_inset

, so that given a patient 
\begin_inset Formula $x'$
\end_inset

, we obtain a prediction 
\begin_inset Formula $y'\in\left[0,1\right]$
\end_inset

, indicating the probability of the patient being sick.
 
\end_layout

\begin_layout Standard
In classification with 
\begin_inset Formula $K>2$
\end_inset

 classes, it is common to encode the labels using 
\series bold
one-hot encoding
\series default
, meaning that we map the labels into the set 
\begin_inset Formula $\left\{ 0,1\right\} ^{K}$
\end_inset

.
 For example, if we have three labels, then they are encoded as 
\begin_inset Formula $\left\{ \left(1,0,0\right),\left(0,1,0\right),\left(0,0,1\right)\right\} $
\end_inset

.
 When in this scenario, predictions are usually points in the 
\begin_inset Formula $\left(K-1\right)$
\end_inset

-simplex, i.e., a prediction 
\begin_inset Formula $y'=\left(y'_{1},...,y'_{K}\right)$
\end_inset

 must be 
\begin_inset Formula $0\leq y'_{k}\leq1,\forall k$
\end_inset

 and 
\begin_inset Formula $\sum_{k}y'_{k}=1$
\end_inset

.
 Each 
\begin_inset Formula $y'_{k}$
\end_inset

 represents the probability of the input belonging to class 
\begin_inset Formula $k$
\end_inset

.
\end_layout

\begin_layout Subsection
Decision boundary in probabilistic models
\end_layout

\begin_layout Standard
From a probabilistic perspective, we can think of the 
\color blue
joint probability of examples and labels
\color inherit
, 
\begin_inset Formula $p\left(x,y\right)$
\end_inset

.
 When we are building a classifier, we then want to minimize the expected
 loss, or expected error.
 Note, nonetheless, that loss here is a bit different from the regression
 loss.
 A natural way to think about this loss is through 
\color red
loss or cost matrices
\color inherit
.
 A cost matrix is a table as the following:
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="5">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Real
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $y=l_{1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $y=l_{2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
...
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $y=l_{K}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Predicted
\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $y'=l_{1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $c_{1,2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
...
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $c_{1,K}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $y'=l_{2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $c_{2,1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
...
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $c_{2,K}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\vdots$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\vdots$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\vdots$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\ddots$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\vdots$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $y'=l_{K}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $c_{K,1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $c_{K,2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
...
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
This matrix indicates the cost of each error.
 Note that not all errors need to have the same cost.
 For example, in a medical context, it has a higher cost to predict that
 a sick patient is healthy (this person could potentially die), than to
 predict that a healthy person is sick (in which case further tests would
 probably correct the mistake).
 A cost matrix for this case could look like the following:
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Real
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $y=healthy$
\end_inset


\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $y=sick$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Predicted
\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $y'=healthy$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
60
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $y'=sick$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
We will focus in the case of all errors having the same impact, which is
 called the 
\color blue
0-1 loss
\color inherit
.
\end_layout

\begin_layout Standard
Let's now see how, given a new example 
\begin_inset Formula $x$
\end_inset

, we can use a 'rule' to choose the label that 
\begin_inset Formula $x$
\end_inset

 should have.
 We have random variables 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 with joint distribution 
\begin_inset Formula $p\left(X,Y\right)$
\end_inset

.
 We can compute the expected loss of assigning the label 
\begin_inset Formula $c\in\mathcal{Y}$
\end_inset

 to 
\begin_inset Formula $x$
\end_inset

.
 For this, we first define the loss function as the 0-1 loss:
\begin_inset Formula 
\[
L\left(a,b\right)=\begin{cases}
0 & if\ a==b\\
1 & otherwise
\end{cases}.
\]

\end_inset

 Therefore:
\begin_inset Formula 
\begin{align*}
E_{Y}\left[L\left(Y,c\right)\right]= & \sum_{y\in\mathcal{Y}}L\left(y,c\right)p\left(Y=y|x\right)\\
= & \sum_{y\neq c}p\left(Y=y|x\right)\\
= & p\left(Y\neq c|x\right)\\
= & 1-p\left(Y=c|x\right).
\end{align*}

\end_inset

 Of course, we want to minimize the expected loss, so we aim at predicting
 the class 
\begin_inset Formula $y'$
\end_inset

 minimizing 
\begin_inset Formula $E_{Y}\left[L\left(Y,y'\right)\right]$
\end_inset

:
\begin_inset Formula 
\[
y'=\arg\min_{y}E_{Y}\left[L\left(Y,y\right)\right]=\arg\min_{y}\left\{ 1-p\left(y|x\right)\right\} =\arg\max_{y}p\left(y|x\right).
\]

\end_inset

 This is called the 
\series bold
Bayes classifier
\series default
, and it is optimal when we use 0-1 loss.
 Its error is given by the so-called 
\series bold
Bayes error rate
\series default
:
\begin_inset Formula 
\[
BER=1-E_{X}\left[p\left(y'|x\right)\right]=1-\int_{x}p\left(y'|x\right)p\left(x\right)dx=1-\int_{x}p\left(x|y'\right)p\left(y'\right)dx.
\]

\end_inset

 Of course, we can use this classifier to partition the feature space into
 regions 
\begin_inset Formula $\mathcal{R}_{c},c\in\mathcal{Y}$
\end_inset

, and we can compute the BER summing over all regions:
\begin_inset Formula 
\[
BER=1-\sum_{c}\int_{x\in\mathcal{R}_{c}}p\left(x|c\right)p\left(c\right)dx.
\]

\end_inset

 Before, we claimed that the Bayes classifier is optimal.
 However, in practice we don't know the distribution 
\begin_inset Formula $p\left(y,x\right)$
\end_inset

, so it cannot be implemented exactly.
 Therefore, 
\begin_inset Formula $p\left(y|x\right)$
\end_inset

 is estimated from data, and this estimates are used for classification,
 incurring in additional errors.
 To learn 
\begin_inset Formula $p\left(y|x\right)$
\end_inset

, there are two basic approaches, namely discrimintative classifiers and
 generative classifiers.
\end_layout

\begin_layout Subsection
Generative classifiers
\end_layout

\begin_layout Standard
Generative classifiers learn 
\begin_inset Formula $p\left(y|x\right)$
\end_inset

 through the Bayes rule.
\end_layout

\begin_layout Subsubsection
Discriminant analysis
\end_layout

\begin_layout Standard
Discriminant analysis is the result of implementing a Bayes classifier assuming
 that the 
\color blue
class-conditional distributions 
\begin_inset Formula $p\left(x|y\right)$
\end_inset

 are gaussian
\color inherit
.
 This means that, having 
\begin_inset Formula $\mathcal{Y}=\left\{ c_{1},...,c_{K}\right\} $
\end_inset

, then it is
\begin_inset Formula 
\[
p\left(x|y=c_{k}\right)\sim\mathcal{N}\left(\mu_{k},\Sigma_{k}\right).
\]

\end_inset

 If we also assume that the prior distributions are 
\begin_inset Formula 
\[
p\left(y=c_{k}\right)=\pi_{k},
\]

\end_inset

 with 
\begin_inset Formula $\sum_{k}\pi_{k}=1$
\end_inset

, then we define the 
\series bold
discriminant functions
\begin_inset Formula 
\begin{align*}
g_{k}\left(x\right)= & \log\left(P\left(y=c_{k}\right)P\left(x|y=x_{k}\right)\right)\\
= & \log\left(\pi_{k}\cdot\frac{1}{\det\left(\Sigma_{k}\right)^{\frac{1}{2}}\left(2\pi\right)^{\frac{d}{2}}}\exp\left\{ -\frac{1}{2}\left(x-\mu_{k}\right)^{T}\Sigma^{-1}\left(x-\mu_{k}\right)\right\} \right)\\
= & \log\pi_{k}-\frac{1}{2}\log\left(\det\Sigma_{k}\right)-\frac{d}{2}\log\left(2\pi\right)-\frac{1}{2}\left(x-\mu_{k}\right)^{T}\Sigma^{-1}\left(x-\mu_{k}\right)\\
= & \log\pi_{k}-\frac{1}{2}\left[\log\left(\det\Sigma_{k}+\left(x-\mu_{k}\right)^{T}\Sigma_{k}^{-1}\left(x-\mu_{k}\right)\right)\right]+const.
\end{align*}

\end_inset

 
\series default
This is a 
\series bold
quadratic discriminant function
\series default
, and the corresponding classifier is implemented by predicting
\begin_inset Formula 
\[
y'=c_{k'},\ where\ k'=\arg\max_{k}g_{k}\left(x\right).
\]

\end_inset

 This corresponds to chossing the label with maximum probability a posteriori.
\end_layout

\begin_layout Standard
The 
\series bold
decision boundaries
\series default
 in this case are those regions in which there exist 
\begin_inset Formula $k_{1},k_{2}$
\end_inset

 with
\begin_inset Formula 
\[
g_{k_{1}}\left(x\right)=g_{k_{2}}\left(x\right).
\]

\end_inset


\end_layout

\begin_layout Standard
These corresponds to hyper-quadrics in the feature space, and this is a
 quadratic method, usually called 
\color green
quadratic discriminant analysis (QDA)
\color inherit
.
\end_layout

\begin_layout Standard
Of course, we can further simplify our assumptions, by assuming that all
 labels have the same covariance matrix, 
\begin_inset Formula $\Sigma_{k}=\Sigma$
\end_inset

 for all 
\begin_inset Formula $k=1,...,K$
\end_inset

.
 In this simpler case, the discriminant functions end up being
\begin_inset Formula 
\[
g_{k}\left(x\right)=\log\pi_{k}+\mu_{k}^{T}\Sigma^{-1}x-\frac{1}{2}\mu_{k}^{T}\Sigma^{-1}\mu_{k},
\]

\end_inset

 because now 
\begin_inset Formula $\det\Sigma_{k}=\det\Sigma$
\end_inset

 is constant for all 
\begin_inset Formula $k$
\end_inset

, so we can remove it.
 Furthermore, the term 
\begin_inset Formula $x^{T}\Sigma_{k}^{-1}x=x^{T}\Sigma^{-1}x$
\end_inset

 is also constant with respect to 
\begin_inset Formula $k$
\end_inset

, so it will not affect the 
\begin_inset Formula $k$
\end_inset

 chosen.
 Therefore, we end up with 
\series bold
linear discriminant functions
\series default
, in which the 
\series bold
decision boundaries
\series default
 correspond to hyperplanes in the feature space.
 This is a linear method, usually called 
\color green
linear discriminant analysis (LDA)
\color inherit
.
\end_layout

\begin_layout Standard
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Different-classifiers.-Linear"
plural "false"
caps "false"
noprefix "false"

\end_inset

, the left diagram corresponds to a LDA partitioning of the feature space
 for the iris dataset, while the one in the center is a QDA partitioning.
\end_layout

\begin_layout Subsubsection*
Further assumptions
\end_layout

\begin_layout Standard
Of course, we can make more simplifying assumptions for our model, such
 as:
\end_layout

\begin_layout Itemize
\begin_inset Formula $\Sigma=diag\left(\sigma_{1}^{2},...,\sigma_{d}^{2}\right)$
\end_inset

 is diagonal.
 In this case, we obtain
\begin_inset Formula 
\[
g_{k}\left(x\right)=\log\pi_{k}-\frac{1}{2}\sum_{j=1}^{d}\frac{\left(\mu_{kj}-x_{j}\right)^{2}}{\sigma_{j}^{2}}.
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\Sigma$
\end_inset

 is an isotropic Gaussian, i.e., 
\begin_inset Formula $\Sigma=\sigma^{2}I$
\end_inset

.
 In this case, 
\begin_inset Formula 
\[
g_{k}\left(x\right)=\log\pi_{k}-\frac{1}{2\sigma^{2}}\left\Vert \mu_{k}-x\right\Vert ^{2}.
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\pi_{k}=\frac{1}{K},\forall k=1,...,K$
\end_inset

, all priors are equal.
 In this case
\begin_inset Formula 
\[
g_{k}\left(x\right)=-\frac{1}{2}\left\Vert \mu_{k}-x\right\Vert ^{2}.
\]

\end_inset


\end_layout

\begin_layout Subsubsection*
Distance-based learning perspective
\end_layout

\begin_layout Standard
In all seen cases, we have a minimum-distance classifier in 
\begin_inset Formula $\mathbb{R}^{d}$
\end_inset

:
\end_layout

\begin_layout Itemize
The general QDA case corresponds to using different Mahalanobis distance
 from 
\begin_inset Formula $x$
\end_inset

 to each class center 
\begin_inset Formula $\mu_{k}$
\end_inset

.
\end_layout

\begin_layout Itemize
The LDA case uses the same Mahalanobis distance from 
\begin_inset Formula $x$
\end_inset

 to each class center 
\begin_inset Formula $\mu_{k}$
\end_inset

.
\end_layout

\begin_layout Itemize
In the case of all covariance matrix being equal and diagonal, the distance
 is the weighted Euclidean distance.
\end_layout

\begin_layout Itemize
In the isotropic Gaussians case, the distance corresponds to the usual Euclidean
 distance.
\end_layout

\begin_layout Subsubsection*
Implementation
\end_layout

\begin_layout Standard
It is usual to use MLE and estimate the centers and covariance matrices
 using the training dataset.
 If we define the sets
\begin_inset Formula 
\[
S_{k}=\left\{ x_{i}|y_{i}=c_{k},\left(x_{i},y_{i}\right)\in\mathcal{D}\right\} 
\]

\end_inset

 and 
\begin_inset Formula $n_{k}=card\left(S_{k}\right)$
\end_inset

, then, the estimates are
\begin_inset Formula 
\[
\hat{\pi}_{k}=\frac{n_{k}}{n},\ \hat{\mu}_{k}=\frac{1}{n_{k}}\sum_{x\in S_{k}}x,
\]

\end_inset

 and the covariance matrix is:
\end_layout

\begin_layout Itemize
In QDA:
\begin_inset Formula 
\[
\hat{\Sigma}_{k}=\frac{1}{n_{k}-1}\sum_{x\in S_{k}}\left(x-\mu_{k}\right)\left(x-\mu_{k}\right)^{T}.
\]

\end_inset


\end_layout

\begin_layout Itemize
In LDA
\begin_inset Formula 
\[
\hat{\Sigma}=\sum_{k=1}^{K}\frac{n_{k}-1}{n-n_{k}}\hat{\Sigma}_{k}.
\]

\end_inset


\end_layout

\begin_layout Subsubsection*
Final remarks on discriminant analysis
\end_layout

\begin_layout Standard
Bayesian classifiers are optimal when the class-conditional densities and
 priors are known.
 This means that, if know the underlying distributions, then these classifiers
 are our best choice.
 Of course, this is not realistic, and estimations need to be made.
 However, normal distributions appear in a wide range of real scenarios,
 and even if we have to estimate the centers and covariance matrices, QDA
 and LDA are a very good choice when data resembles Gaussian distributions.
 In addition, they are well-principled, having a solid mathematical theory
 behind them, they are fast and reliable.
\end_layout

\begin_layout Standard
Of course, if the real distribution is very far from being a Gaussian, then
 the model obtained will be poor, so one should take care of this.
\end_layout

\begin_layout Standard
Also, it is important to ensure that we are correctly estimating the parameters
 of the Gaussians, because otherwise the model will not work, not even with
 underlying Gaussian data.
\end_layout

\begin_layout Standard
And it is clear that once we are relying on sample estimates instead of
 population parameters, we loose the optimality of the method.
\end_layout

\begin_layout Standard
In practice, it is really hard to assess which assumptions hold and which
 ones do not, so we can be limited to use a trial and error approach.
\end_layout

\begin_layout Subsubsection
Regularized discriminant analysis (RDA)
\end_layout

\begin_layout Standard
When data is scarce, some problems can arise while using discriminant analysis.
 For example:
\end_layout

\begin_layout Itemize
If there are more dimensions than samples with some label, i.e.
 
\begin_inset Formula $d>n_{k}$
\end_inset

, for some 
\begin_inset Formula $k$
\end_inset

, then QDA cannot be applied, because 
\begin_inset Formula $\hat{\Sigma}_{k}$
\end_inset

 is singular.
 The reason is that each of the sample is adding a rank-1 matrix.
 Therefore, to get a rank-
\begin_inset Formula $d$
\end_inset

 matrix, we need at least 
\begin_inset Formula $n_{k}$
\end_inset

 samples.
 Not only this, but in fact, as we are subtracting the sample mean, the
 last value is linearly dependant of the previous 
\begin_inset Formula $n-1$
\end_inset

 values.
 Therefore, we need at least 
\begin_inset Formula $n_{k}+1$
\end_inset

 samples to be able to construct a rank-
\begin_inset Formula $d$
\end_inset

 covariance matrix.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $d>n$
\end_inset

, then we cannot use QDA nor LDA, as all covariance matrices are singular.
\end_layout

\begin_layout Standard

\color blue
RDA
\color inherit
 computes the covariance matrices as
\begin_inset Formula 
\[
\hat{\Sigma}_{k}\left(\alpha\right)=\alpha\hat{\Sigma}_{k}+\left(1-\alpha\right)\hat{\Sigma},
\]

\end_inset

 where 
\begin_inset Formula $\alpha\in\left[0,1\right]$
\end_inset

 is the regularization parameter.
 The method is QDA when 
\begin_inset Formula $\alpha=1$
\end_inset

 and is LDA when 
\begin_inset Formula $\alpha=0$
\end_inset

.
 In any other case, it is something in between.
 
\end_layout

\begin_layout Standard
A further way to regularize the matrices is by
\begin_inset Formula 
\[
\hat{\Sigma}_{k}\left(\alpha,\gamma\right)=\left(1-\gamma\right)\hat{\Sigma}_{k}\left(\alpha\right)+\gamma\hat{\sigma}^{2}I,
\]

\end_inset

 where 
\begin_inset Formula $\hat{\sigma}^{2}=\frac{Tr\left[\hat{\Sigma}_{k}\left(\alpha\right)\right]}{d}$
\end_inset

, and the diagonal term improves the well-conditioning of the method.
\end_layout

\begin_layout Subsection
Na√Øve Bayes
\end_layout

\begin_layout Standard
The 
\color blue
Na√Øve Bayes Classifier
\color inherit
 is a Bayesian classifier that assumes that the features are pair-wise independe
nt in the class-conditional distribution.
 This means that the probability can be written as
\begin_inset Formula 
\[
p\left(x|y\right)=\prod_{j=1}^{d}p\left(x_{j}|y\right).
\]

\end_inset

 This assumptions does not hold in general, but this approach can provide
 a good approximation in many cases.
 Also, it is practical, as the amount of parameters to estimate is small.
 
\end_layout

\begin_layout Standard
As before, we classify the input record 
\begin_inset Formula $x$
\end_inset

 in the class 
\begin_inset Formula $c_{k}$
\end_inset

 that maximizes the discriminant function:
\begin_inset Formula 
\[
g_{k}\left(x\right)=\log\pi_{k}+\sum_{j=1}^{K}\log p\left(x_{j}|y=c_{k}\right).
\]

\end_inset

 Therefore, we need to
\end_layout

\begin_layout Enumerate
Estimate the class priors as the sample frequency, 
\begin_inset Formula $\pi_{k}=\frac{n_{k}}{n}.$
\end_inset


\end_layout

\begin_layout Enumerate
Estimate the class-conditional densities for each input feature independently.
\end_layout

\begin_layout Standard
Na√Øve Bayes can also be used in the case of categorical variables.
 To model binary features, we can use, for example, the Bernoulli distribution
\begin_inset Formula 
\[
P\left(x|p\right)=p^{x}\left(1-p\right)^{1-x},
\]

\end_inset

where 
\begin_inset Formula $x\in\left\{ 0,1\right\} $
\end_inset

 and 
\begin_inset Formula $p\in\left[0,1\right]$
\end_inset

 is the probability of the event happening.
 For a binary feature, we would need to estimate 
\begin_inset Formula $K$
\end_inset

 parameters, one for each class, so that
\begin_inset Formula 
\[
P\left(x|y=c_{k}\right)=p_{k}^{x}\left(1-p_{k}\right)^{1-x}.
\]

\end_inset

 If we had all our features as binary, then the discriminant functions would
 be
\begin_inset Formula 
\begin{align*}
g_{k}\left(x\right)= & \log\pi_{k}+\sum_{j=1}^{d}\log P\left(x_{j}|y=c_{k}\right)\\
= & \log\pi_{k}+\sum_{j=1}^{d}\left[x_{j}\log p_{k,j}+\left(1-x_{j}\right)\log\left(1-p_{k,j}\right)\right],
\end{align*}

\end_inset

 where 
\begin_inset Formula $p_{k,j}$
\end_inset

 is the Bernoulli parameter for the 
\begin_inset Formula $j^{th}$
\end_inset

 feature and the 
\begin_inset Formula $k^{th}$
\end_inset

 class.
 Note that this is a linear function with respect to to 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
If instead we have categorical features with more values, we can then use
 the Categorical distribution
\begin_inset Formula 
\[
g_{k}\left(x\right)=\log\pi_{k}+\sum_{j=1}^{d}\sum_{v}\left[x_{j}=v\right]\log p_{k,j,v},
\]

\end_inset

 where 
\begin_inset Formula $\left[expr\right]$
\end_inset

 is 1 if 
\begin_inset Formula $expr$
\end_inset

 is True and 0 otherwise, and 
\begin_inset Formula $p_{k,j,v}$
\end_inset

 is the Categorical parameter for the value 
\begin_inset Formula $v$
\end_inset

 of the 
\begin_inset Formula $j^{th}$
\end_inset

 feature and the 
\begin_inset Formula $k^{th}$
\end_inset

 class.
\end_layout

\begin_layout Standard
Now, we need to estimate these parameters, for which we can use the sample
 frequencies.
 Note how 0-frequencies can be a problem, so it is a common approach to
 utilize 
\series bold
Laplace smoothing
\series default

\begin_inset Formula 
\[
\hat{p}\left(v|y=c_{k}\right)=\frac{n_{k,v}+p}{n_{k}+pV}.
\]

\end_inset

 Here:
\end_layout

\begin_layout Itemize
\begin_inset Formula $p\in\mathbb{R}^{+}$
\end_inset

 is a weight parameter assigned to the prior distribution of observing values
 
\begin_inset Formula $v$
\end_inset

.
 It is typically set to 1, just to avoid 0 values.
\end_layout

\begin_layout Itemize
\begin_inset Formula $V$
\end_inset

 is the number of modalities (number of distinct values) of the feature
 modelled.
\end_layout

\begin_layout Example
Na√Øve Bayes in MATLAB
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
matlabtitle{Na√Øve Bayes Classifier Example}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

Outlook = categorical({'Sunny'; 'Sunny'; 'Overcast'; 'Rainy'; 'Rainy'; 'Rainy';
 'Overcast'; 'Sunny'; 'Sunny'; 'Rainy'; 'Sunny'; 'Overcast'; 'Overcast';
 'Rainy'});
\end_layout

\begin_layout Plain Layout

Temperature = categorical({'Hot'; 'Hot'; 'Hot'; 'Mild'; 'Cool'; 'Cool';
 'Cool'; 'Mild'; 'Cool'; 'Mild'; 'Mild'; 'Mild'; 'Hot'; 'Mild'});
\end_layout

\begin_layout Plain Layout

Humidity = categorical({'High'; 'High'; 'High'; 'High'; 'Normal'; 'Normal';
 'Normal'; 'High'; 'Normal'; 'Normal'; 'Normal'; 'High'; 'Normal'; 'High'});
\end_layout

\begin_layout Plain Layout

Wind = categorical({'Weak'; 'Strong'; 'Weak'; 'Weak'; 'Weak'; 'Strong';
 'Strong'; 'Weak'; 'Weak'; 'Weak'; 'Strong'; 'Strong'; 'Weak'; 'Strong'});
\end_layout

\begin_layout Plain Layout

PlayBall = categorical({'No'; 'No'; 'Yes'; 'Yes'; 'Yes'; 'No'; 'Yes'; 'No';
 'Yes'; 'Yes'; 'Yes'; 'Yes'; 'Yes'; 'No'});
\end_layout

\begin_layout Plain Layout

dataset = table(Outlook, Temperature, Humidity, Wind, PlayBall);
\end_layout

\begin_layout Plain Layout

dataset
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabtableoutput}
\end_layout

\begin_layout Plain Layout

{
\end_layout

\begin_layout Plain Layout


\backslash
begin{tabular} {|c|c|c|c|c|c|}
\backslash
hline
\end_layout

\begin_layout Plain Layout


\backslash
mlcell{ } & 
\backslash
mlcell{Outlook} & 
\backslash
mlcell{Temperature} & 
\backslash
mlcell{Humidity} & 
\backslash
mlcell{Wind} & 
\backslash
mlcell{PlayBall} 
\backslash

\backslash
 
\backslash
hline
\end_layout

\begin_layout Plain Layout


\backslash
mlcell{1} & 
\backslash
mlcell{Sunny} & 
\backslash
mlcell{Hot} & 
\backslash
mlcell{High} & 
\backslash
mlcell{Weak} & 
\backslash
mlcell{No} 
\backslash

\backslash
 
\backslash
hline
\end_layout

\begin_layout Plain Layout


\backslash
mlcell{2} & 
\backslash
mlcell{Sunny} & 
\backslash
mlcell{Hot} & 
\backslash
mlcell{High} & 
\backslash
mlcell{Strong} & 
\backslash
mlcell{No} 
\backslash

\backslash
 
\backslash
hline
\end_layout

\begin_layout Plain Layout


\backslash
mlcell{3} & 
\backslash
mlcell{Overcast} & 
\backslash
mlcell{Hot} & 
\backslash
mlcell{High} & 
\backslash
mlcell{Weak} & 
\backslash
mlcell{Yes} 
\backslash

\backslash
 
\backslash
hline
\end_layout

\begin_layout Plain Layout


\backslash
mlcell{4} & 
\backslash
mlcell{Rainy} & 
\backslash
mlcell{Mild} & 
\backslash
mlcell{High} & 
\backslash
mlcell{Weak} & 
\backslash
mlcell{Yes} 
\backslash

\backslash
 
\backslash
hline
\end_layout

\begin_layout Plain Layout


\backslash
mlcell{5} & 
\backslash
mlcell{Rainy} & 
\backslash
mlcell{Cool} & 
\backslash
mlcell{Normal} & 
\backslash
mlcell{Weak} & 
\backslash
mlcell{Yes} 
\backslash

\backslash
 
\backslash
hline
\end_layout

\begin_layout Plain Layout


\backslash
mlcell{6} & 
\backslash
mlcell{Rainy} & 
\backslash
mlcell{Cool} & 
\backslash
mlcell{Normal} & 
\backslash
mlcell{Strong} & 
\backslash
mlcell{No} 
\backslash

\backslash
 
\backslash
hline
\end_layout

\begin_layout Plain Layout


\backslash
mlcell{7} & 
\backslash
mlcell{Overcast} & 
\backslash
mlcell{Cool} & 
\backslash
mlcell{Normal} & 
\backslash
mlcell{Strong} & 
\backslash
mlcell{Yes} 
\backslash

\backslash
 
\backslash
hline
\end_layout

\begin_layout Plain Layout


\backslash
mlcell{8} & 
\backslash
mlcell{Sunny} & 
\backslash
mlcell{Mild} & 
\backslash
mlcell{High} & 
\backslash
mlcell{Weak} & 
\backslash
mlcell{No} 
\backslash

\backslash
 
\backslash
hline
\end_layout

\begin_layout Plain Layout


\backslash
mlcell{9} & 
\backslash
mlcell{Sunny} & 
\backslash
mlcell{Cool} & 
\backslash
mlcell{Normal} & 
\backslash
mlcell{Weak} & 
\backslash
mlcell{Yes} 
\backslash

\backslash
 
\backslash
hline
\end_layout

\begin_layout Plain Layout


\backslash
mlcell{10} & 
\backslash
mlcell{Rainy} & 
\backslash
mlcell{Mild} & 
\backslash
mlcell{Normal} & 
\backslash
mlcell{Weak} & 
\backslash
mlcell{Yes} 
\backslash

\backslash
 
\backslash
hline
\end_layout

\begin_layout Plain Layout


\backslash
mlcell{11} & 
\backslash
mlcell{Sunny} & 
\backslash
mlcell{Mild} & 
\backslash
mlcell{Normal} & 
\backslash
mlcell{Strong} & 
\backslash
mlcell{Yes} 
\backslash

\backslash
 
\backslash
hline
\end_layout

\begin_layout Plain Layout


\backslash
mlcell{12} & 
\backslash
mlcell{Overcast} & 
\backslash
mlcell{Mild} & 
\backslash
mlcell{High} & 
\backslash
mlcell{Strong} & 
\backslash
mlcell{Yes} 
\backslash

\backslash
 
\backslash
hline
\end_layout

\begin_layout Plain Layout


\backslash
mlcell{13} & 
\backslash
mlcell{Overcast} & 
\backslash
mlcell{Hot} & 
\backslash
mlcell{Normal} & 
\backslash
mlcell{Weak} & 
\backslash
mlcell{Yes} 
\backslash

\backslash
 
\backslash
hline
\end_layout

\begin_layout Plain Layout


\backslash
mlcell{14} & 
\backslash
mlcell{Rainy} & 
\backslash
mlcell{Mild} & 
\backslash
mlcell{High} & 
\backslash
mlcell{Strong} & 
\backslash
mlcell{No} 
\backslash

\backslash
 
\end_layout

\begin_layout Plain Layout


\backslash
hline
\end_layout

\begin_layout Plain Layout


\backslash
end{tabular}
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabtableoutput}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

We compute the priors:
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

num_yes = sum(dataset.PlayBall == "Yes");
\end_layout

\begin_layout Plain Layout

num_no = sum(dataset.PlayBall =="No");
\end_layout

\begin_layout Plain Layout

n = num_yes + num_no;
\end_layout

\begin_layout Plain Layout

prior_yes = num_yes / n;
\end_layout

\begin_layout Plain Layout

prior_no = num_no / n;
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

We create a new record $x=(Sunny,Hot,Normal,Weak)$ and predict its class
 using our functions:
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

new_record = table(categorical({'Sunny'}), categorical({'Hot'}), categorical({'N
ormal'}), categorical({'Weak'}), ...
\end_layout

\begin_layout Plain Layout

                   'VariableNames', {'Outlook', 'Temperature', 'Humidity',
 'Wind'});
\end_layout

\begin_layout Plain Layout

[predicted_class, conf] = naive_bayes_classify(new_record, {'Outlook', 'Temperat
ure', 'Humidity', 'Wind'}, dataset, [prior_yes, prior_no], 0);
\end_layout

\begin_layout Plain Layout

fprintf('Predicted class for new record: %s with confidence %f
\backslash
n', char(predicted_class), conf);
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

Predicted class for new record: Yes with confidence 0.672948
\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

[predicted_class, conf] = naive_bayes_classify(new_record, {'Outlook', 'Temperat
ure', 'Humidity', 'Wind'}, dataset, [prior_yes, prior_no], 1);
\end_layout

\begin_layout Plain Layout

fprintf('Predicted class for new record using Laplace Smoothing: %s with
 confidence %f
\backslash
n', char(predicted_class), conf);
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

Predicted class for new record using Laplace Smoothing: Yes with confidence
 0.634538
\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

Now with the built-in classifier:
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

X = table(Outlook, Temperature, Humidity, Wind);
\end_layout

\begin_layout Plain Layout

Y = PlayBall;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

mdl = fitcnb(X, Y, 'ClassNames', {'Yes', 'No'});
\end_layout

\begin_layout Plain Layout

[predicted_class, conf, cost] = predict(mdl, new_record);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% Display the predicted class and confidence
\end_layout

\begin_layout Plain Layout

fprintf('Predicted class for new record: %s with confidence %f
\backslash
n', char(predicted_class(1)), conf(1));
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

Predicted class for new record: Yes with confidence 0.664913
\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

function prob = cond_prob(data, feature, target_class, value)
\end_layout

\begin_layout Plain Layout

    subset = data(data.PlayBall == target_class, :);
\end_layout

\begin_layout Plain Layout

    count = sum(subset.(feature) == value);
\end_layout

\begin_layout Plain Layout

    prob = count / size(subset, 1);
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

function prob = laplace_sm(data, feature, target_class, value, p)
\end_layout

\begin_layout Plain Layout

    subset = data(data.PlayBall == target_class, :);
\end_layout

\begin_layout Plain Layout

    count = sum(subset.(feature) == value);
\end_layout

\begin_layout Plain Layout

    modalities = numel(unique(subset.(feature)));
\end_layout

\begin_layout Plain Layout

    prob = (count + p) / (size(subset, 1) + p*modalities);
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

function [predicted_class, conf] = naive_bayes_classify(record, features,
 training_data, priors, lap)
\end_layout

\begin_layout Plain Layout

    classes = categorical({'Yes', 'No'});
\end_layout

\begin_layout Plain Layout

    probs = zeros(size(classes));
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    for c = 1:length(classes)
\end_layout

\begin_layout Plain Layout

        p = priors(c);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

        for f = 1:length(features)
\end_layout

\begin_layout Plain Layout

            feature = features{f};
\end_layout

\begin_layout Plain Layout

            value = record.(feature);
\end_layout

\begin_layout Plain Layout

            if lap == 0
\end_layout

\begin_layout Plain Layout

                p = p * cond_prob(training_data, feature, classes(c), value);
\end_layout

\begin_layout Plain Layout

            else
\end_layout

\begin_layout Plain Layout

                p = p * laplace_sm(training_data, feature, classes(c), value,
 lap);
\end_layout

\begin_layout Plain Layout

            end
\end_layout

\begin_layout Plain Layout

        end
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

        probs(c) = p;
\end_layout

\begin_layout Plain Layout

    end
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    [~, idx] = max(probs);
\end_layout

\begin_layout Plain Layout

    predicted_class = classes(idx);
\end_layout

\begin_layout Plain Layout

    conf = probs(idx) / sum(probs);
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Gaussian Na√Øve Bayes
\end_layout

\begin_layout Standard
If we have numericla features, the usual approach is to assume they follow
 Gaussian distributions, then we estimate their mean and variance using
 MLE.
 If all features are assumed Gaussian, this approach is equivalent to QDA
 with diagonal covariance matrices.
\end_layout

\begin_layout Standard
Other approaches are:
\end_layout

\begin_layout Itemize
Discretize numerical values and proceed with Categorical NB.
\end_layout

\begin_layout Itemize
Assume a different distribution and estimate its sample parameters from
 data.
\end_layout

\begin_layout Standard
Note that when the data is mixed, we can assume a different distribution
 for each feature, and then add the log-likelihoods altogether.
\end_layout

\begin_layout Subsection
Perceptron and Logistic Regression
\end_layout

\begin_layout Standard
The 
\series bold
perceptron
\series default
 is a mathematical model of the functioning of a neuron in our brain.
 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:A-neuron."
plural "false"
caps "false"
noprefix "false"

\end_inset

 we can see a diagram of a neuron.
 Other neurons transmit signals into our neuron via the dendrites, then
 'something' happens inside the neuron, and it transmits or not signals
 through its axon towards the neurons to which it is connected.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename neuron.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
A neuron.
\begin_inset CommandInset label
LatexCommand label
name "fig:A-neuron."

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
This behavior was mathematically modeled by Roosenblatt in his pioneer paper
 
\begin_inset CommandInset citation
LatexCommand cite
key "Rosenblatt_1958"
literal "false"

\end_inset

.
 He did this with the concept of perceptron:
\end_layout

\begin_layout Itemize
A perceptron is a function 
\begin_inset Formula $F:\mathbb{R}^{d}\rightarrow\left\{ -1,1\right\} $
\end_inset

, where
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $d$
\end_inset

 is the number of inputs.
 In the neuron analogy, it is the number of dendrites of the neuron.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\left\{ -1,1\right\} $
\end_inset

 represents that a neuron can send a signal, 
\begin_inset Formula $F=1$
\end_inset

, or not, 
\begin_inset Formula $F=-1$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
This function operates in the following maner:
\end_layout

\begin_deeper
\begin_layout Itemize
For each input 
\begin_inset Formula $x_{i}$
\end_inset

, there is a weight 
\begin_inset Formula $w_{i}$
\end_inset

, for 
\begin_inset Formula $i=1,...,d$
\end_inset

.
 This models somehow the strength of the connection between two neurons.
\end_layout

\begin_layout Itemize
There is an aritificial input called 
\series bold
bias
\series default
, which is always set to 1 and serves the purpose of being able to center
 the inputs.
 It is usually depicted by 
\begin_inset Formula $x_{0}=1$
\end_inset

 and goes with its weight 
\begin_inset Formula $w_{0}$
\end_inset

, used to determine the true bias.
\end_layout

\begin_layout Itemize
The inputs are weighted and sum, to form a state of the neuron
\begin_inset Formula 
\[
S=\sum_{i=0}^{d}w_{i}x_{i}=W^{T}X.
\]

\end_inset


\end_layout

\begin_layout Itemize
Finally, we apply the function 
\begin_inset Formula 
\[
f\left(S\right)=\begin{cases}
1 & if\ S>0\\
-1 & otherwise
\end{cases}.
\]

\end_inset

 
\end_layout

\begin_layout Standard
Therefore,
\begin_inset Formula 
\[
F\left(x\right)=f\left(S\left(x\right)\right).
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
This is depicted in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:A-simple-perceptron."
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Notice
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename perceptron.drawio.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
A simple perceptron.
\begin_inset CommandInset label
LatexCommand label
name "fig:A-simple-perceptron."

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
This way, we can classify the input variables 
\begin_inset Formula $X\in\mathbb{R}^{d}$
\end_inset

 into two classes, 1 or -1.
 But...
 what are the weights?
\end_layout

\begin_layout Example
Imagine we have the following classification function
\begin_inset Formula 
\[
class\left(x\right)=\begin{cases}
1 & if\ x>0.5\\
-1 & otherwise
\end{cases},
\]

\end_inset

 then it is easy to construct a perceptron that can classify the instances.
 We have:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $d=1$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $w_{0}=0.5,w_{1}=1$
\end_inset

.
\end_layout

\begin_layout Itemize
Therefore, we have
\begin_inset Formula 
\[
F\left(X\right)=f\left(w_{0}+w_{1}x\right)=f\left(x-0.5\right)=\begin{cases}
1 & if\ x-0.5>0\\
-1 & otherwise
\end{cases}=\begin{cases}
1 & if\ x>0.5\\
-1 & otherwise
\end{cases}.
\]

\end_inset


\end_layout

\begin_layout Standard
And we are good!
\end_layout

\end_deeper
\begin_layout Standard
In this previous example we can grab some intuition on how the weights are
 chosen, but doing this by hand is not scalable at all.
 When we have several input variables, this process is complicated quite
 a lot.
 Luckily, the 
\series bold
perceptron algorithm
\series default
 helps us estimate the weigths.
\end_layout

\begin_layout Standard
This algorithm is an on-line algorithm, meaning that it process one training
 example at a time, updating 
\begin_inset Formula $W$
\end_inset

 incrementally.
 The training examples are pairs 
\begin_inset Formula $\left(X,y\right)$
\end_inset

, where 
\begin_inset Formula $X\in\mathbb{R}^{d}$
\end_inset

 and 
\begin_inset Formula $y\in\left\{ -1,1\right\} $
\end_inset

.
 Also, we can scale all 
\begin_inset Formula $X$
\end_inset

 to lie in the unit sphere, because this also scales the hyperplanes of
 classification and classes remain the same.
 The algorithm goes as follows:
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset listings
lstparams "language=Python"
inline false
status open

\begin_layout Plain Layout

init W=0
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

for i in range(1,e):
\end_layout

\begin_layout Plain Layout

	for each (X,y):
\end_layout

\begin_layout Plain Layout

	
\end_layout

\begin_layout Plain Layout

		compute prediction=<@$F(S(W^TX))$@>
\end_layout

\begin_layout Plain Layout

	
\end_layout

\begin_layout Plain Layout

		if prediction != y:
\end_layout

\begin_layout Plain Layout

			# update W
\end_layout

\begin_layout Plain Layout

			W = W + lr*X*y
\end_layout

\begin_layout Plain Layout

return W
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Perceptron algorithm (input X, classes y, learning rate lr, epochs e) ->
 weights W
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The updates are made like that because:
\end_layout

\begin_layout Itemize
if 
\begin_inset Formula $y=1$
\end_inset

, then we want 
\begin_inset Formula $S\left(W^{T}X\right)$
\end_inset

 to be > 0, but it is < 0.
 With the update, we now have
\begin_inset Formula 
\[
\left(W+X\right)^{T}X=W^{T}X+X^{T}X=W^{T}X+1>W^{T}X,
\]

\end_inset

 which is one unit closer to our objective.
\end_layout

\begin_layout Itemize
if 
\begin_inset Formula $y=-1$
\end_inset

, the same logic applies.
\end_layout

\begin_layout Example
A simple perceptron in MATLAB
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
matlabtitle{Perceptron Example}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

rng(5)
\end_layout

\begin_layout Plain Layout

X = rand([100,1]);
\end_layout

\begin_layout Plain Layout

y = 2*(double(X>0.5)-0.5*ones(size(X)));
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

f=gscatter(X,zeros(size(X)),y,'br','o');
\end_layout

\begin_layout Plain Layout

hold on
\end_layout

\begin_layout Plain Layout

ylim([-0.1,0.1])
\end_layout

\begin_layout Plain Layout

hold off
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename scripts/perceptron_example_images/figure_0.eps
	scale 60

\end_inset


\end_layout

\end_deeper
\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

Now, let's manually use our perceptron:
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

W = [-0.5; 1]
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

W = 2x1    
\end_layout

\begin_layout Plain Layout

   -0.5000
\end_layout

\begin_layout Plain Layout

    1.0000
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% Add the bias
\end_layout

\begin_layout Plain Layout

X_bias = [ones(size(X)) X];
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

pred = zeros(size(X));
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

for i=1:length(X)
\end_layout

\begin_layout Plain Layout

    x = X_bias(i,:)';
\end_layout

\begin_layout Plain Layout

    S = state(x,W);
\end_layout

\begin_layout Plain Layout

    pred(i) = my_sign(S);
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

f2=gscatter(X,zeros(size(X)),pred,'br','o');
\end_layout

\begin_layout Plain Layout

hold on
\end_layout

\begin_layout Plain Layout

ylim([-0.1,0.1])
\end_layout

\begin_layout Plain Layout

hold off
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename scripts/perceptron_example_images/figure_1.eps
	scale 60

\end_inset


\end_layout

\end_deeper
\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

It classified perfectly! Now let's train the algorithm:
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

% Add the bias
\end_layout

\begin_layout Plain Layout

X_bias = [ones(size(X)) X];
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% Train the model
\end_layout

\begin_layout Plain Layout

W2 = train(X_bias,y,0.1,10)
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

W2 = 2x1    
\end_layout

\begin_layout Plain Layout

   -0.1000
\end_layout

\begin_layout Plain Layout

    0.2013
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

pred2 = zeros(size(X_bias));
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

for i=1:length(X)
\end_layout

\begin_layout Plain Layout

    x = X_bias(i,:)';
\end_layout

\begin_layout Plain Layout

    S = state(x,W2);
\end_layout

\begin_layout Plain Layout

    pred2(i) = my_sign(S);
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

f3=gscatter(X,zeros(size(X)),pred2,'br','o');
\end_layout

\begin_layout Plain Layout

hold on
\end_layout

\begin_layout Plain Layout

ylim([-0.1,0.1])
\end_layout

\begin_layout Plain Layout

hold off
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename scripts/perceptron_example_images/figure_2.eps
	scale 60

\end_inset


\end_layout

\end_deeper
\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

This is quite interesting...
 it found different weights, but it classified eveything well.
 This must break somewhere.
 Let's try to determine where:
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

X = 0.45:0.001:0.55;
\end_layout

\begin_layout Plain Layout

X = X';
\end_layout

\begin_layout Plain Layout

y = 2*(double(X>0.5)-0.5*ones(size(X)));
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

X_bias = [ones(size(X)) X];
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

pred3 = zeros(size(X));
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

for i=1:length(X)
\end_layout

\begin_layout Plain Layout

    x = X_bias(i,:)';
\end_layout

\begin_layout Plain Layout

    S = state(x,W2);
\end_layout

\begin_layout Plain Layout

    pred3(i) = my_sign(S);
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

f4=gscatter(X_bias,zeros(size(X)),pred3==y,'br','o');
\end_layout

\begin_layout Plain Layout

hold on
\end_layout

\begin_layout Plain Layout

ylim([-0.1,0.1])
\end_layout

\begin_layout Plain Layout

xlim([0.45,0.55])
\end_layout

\begin_layout Plain Layout

hold off
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename scripts/perceptron_example_images/figure_3.eps
	scale 60

\end_inset


\end_layout

\end_deeper
\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

y(pred3~=y)
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

ans = 4x1    
\end_layout

\begin_layout Plain Layout

    -1
\end_layout

\begin_layout Plain Layout

    -1
\end_layout

\begin_layout Plain Layout

    -1
\end_layout

\begin_layout Plain Layout

    -1
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

This means that our model is actually setting the bound a little bit before
 0.5 (as can be seen in the graph).
 This should be improved by increasing the size of X.
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

function S = state(X, W)
\end_layout

\begin_layout Plain Layout

    S = W'*X;
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

function pred = my_sign(S)
\end_layout

\begin_layout Plain Layout

    if S > 0
\end_layout

\begin_layout Plain Layout

        pred = 1;
\end_layout

\begin_layout Plain Layout

    else
\end_layout

\begin_layout Plain Layout

        pred = -1;
\end_layout

\begin_layout Plain Layout

    end
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

function W = train(X, y, lr, epochs)
\end_layout

\begin_layout Plain Layout

    [~, d] = size(X);
\end_layout

\begin_layout Plain Layout

    W = zeros(d, 1);
\end_layout

\begin_layout Plain Layout

    for epoch = 1:epochs
\end_layout

\begin_layout Plain Layout

        for i = 1:length(X)
\end_layout

\begin_layout Plain Layout

            x = X(i,:)';
\end_layout

\begin_layout Plain Layout

            pred = my_sign(state(x, W));
\end_layout

\begin_layout Plain Layout

            if pred ~= y(i)
\end_layout

\begin_layout Plain Layout

                W = W + lr*x*y(i);
\end_layout

\begin_layout Plain Layout

            end
\end_layout

\begin_layout Plain Layout

        end
\end_layout

\begin_layout Plain Layout

    end
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\start_of_appendix
Notes on probability theory, Bayes theorem and Bayesian learning
\begin_inset CommandInset label
LatexCommand label
name "sec:Notes-on-probability"

\end_inset


\end_layout

\begin_layout Standard
These notes are adapted from 
\begin_inset CommandInset citation
LatexCommand cite
key "AriasProbability"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Probability theory basic
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\Omega$
\end_inset

 be a sample space, i.e., the set of possible outcomes of an event, and 
\begin_inset Formula $A\subset\Omega$
\end_inset

 an event.
 A probability measure is a function
\begin_inset Formula 
\[
P:\mathcal{P}\left(\Omega\right)\rightarrow\mathbb{R},
\]

\end_inset

 that assigns a real number to every event, 
\begin_inset Formula $P\left(A\right)$
\end_inset

.
 This represents how likely it is that the experiment's outcome is in 
\begin_inset Formula $A$
\end_inset

.
\end_layout

\begin_layout Standard
For 
\begin_inset Formula $\left(\Omega,P\right)$
\end_inset

 to be a probability space
\begin_inset Foot
status open

\begin_layout Plain Layout
In fact, we need one more ingredient, 
\begin_inset Formula $\mathcal{F}$
\end_inset

, which is a 
\begin_inset Formula $\sigma$
\end_inset

-algebra of subsets of 
\begin_inset Formula $\Omega$
\end_inset

.
 This is just a formalization, but we can abuse notation here to simplify
 some things.
 
\end_layout

\end_inset

 we have to impose three axioms:
\end_layout

\begin_layout Enumerate
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
A1)
\end_layout

\end_inset

 
\begin_inset Formula $P\left(A\right)\geq0,\forall A\subset\Omega$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
A2)
\end_layout

\end_inset

 
\begin_inset Formula $P\left(\Omega\right)=1$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
A3)
\end_layout

\end_inset

 
\begin_inset Formula $P\left(A\cup B\right)=P\left(A\right)+P\left(B\right)$
\end_inset

 if 
\begin_inset Formula $A\cap B=\emptyset$
\end_inset

.
\end_layout

\begin_layout Standard
From these axioms, some consequences can be derived:
\end_layout

\begin_layout Itemize
\begin_inset Formula $P\left(\overline{A}\right)\overset{def}{:=}P\left(\Omega\setminus A\right)=1-P\left(A\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Proof
We can write 
\begin_inset Formula $\Omega=A\cup\left(\Omega\setminus A\right)$
\end_inset

 and 
\begin_inset Formula $A\cap\left(\Omega\setminus A\right)=\emptyset$
\end_inset

, so we have
\begin_inset Formula 
\[
1\overset{A2}{=}P\left(\Omega\right)=P\left(A\cup\left(\Omega\setminus A\right)\right)\overset{A3}{=}P\left(A\right)+P\left(\Omega\setminus A\right),
\]

\end_inset

 thus:
\begin_inset Formula 
\[
P\left(\overline{A}\right)=P\left(\Omega\setminus A\right)=1-P\left(A\right).
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $P\left(\emptyset\right)=0$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Proof
\begin_inset Formula $P\left(\emptyset\right)=P\left(\overline{\Omega}\right)=1-P\left(\Omega\right)=0$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
If 
\begin_inset Formula $A\subset B$
\end_inset

, then 
\begin_inset Formula $P\left(A\right)\leq P\left(B\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Proof
In this case, we can write 
\begin_inset Formula $B=A\cup\left(B\setminus A\right)$
\end_inset

, so 
\begin_inset Formula $P\left(B\right)=P\left(A\right)+P\left(B\setminus A\right)$
\end_inset

 and we have the inequality.
\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $P\left(A\cup B\right)=P\left(A\right)+P\left(B\right)-P\left(A\cap B\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Proof
We have 
\begin_inset Formula $A=\left(A\setminus B\right)\cup\left(A\cap B\right)$
\end_inset

, 
\begin_inset Formula $B=\left(B\setminus A\right)\cup\left(A\cap B\right)$
\end_inset

 and 
\begin_inset Formula $A\cup B=\left(A\setminus B\right)\cup\left(A\cap B\right)\cup\left(B\setminus A\right)$
\end_inset

, so
\begin_inset Formula 
\begin{align*}
P\left(A\cup B\right)= & P\left(A\setminus B\right)+P\left(A\cap B\right)+P\left(B\setminus A\right)\\
= & P\left(A\right)-P\left(A\cap B\right)+P\left(A\cap B\right)+P\left(B\right)-P\left(A\cap B\right)\\
= & P\left(A\right)+P\left(B\right)-P\left(A\cap B\right).
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $P\left(A\cup B\right)\leq P\left(A\right)+P\left(B\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Proof
This is obvious from the previous result.
\end_layout

\end_deeper
\begin_layout Subsubsection
Joint probability
\end_layout

\begin_layout Standard
It is usal to be interested in the probability of two events happening simultane
ously.
 This is called the 
\series bold
joint probability
\series default
 of events 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

:
\begin_inset Formula 
\[
P\left(A,B\right)\overset{def}{:=}P\left(A\cap B\right).
\]

\end_inset

 The join probability is useful when an event can be decomposed in simpler
 disjoint events, i.e., we have 
\begin_inset Formula $A\subset B_{1}\cup...\cup B_{n}$
\end_inset

, with 
\begin_inset Formula $B_{i}\cap B_{j}=\emptyset,\forall i\neq j$
\end_inset

.
 In this situation, we can use the 
\series bold
sum rule
\series default
:
\begin_inset Formula 
\[
P\left(A\right)=\sum_{i=1}^{n}P\left(A,B_{i}\right).
\]

\end_inset

 This is also known as 
\series bold
marginalization
\series default
: when we know the joint probability 
\begin_inset Formula $p\left(x,y\right)$
\end_inset

 and we want to compute 
\begin_inset Formula $p\left(x\right)$
\end_inset

, we marginalize out 
\begin_inset Formula $y$
\end_inset

.
 This basically means that if we know the probabilities of all possible
 pairs 
\begin_inset Formula $\left(x,y\right)$
\end_inset

, we can know the probability of 
\begin_inset Formula $x$
\end_inset

 by exploring all the possibilities.
 Here, 'exploring' is using the sum rule:
\begin_inset Formula 
\[
p\left(x\right)=\sum_{y}p\left(x,y\right)
\]

\end_inset

 or
\begin_inset Formula 
\[
p\left(x\right)=\int_{y}p\left(x,y\right)dy,
\]

\end_inset

 if 
\begin_inset Formula $y$
\end_inset

 is continuous.
\end_layout

\begin_layout Example
We have two events:
\end_layout

\begin_layout Itemize
\begin_inset Formula $x$
\end_inset

: earns more than 100k or earns less than 100k.
\end_layout

\begin_layout Itemize
\begin_inset Formula $y$
\end_inset

: is a professor, a software engineer or a data scientist.
\end_layout

\begin_layout Standard
We have some sources of information, and are able to determine that
\begin_inset Formula 
\[
p\left(>100,prof\right)=0.05,\ p\left(>100,seng\right)=0.1,\ p\left(>100,dsci\right)=0.2,
\]

\end_inset

 then we can conclude that
\begin_inset Formula 
\[
p\left(>100\right)=0.05+0.1+0.2=0.35.
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Conditional probability
\end_layout

\begin_layout Standard
The 
\series bold
conditional probability
\series default
 of 
\begin_inset Formula $B$
\end_inset

 given 
\begin_inset Formula $A$
\end_inset

 is the probability that 
\begin_inset Formula $B$
\end_inset

 occurs, knowing that 
\begin_inset Formula $A$
\end_inset

 has occurred.
 This means that we have to restrict the space of possible outcomes, from
 
\begin_inset Formula $\Omega$
\end_inset

 to 
\begin_inset Formula $A$
\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
Note that 
\begin_inset Formula $P\left(A\right)>0$
\end_inset

 is needed.
\end_layout

\end_inset

:
\begin_inset Formula 
\[
P\left(B|A\right)=\frac{P\left(A\cap B\right)}{P\left(A\right)}.
\]

\end_inset

 If we rearrange the terms, we can obtain the 
\series bold
product rule
\series default
:
\begin_inset Formula 
\[
P\left(A,B\right)=P\left(B|A\right)P\left(A\right).
\]

\end_inset

 This formula can be generalized to an arbitrary number of events, the 
\series bold
general product rule
\series default
, given by
\begin_inset Foot
status open

\begin_layout Plain Layout
Note that here it is needed that all the intersections 
\begin_inset Formula $A_{1}\cap...\cap A_{i}$
\end_inset

 have non-zero probability, for 
\begin_inset Formula $i=1,...,n-1$
\end_inset

.
\end_layout

\end_inset


\begin_inset Formula 
\[
P\left(A_{1},...,A_{n}\right)=\prod_{i=1}^{n}P\left(A_{i}|A_{1},...,A_{i-1}\right).
\]

\end_inset


\end_layout

\begin_layout Exercise
Prove that
\begin_inset Formula 
\[
P\left(A,B,C\right)=P\left(A\right)P\left(B|A\right)P\left(C|A,B\right)=P\left(C\right)P\left(B|C\right)P\left(A|B,C\right).
\]

\end_inset


\end_layout

\begin_layout Exercise
Then, prove the general product rule.
\end_layout

\begin_layout Standard
Basically, we are asked to prove the general product rule by induction.
\end_layout

\begin_layout Standard
For 
\begin_inset Formula $n=2$
\end_inset

, we have already proven it.
\end_layout

\begin_layout Standard
For 
\begin_inset Formula $n=3$
\end_inset

, we hav
\begin_inset Formula 
\begin{align*}
P\left(\left(A_{1}\cap A_{2}\right)\cap A_{3}\right)= & P\left(A_{3}|A_{1}\cap A_{2}\right)P\left(A_{1}\cap A_{2}\right)\\
= & P\left(A_{3}|A_{1}\cap A_{2}\right)P\left(A_{2}|A_{1}\right)P\left(A_{1}\right),
\end{align*}

\end_inset

 which is what we wanted.
\end_layout

\begin_layout Standard
Now, assume it is true for 
\begin_inset Formula $n-1$
\end_inset

.
 Then, we have
\begin_inset Formula 
\begin{align*}
P\left(A_{1}\cap...\cap A_{n}\right)= & P\left(\left(A_{1}\cap...\cap A_{n-1}\right)\cap A_{n}\right)\\
= & P\left(A_{n}|A_{1}\cap...\cap A_{n-1}\right)P\left(A_{1}\cap...\cap A_{n-1}\right)\\
\overset{induction}{=} & P\left(A_{n}|A_{1}\cap...\cap A_{n-1}\right)\prod_{i=1}^{n-1}P\left(A_{i}|A_{1},...,A_{i-1}\right)\\
= & \prod_{i=1}^{n}P\left(A_{i}|A_{1},...,A_{i-1}\right).
\end{align*}

\end_inset

 
\end_layout

\begin_layout Subsubsection
Bayes rule
\end_layout

\begin_layout Standard
Bayes theorem gives an alternative formula for the conditional probability:
\begin_inset Formula 
\[
P\left(A|B\right)=\frac{P\left(B|A\right)P\left(A\right)}{P\left(B\right)}.
\]

\end_inset


\end_layout

\begin_layout Proof
Assuming all involved probabilities are non-zero:
\begin_inset Formula 
\[
P\left(A|B\right)=\frac{P\left(A\cap B\right)}{P\left(B\right)}\overset{prod\ rule}{=}\frac{P\left(B|A\right)P\left(A\right)}{P\left(B\right)}.
\]

\end_inset


\end_layout

\begin_layout Standard
This rule is known to be useful to update the probability of an event happening,
 when we are able to gather new information of related events.
 
\begin_inset Formula $P\left(A\right)$
\end_inset

 is usually called the 
\series bold
prior probability
\series default
, and 
\begin_inset Formula $P\left(A|B\right)$
\end_inset

 is the 
\series bold
a posteriori probability
\series default
, which means that we have observed 
\begin_inset Formula $B$
\end_inset

, and want to update the probability estimate for 
\begin_inset Formula $A$
\end_inset

.
\end_layout

\begin_layout Example
Example of the Bayes rule in action
\end_layout

\begin_layout Example
An English-speaking tourist visits a city whose language is not English.
 A local friend tells him that 1 in 10 natives speak English, 1 in 5 people
 in the streets are tourists and that half of the tourists speak English.
 Our visitor stops comeone in the street and finds that this person speaks
 English.
 What is the probability that this person is a tourist?
\end_layout

\begin_layout Example
We have
\begin_inset Formula 
\[
P\left(EN|Tourist\right)=\frac{1}{2},\ P\left(EN|Local\right)=\frac{1}{10},\ P\left(Tourist\right)=\frac{1}{5}
\]

\end_inset


\end_layout

\begin_layout Example
We want to update our knowledge about the event of this person being a tourist.
 The prior probability is 
\begin_inset Formula $\frac{1}{5}$
\end_inset

, but since we know that this person speaks english, we have new information
 useful for updating the probability.
 
\end_layout

\begin_layout Example
First, the total probability of someone speaking english is
\begin_inset Formula 
\[
P\left(EN\right)\overset{sum\ rule+product\ rule}{=}P\left(EN|Tourist\right)P\left(Tourist\right)+P\left(EN|Local\right)P\left(Local\right)=\frac{1}{2}\frac{1}{5}+\frac{1}{10}\frac{4}{5}=\frac{9}{50}.
\]

\end_inset


\end_layout

\begin_layout Example
Now, the a posteriori probability of the person being a tourist, now that
 we know that he speaks english is
\begin_inset Formula 
\[
P\left(tourist|EN\right)=\frac{P\left(EN|Tourist\right)P\left(Tourist\right)}{P\left(EN\right)}=\frac{\frac{1}{2}\frac{1}{5}}{\frac{9}{50}}=\frac{5}{9}.
\]

\end_inset

 As we can see (and as we should expect), knowing that the person speaks
 english, our confidence that he is a tourist increases.
\end_layout

\begin_layout Subsection
Bayes rule in the context of learning
\end_layout

\begin_layout Standard
As have been explained, Bayes rule allows us to reason about hypotheses
 from data:
\begin_inset Formula 
\[
P\left(hypothesis|data\right)=\frac{P\left(data|hypothesis\right)P\left(hypothesis\right)}{P\left(data\right)}.
\]

\end_inset

 In the jargon of parameters and datasets, this is: 
\emph on
let 
\begin_inset Formula $\theta$
\end_inset

 be a random variable with support 
\begin_inset Formula $\Theta$
\end_inset

, and let 
\begin_inset Formula $D$
\end_inset

 be the data that has been observed.
 Then, it is
\begin_inset Formula 
\[
P\left(\theta|D\right)=\frac{P\left(D|\theta\right)P\left(\theta\right)}{P\left(D\right)}=\frac{P\left(D|\theta\right)P\left(\theta\right)}{\int_{\Theta}P\left(D|\theta\right)P\left(\theta\right)d\theta}.
\]

\end_inset

 
\emph default
Here, 
\begin_inset Formula $P\left(\theta\right)$
\end_inset

 is the 
\series bold
prior distribution of 
\begin_inset Formula $\theta$
\end_inset


\series default
.
 This means it is the distribution that we assume, before observing 
\begin_inset Formula $D$
\end_inset

.
 
\begin_inset Formula $P\left(D|\theta\right)$
\end_inset

 is the 
\series bold
likelihood
\series default
 of 
\begin_inset Formula $\theta$
\end_inset

: the probability of observing 
\begin_inset Formula $D$
\end_inset

 if the parameters are 
\begin_inset Formula $\theta$
\end_inset

.
 
\begin_inset Formula $P\left(D\right)$
\end_inset

 is the 
\series bold
evidence
\series default
 or expected likelihood.
 
\begin_inset Formula $P\left(\theta|D\right)$
\end_inset

 is the 
\series bold
posterior distribution of 
\series default

\begin_inset Formula $\theta$
\end_inset

, our quantity of interest, expressing what we know about 
\begin_inset Formula $\theta$
\end_inset

 after having observed 
\begin_inset Formula $D$
\end_inset

.
\end_layout

\begin_layout Standard
Thus, we can continue this line of thought to tackle a new way of creating
 a model: given some data 
\begin_inset Formula $D$
\end_inset

, find the best possible values for the unknown parameters 
\begin_inset Formula $\theta$
\end_inset

, so that the posterior or its likelihood is maximized.
\end_layout

\begin_layout Standard
There are, basically, two different approaches:
\end_layout

\begin_layout Itemize

\series bold
Maximum likelihood
\series default
: in this case, we want to choose 
\begin_inset Formula $\theta$
\end_inset

 in order to maximize its likelihood:
\begin_inset Formula 
\[
\theta_{ML}=\arg\max_{\theta}P\left(D|\theta\right).
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Maximum a posteriori
\series default
: in this case, we take into account a prior distribution for 
\begin_inset Formula $\theta$
\end_inset

 and estimate its value maximizing its posterior distribution:
\begin_inset Formula 
\[
\theta_{MAP}=\arg\max_{\theta}P\left(D|\theta\right)P\left(\theta\right).
\]

\end_inset


\end_layout

\begin_layout Subsection
Maximum likelihood estimation
\end_layout

\begin_layout Standard
Given a sample 
\begin_inset Formula $D=\left\{ x_{1},...,x_{n}\right\} $
\end_inset

, where 
\begin_inset Formula $x_{i}$
\end_inset

 are independent are identically distributed observations from a random
 variable 
\begin_inset Formula $X$
\end_inset

, following a distribution 
\begin_inset Formula $p\left(X;\theta\right)$
\end_inset

, with 
\begin_inset Formula $\theta$
\end_inset

 the paremeters of the distribution.
 Our objective will be to obtain the best values for 
\begin_inset Formula $\theta$
\end_inset

, according to our data, assuming some special form for 
\begin_inset Formula $p$
\end_inset

.
 
\end_layout

\begin_layout Standard
For this, the likelihood can be used: since the 
\begin_inset Formula $x_{i}$
\end_inset

 are independent, the probability of having the sample 
\begin_inset Formula $D$
\end_inset

 is
\begin_inset Formula 
\[
p\left(D;\theta\right)=\prod_{i=1}^{n}p\left(x;\theta\right).
\]

\end_inset

 The 
\series bold
likelihood function
\series default
 is thus defined as
\begin_inset Formula 
\[
L\left(\theta\right)=p\left(D;\theta\right).
\]

\end_inset

 Note that this is done with a fixed data 
\begin_inset Formula $D$
\end_inset

, so it is not a probability distribution, but a function of 
\begin_inset Formula $\theta$
\end_inset

.
 This way, the maximum likelihood estimator for 
\begin_inset Formula $\theta$
\end_inset

 is given by
\begin_inset Formula 
\[
\theta_{ML}=\arg\max_{\theta}L\left(\theta\right).
\]

\end_inset

 There is a numerical issue here, though: as we are multiplying probabilities,
 which are values between 0 and 1, and we are likely to be multiplying many
 of them, we expect to obtain values very close to 0, which can lead to
 underflow in computations.
 Thus, it is convenient to use the 
\series bold
log-likelihood
\series default
:
\begin_inset Formula 
\[
\theta_{ML}=\arg\max_{\theta}L\left(\theta\right)=\arg\max_{\theta}\left[\log L\left(\theta\right)\right]=\arg\min_{\theta}\left[-\log L\left(\theta\right)\right].
\]

\end_inset


\end_layout

\begin_layout Example
Compute the maximum likelihood estimator (MLE) for univariate Gaussian distribut
ion.
 
\end_layout

\begin_layout Example
For this, we assume the data 
\begin_inset Formula $D=\left\{ x_{1},...,x_{n}\right\} $
\end_inset

, where each 
\begin_inset Formula $x_{i}\sim\mathcal{N}\left(\mu,\sigma^{2}\right).$
\end_inset

 In this situation, the parameters are 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 The likelihood function is
\begin_inset Formula 
\begin{align*}
L\left(D;\mu,\sigma^{2}\right)= & \prod_{i=1}^{n}f\left(x;\mu,\sigma^{2}\right)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{1}{2}\frac{\left(x_{i}-\mu\right)^{2}}{\sigma^{2}}}\\
= & \frac{1}{\left(2\pi\sigma^{2}\right)^{\frac{n}{2}}}e^{-\frac{1}{2\sigma^{2}}\sum\left(x_{i}-\mu\right)^{2}}.
\end{align*}

\end_inset

 Now, the log-likelihood is
\begin_inset Formula 
\begin{align*}
logL\left(D;\mu,\sigma^{2}\right)=\log\left(L\left(D;\mu,\sigma^{2}\right)\right)= & \log\left(\frac{1}{\left(2\pi\sigma^{2}\right)^{\frac{n}{2}}}e^{-\frac{1}{2\sigma^{2}}\sum\left(x_{i}-\mu\right)^{2}}\right)\\
= & \log\left(\frac{1}{\left(2\pi\sigma^{2}\right)^{\frac{n}{2}}}\right)+\log\left(e^{-\frac{1}{2\sigma^{2}}\sum\left(x_{i}-\mu\right)^{2}}\right)\\
= & \log\left(1\right)-\log\left(\left(2\pi\sigma^{2}\right)^{\frac{n}{2}}\right)-\frac{1}{2\sigma^{2}}\sum\left(x_{i}-\mu\right)^{2}\log\left(e\right)\\
= & 0-\frac{n}{2}\log\left(2\pi\sigma^{2}\right)-\frac{1}{2\sigma^{2}}\sum\left(x_{i}-\mu\right)^{2}.
\end{align*}

\end_inset

 At this point, to obtain the MLE, we need to maximize this function with
 respect to 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}$
\end_inset

:
\begin_inset Formula 
\[
\frac{\partial}{\partial\mu}logL=-\frac{1}{\sigma^{2}}\sum\left(x_{i}-\mu\right)=0\iff\sum\left(x_{i}-\mu\right)=0\iff\sum x_{i}-n\mu=0\iff\mu_{MLE}=\frac{\sum x_{i}}{n},
\]

\end_inset


\begin_inset Formula 
\[
\frac{\partial}{\partial\sigma^{2}}logL=-\frac{n}{2}\frac{1}{2\pi\sigma^{2}}2\pi+\frac{1}{2\sigma^{4}}\sum\left(x_{i}-\mu\right)^{2}=-\frac{n}{2\sigma^{2}}+\frac{1}{2\sigma^{4}}\sum\left(x_{i}-\mu\right)^{2}=0
\]

\end_inset


\begin_inset Formula 
\[
\iff\frac{1}{2\sigma^{4}}\sum\left(x_{i}-\mu\right)^{2}=\frac{n}{2\sigma^{2}}\iff\sum\left(x_{i}-\mu\right)^{2}=n\sigma^{2}\iff\sigma^{2}=\frac{\sum\left(x_{i}-\mu\right)^{2}}{n}.
\]

\end_inset

 Now, we substitute the value obtained for 
\begin_inset Formula $\mu$
\end_inset

.
\begin_inset Formula 
\[
\sigma_{MLE}^{2}=\frac{\sum\left(x_{i}-\mu_{MLE}\right)^{2}}{n}.
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Example
Compute the MLE for a Bernoulli distribution.
 
\end_layout

\begin_layout Example
Now the observations are the results of 
\begin_inset Formula $n$
\end_inset

 coin tosses.
 We have to compute the parameter 
\begin_inset Formula $p_{ML}$
\end_inset

 of the Bernoulli random variable, whose probability function is given by
 
\begin_inset Formula 
\[
f\left(x\right)=p^{x}\left(1-p\right)^{1-x},\ p\in\left(0,1\right),\ x\in\left\{ 0,1\right\} .
\]

\end_inset

 Now, we have 
\begin_inset Formula $D=\left\{ x_{1},...,x_{n}\right\} \subset\left\{ 0,1\right\} ^{n}$
\end_inset

.
 The likelihood function is
\begin_inset Formula 
\begin{align*}
L\left(D;p\right)= & \prod_{i=1}^{n}f\left(x_{i}\right)=\prod_{i=1}^{n}p^{x_{i}}\left(1-p\right)^{1-x_{i}}=p^{\sum x_{i}}\left(1-p\right)^{n-\sum x_{i}}.
\end{align*}

\end_inset

 We differentiate:
\begin_inset Formula 
\begin{align*}
\frac{\partial}{\partial p}L= & \sum x_{i}p^{\sum x_{i}-1}\left(1-p\right)^{n-\sum x_{i}}-p^{\sum x_{i}}\left[n-\sum x_{i}\right]\left(1-p\right)^{n-\sum x_{i}-1}\\
= & p^{\sum x_{i}-1}\left(1-p\right)^{n-\sum x_{i}-1}\left[\sum x_{i}\left(1-p\right)-p\left(n-\sum x_{i}\right)\right]\\
= & p^{\sum x_{i}-1}\left(1-p\right)^{n-\sum x_{i}-1}\left[\sum x_{i}-\cancel{p\sum x_{i}}-pn+\cancel{p\sum x_{i}}\right]\\
= & p^{\sum x_{i}-1}\left(1-p\right)^{n-\sum x_{i}-1}\left[\sum x_{i}-pn\right].
\end{align*}

\end_inset

 This derivative is zero if and only if
\begin_inset Formula 
\[
\sum x_{i}-pn=0\iff p=\frac{\sum x_{i}}{n}=\overline{x}.
\]

\end_inset


\end_layout

\begin_layout Subsection
Properties of estimators
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tcolorbox}
\end_layout

\end_inset


\end_layout

\begin_layout Definition
If we have a dataset 
\begin_inset Formula $D=\left\{ x_{1},...,x_{n}\right\} $
\end_inset

 where 
\begin_inset Formula $x_{i}\sim X$
\end_inset

 and 
\begin_inset Formula $X$
\end_inset

 is a random variable, we call 
\series bold
estimator
\series default
 a function 
\begin_inset Formula $h:\mathbb{R}^{n}\rightarrow\mathbb{R}^{k}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{tcolorbox}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Usually, we focus on estimators that tell us something about the underlying
 distribution 
\begin_inset Formula $X$
\end_inset

.
 For example, it is usual to assume that 
\begin_inset Formula $X$
\end_inset

 belongs to a certain family of random variables 
\begin_inset Formula $X\in F\left(\theta\right)$
\end_inset

, so that we need to estimate the parameters 
\begin_inset Formula $\theta=\left(\theta_{1},...,\theta_{k}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
There are some properties that are considered desirable for an estimator
 to have:
\end_layout

\begin_layout Itemize

\series bold
Unbiasedness
\series default
: an estimator 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is unbiased if in the long run it takes the value of estimated parameter.
 If we define the 
\series bold
bias
\series default
 of an estimator as 
\begin_inset Formula 
\[
Bias\left[\hat{\theta}\right]=E\left[\hat{\theta}\right]-\theta,
\]

\end_inset

 then, the estimator is unbiased if
\begin_inset Formula 
\[
Bias\left[\hat{\theta}\right]=0.
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Low variance
\series default
: the variance of an estimator tells us how sensitive it is to variations
 in the input data 
\begin_inset Formula $D$
\end_inset

:
\begin_inset Formula 
\[
Var\left[\hat{\theta}\right]=E\left[\left(\hat{\theta}-E\left[\hat{\theta}\right]\right)^{2}\right]=E\left[\hat{\theta}^{2}\right]-E\left[\hat{\theta}\right]^{2}.
\]

\end_inset

 In the case that
\begin_inset Formula 
\[
Var\left[\hat{\theta}_{1}\right]<Var\left[\hat{\theta}_{2}\right],
\]

\end_inset

 we say that 
\begin_inset Formula $\hat{\theta}_{1}$
\end_inset

 is 
\series bold
more efficient
\series default
 than 
\begin_inset Formula $\hat{\theta}_{2}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Standard
The 
\series bold
Cramer-Rao bound
\series default
 gives a theoretical lower bound to the variance of an estimator, under
 some hypotheses that are usually assumed true
\begin_inset Foot
status open

\begin_layout Plain Layout
For further information one could read my notes from a course in Statistics
 (in Spanish), 
\begin_inset CommandInset citation
LatexCommand cite
key "lorencio2021"
literal "false"

\end_inset

.
\end_layout

\end_inset

:
\begin_inset Formula 
\[
Var\left(\hat{\theta}\right)\geq\frac{\left(\frac{\partial E\left[\hat{\theta}\right]}{\partial\theta}\right)^{2}}{E\left[\left(\frac{\partial\ln L}{\partial\theta}\right)^{2}\right]}.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Efficiency
\series default
: an estimator is efficient if:
\end_layout

\begin_deeper
\begin_layout Itemize
It is unbiased.
\end_layout

\begin_layout Itemize
There is no other unbiased estimator with lower variance.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Consistency
\series default
: a sequence of estimators 
\begin_inset Formula $\left\{ \hat{\theta}_{n}\right\} _{n\in\mathbb{N}}$
\end_inset

 is consistent if it converges in probability to the true value of the parameter
 
\begin_inset Formula $\theta$
\end_inset

:
\begin_inset Formula 
\[
\forall\varepsilon>0,\lim_{n\rightarrow\infty}P\left(\left|\theta-\hat{\theta}\right|<\varepsilon\right)=1.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Remark
If the bias and the variance of an estimator tend to 0 with 
\begin_inset Formula $n$
\end_inset

, then it is consistent.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Mean squeared error
\series default
: the mean squared error of an estimator is
\begin_inset Formula 
\[
MSE\left(\hat{\theta}\right)=E\left[\left(\theta-\hat{\theta}\right)^{2}\right],
\]

\end_inset

 which is a value that we seek to minimize.
\end_layout

\begin_layout Example
Show that
\begin_inset Formula 
\[
MSE\left(\hat{\theta}\right)=Bias\left[\hat{\theta}\right]^{2}+Var\left[\hat{\theta}\right].
\]

\end_inset


\end_layout

\begin_layout Example
Let's start from the definition:
\begin_inset Formula 
\begin{align*}
MSE\left(\hat{\theta}\right)=E\left[\left(\theta-\hat{\theta}\right)^{2}\right]= & E\left[\theta^{2}-2\theta\hat{\theta}+\hat{\theta}^{2}\right]=E\left[\theta^{2}\right]-2\theta E\left[\hat{\theta}\right]+E\left[\hat{\theta}^{2}\right]\\
= & \theta^{2}-\theta E\left[\hat{\theta}\right]-\theta E\left[\hat{\theta}\right]+E\left[\hat{\theta}^{2}\right]-{\color{teal}E\left[\hat{\theta}\right]^{2}+E\left[\hat{\theta}\right]^{2}}\\
= & \theta\left({\color{red}\theta-E\left[\hat{\theta}\right]}\right)-\theta E\left[\hat{\theta}\right]+{\color{blue}E\left[\hat{\theta}^{2}\right]-E\left[\hat{\theta}\right]^{2}}+E\left[\hat{\theta}\right]^{2}\\
= & {\color{blue}Var\left[\hat{\theta}\right]}-\theta\cdot{\color{red}Bias\left[\hat{\theta}\right]}+E\left[\hat{\theta}\right]\left({\color{red}E\left[\hat{\theta}\right]-\theta}\right)\\
= & {\color{blue}Var\left[\hat{\theta}\right]}+{\color{red}Bias\left[\hat{\theta}\right]}\left({\color{red}E\left[\hat{\theta}\right]-\theta}\right)\\
= & {\color{blue}Var\left[\hat{\theta}\right]}+{\color{red}Bias\left[\hat{\theta}\right]}^{{\color{red}2}}.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Example
Compute the bias and the variance of the ML estimates 
\begin_inset Formula $\left(\mu,\sigma^{2}\right)$
\end_inset

 of an univariate Gaussian.
 Show that 
\begin_inset Formula $\sigma_{ML}$
\end_inset

 is biased and that we can correct its biasedness by using a different estimator
\begin_inset Formula 
\[
\hat{\sigma}^{2}=\frac{n}{n-1}\sigma_{ML}^{2}.
\]

\end_inset

 Compute the bias and the variance of this new estimator.
\end_layout

\begin_layout Example
Let's start with 
\begin_inset Formula $\mu$
\end_inset

:
\end_layout

\begin_layout Example
\begin_inset Formula 
\begin{align*}
Bias\left(\mu_{MLE}\right)= & Bias\left(\frac{\sum x_{i}}{n}\right)=E\left[\frac{\sum x_{i}}{n}\right]-E\left[X\right]=\frac{1}{n}E\left[\sum x_{i}\right]-E\left[X\right]=\frac{1}{n}\sum E\left[x_{i}\right]-E\left[X\right]\\
= & \frac{1}{n}\sum E\left[X\right]-E\left[X\right]=\frac{n}{n}E\left[X\right]-E\left[X\right]=0,
\end{align*}

\end_inset


\begin_inset Formula 
\begin{align*}
Variance\left(\mu_{MLE}\right)= & E\left[\left(\frac{\sum x_{i}}{n}\right)^{2}\right]-E\left[\frac{\sum x_{i}}{n}\right]^{2}\\
= & \frac{1}{n^{2}}E\left[\left(\sum x_{i}\right)^{2}\right]-\frac{1}{n^{2}}E\left[\sum x_{i}\right]^{2}\\
= & \frac{1}{n^{2}}E\left[\sum x_{i}^{2}+\sum_{i\neq j}x_{i}x_{j}\right]-\frac{1}{n^{2}}\left(\sum E\left[x_{i}\right]\right)^{2}\\
= & \frac{1}{n^{2}}\left(\sum E\left[x_{i}^{2}\right]+\sum_{i\neq j}E\left[x_{i}x_{j}\right]\right)-\frac{1}{n^{2}}\left(\sum E\left[X\right]\right)^{2}\\
= & \frac{1}{n^{2}}\left(\sum E\left[X^{2}\right]+\sum_{i\neq j}E\left[x_{i}\right]E\left[x_{j}\right]\right)-\frac{1}{n^{2}}n^{2}E\left[X\right]^{2}\\
= & \frac{1}{n¬≤}\left(nE\left[X^{2}\right]+\sum_{i\neq j}E\left[X\right]^{2}\right)-E\left[X\right]^{2}\\
= & \frac{E\left[X^{2}\right]+n\left(n-1\right)E\left[X\right]^{2}-n^{2}E\left[X\right]^{2}}{n^{2}}\\
= & \frac{E\left[X^{2}\right]-E\left[X\right]^{2}}{n^{2}}=\frac{Var\left[X\right]}{n^{2}}.
\end{align*}

\end_inset

 Now 
\begin_inset Formula $\sigma_{MLE}^{2}$
\end_inset

:
\begin_inset Formula 
\begin{align*}
Bias\left(\sigma_{MLE}^{2}\right)= & Bias\left(\frac{\sum\left(x_{i}-\mu_{MLE}\right)^{2}}{n}\right)=E\left[\frac{\sum\left(x_{i}-\mu_{MLE}\right)^{2}}{n}\right]-Var\left[X\right]\\
= & \frac{1}{n}E\left[\sum x_{i}^{2}-2\mu_{MLE}\sum x_{i}+n\mu_{MLE}^{2}\right]-Var\left[X\right]\\
= & E\left[X^{2}\right]-\frac{2}{n}E\left[\mu_{MLE}\right]nE\left[X\right]+E\left[\mu_{MLE}^{2}\right]-Var\left[X\right]\\
= & E\left[X^{2}\right]-2E\left[X\right]^{2}+E\left[\left(\frac{\sum x_{i}}{n}\right)^{2}\right]-Var\left[X\right]\\
= & \cancel{E\left[X^{2}\right]-E\left[X\right]^{2}}-E\left[X\right]^{2}+E\left[\left(\frac{\sum x_{i}}{n}\right)^{2}\right]-\cancel{Var\left[X\right]}\\
= & \frac{1}{n^{2}}\left(nE\left[X^{2}\right]+\sum_{i\neq j}E\left[X\right]^{2}\right)-E\left[X\right]^{2}\\
= & \frac{nE\left[X^{2}\right]+n\left(n-1\right)E\left[X\right]^{2}-n^{2}E\left[X\right]^{2}}{n^{2}}\\
= & \frac{nE\left[X^{2}\right]-nE\left[X\right]^{2}}{n^{2}}=\frac{Var\left[X\right]}{n}.
\end{align*}

\end_inset


\begin_inset Formula 
\begin{align*}
Bias\left(\hat{\sigma}^{2}\right)=Bias\left(\frac{n}{n-1}\sigma_{MLE}^{2}\right)= & Bias\left(\frac{\sum\left(x_{i}-\mu_{MLE}\right)^{2}}{n-1}\right)=E\left[\frac{\sum\left(x_{i}-\mu_{MLE}\right)^{2}}{n-1}\right]-Var\left[X\right]\\
= & \frac{1}{n-1}E\left[\sum x_{i}^{2}-2\mu_{MLE}\sum x_{i}+n\mu_{MLE}^{2}\right]-Var\left[X\right]\\
= & \frac{n}{n-1}E\left[X^{2}\right]-\frac{2}{n-1}E\left[\mu_{MLE}\right]nE\left[X\right]+\frac{n}{n-1}E\left[\mu_{MLE}^{2}\right]-Var\left[X\right]\\
= & \frac{n}{n-1}E\left[X^{2}\right]-\frac{2n}{n-1}E\left[X\right]^{2}+\frac{\cancel{n}}{n-1}\frac{1}{n^{\cancel{2}}}\left(nE\left[X^{2}\right]+\sum_{i\neq j}E\left[X\right]^{2}\right)-Var\left[X\right]\\
= & \frac{n^{2}E\left[X^{2}\right]-2n^{2}E\left[X\right]^{2}+nE\left[X^{2}\right]+n\left(n-1\right)E\left[X\right]^{2}-n\left(n-1\right)Var\left[X\right]}{n\left(n-1\right)}\\
= & \frac{{\color{red}n^{2}Var\left[X\right]}-{\color{blue}n^{2}E\left[X\right]^{2}}+{\color{teal}nE\left[X^{2}\right]}{\color{blue}+n^{2}E\left[X\right]^{2}}-{\color{teal}nE\left[X\right]^{2}}-{\color{red}n^{2}Var\left[X\right]}+{\color{teal}nVar\left[X\right]}}{n\left(n-1\right)}\\
= & 0.
\end{align*}

\end_inset

 And we see how this one is, in fact, unbiased.
\end_layout

\begin_layout Subsection
Maximum a posteriori estimation
\end_layout

\begin_layout Standard
MAP (maximum a posteriori) estimation is a method of estimating the parameters
 of a statistical model by finding the parameter values that maximize the
 posterior probability distribution of the parameters, given the observed
 data and a prior probability distribution over the parameters.
 In contexts where the amount of data is limited or noisy, incorporating
 prior knowledge or beliefs can help to produce more stable and accurate
 estimates.
 The prior distribution serves as a regularization term, allowing us to
 control the degree of influence that the prior has on the estimate:
\begin_inset Formula 
\[
\hat{\theta}_{MAP}=\arg\max_{\theta}P\left(\theta|D\right)=\arg\max_{\theta}\frac{P\left(D|\theta\right)P\left(\theta\right)}{P\left(D\right)}=\arg\max_{\theta}P\left(D|\theta\right)P\left(\theta\right),
\]

\end_inset

 where the denominator can be ignored because it is constant for all possible
 
\begin_inset Formula $\theta$
\end_inset

, so it does not change the 
\begin_inset Formula $\arg\max$
\end_inset

.
\end_layout

\begin_layout Example
Find the MAP estimate for 
\begin_inset Formula $\mu$
\end_inset

 of an univariate Gaussian 
\begin_inset Formula $X\sim\mathcal{N}\left(\mu,\sigma^{2}\right)$
\end_inset

 with Gaussian prior distribution for 
\begin_inset Formula $\mu\sim\mathcal{N}\left(\mu_{0},\sigma_{0}^{2}\right)$
\end_inset

, where 
\begin_inset Formula $\sigma,\sigma_{0}$
\end_inset

 and 
\begin_inset Formula $\mu_{0}$
\end_inset

 are assumed to be known.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $D=\left\{ x_{1},...,x_{n}\right\} $
\end_inset

 where 
\begin_inset Formula $x_{i}\sim X$
\end_inset

, then
\begin_inset Formula 
\begin{align*}
P\left(\mu|D\right)= & \frac{P\left(D|\mu\right)P\left(\mu\right)}{P\left(D\right)}=\frac{\prod_{i=1}^{n}P\left(x_{i}|\mu\right)P\left(\mu\right)}{P\left(D\right)}=\frac{\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{1}{2}\frac{\left(x_{i}-\mu\right)^{2}}{\sigma^{2}}}\cdot\frac{1}{\sqrt{2\pi\sigma_{0}^{2}}}e^{-\frac{1}{2}\frac{\left(\mu-\mu_{0}\right)^{2}}{\sigma_{0}^{2}}}}{P\left(D\right)}\\
= & \frac{\frac{1}{\left(2\pi\sigma^{2}\right)^{\frac{n}{2}}}\frac{1}{\sqrt{2\pi\sigma_{0}^{2}}}e^{-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}}\cdot e^{-\frac{1}{2}\frac{\left(\mu-\mu_{0}\right)^{2}}{\sigma_{0}^{2}}}}{P\left(D\right)},
\end{align*}

\end_inset

 so we want to maximize
\begin_inset Formula 
\[
f\left(\mu\right)=e^{-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}}\cdot e^{-\frac{1}{2}\frac{\left(\mu-\mu_{0}\right)^{2}}{\sigma_{0}^{2}}},
\]

\end_inset

 or, its log function
\begin_inset Formula 
\[
g\left(\mu\right)=\log f\left(\mu\right)=-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}-\frac{1}{2}\frac{\left(\mu-\mu_{0}\right)^{2}}{\sigma_{0}^{2}}.
\]

\end_inset

 Thus,
\begin_inset Formula 
\begin{align*}
\frac{d}{d\mu}g\left(\mu\right)= & \frac{1}{2\sigma^{2}}\sum_{i=1}^{n}2\left(x_{i}-\mu\right)-\frac{1}{2}\frac{2\left(\mu-\mu_{0}\right)}{\sigma_{0}^{2}}\\
= & \frac{\sum\left(x_{i}-\mu\right)}{\sigma^{2}}-\frac{\mu-\mu_{0}}{\sigma_{0}^{2}}\\
= & \frac{\sum x_{i}-n\mu}{\sigma^{2}}-\frac{\mu-\mu_{0}}{\sigma_{0}^{2}}\\
= & \frac{\sum x_{i}}{\sigma^{2}}-\frac{n\mu\sigma_{0}^{2}+\sigma^{2}\mu}{\sigma^{2}\sigma_{0}^{2}}+\frac{\mu_{0}}{\sigma_{0}^{2}}\\
= & \frac{\sum x_{i}}{\sigma^{2}}-\mu\frac{n\sigma_{0}^{2}+\sigma^{2}}{\sigma^{2}\sigma_{0}^{2}}+\frac{\mu_{0}}{\sigma_{0}^{2}}
\end{align*}

\end_inset

 and this is zero if and only if
\begin_inset Formula 
\[
\mu_{MAP}=\frac{\sigma^{2}\sigma_{0}^{2}}{\sigma^{2}+n\sigma_{0}^{2}}\left(\frac{\sum x_{i}}{\sigma^{2}}+\frac{\mu_{0}}{\sigma_{0}^{2}}\right)=\frac{\sigma^{2}\sigma_{0}^{2}}{\sigma^{2}+n\sigma_{0}^{2}}\left(\frac{\sigma_{0}^{2}\sum x_{i}+\sigma^{2}\mu_{0}}{\sigma^{2}\sigma_{0}^{2}}\right)=\frac{\sigma_{0}^{2}\sum x_{i}+\sigma^{2}\mu_{0}}{\sigma^{2}+n\sigma_{0}^{2}}.
\]

\end_inset

 A different way to do this is that we have
\begin_inset Formula 
\[
P\left(\mu|D\right)=\frac{\frac{1}{\left(2\pi\sigma^{2}\right)^{\frac{n}{2}}}\frac{1}{\sqrt{2\pi\sigma_{0}^{2}}}e^{-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}}\cdot e^{-\frac{1}{2}\frac{\left(\mu-\mu_{0}\right)^{2}}{\sigma_{0}^{2}}}}{P\left(D\right)}\propto e^{-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}}\cdot e^{-\frac{1}{2}\frac{\left(\mu-\mu_{0}\right)^{2}}{\sigma_{0}^{2}}},
\]

\end_inset

 and we want 
\begin_inset Formula $\left(\mu_{n},\sigma_{n}\right)$
\end_inset

 such that
\begin_inset Formula 
\[
P\left(\mu|D\right)\propto e^{-\frac{\left(\mu-\mu_{n}\right)^{2}}{2\sigma_{n}^{2}}}
\]

\end_inset

 and 
\begin_inset Formula $P\left(\mu|D\right)\sim N\left(\mu_{n},\sigma_{n}^{2}\right)$
\end_inset

, so we can solve
\begin_inset Formula 
\[
e^{-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}}\cdot e^{-\frac{1}{2}\frac{\left(\mu-\mu_{0}\right)^{2}}{\sigma_{0}^{2}}}=e^{-\frac{\left(\mu-\mu_{n}\right)^{2}}{2\sigma_{n}^{2}}},
\]

\end_inset

 obtaining
\begin_inset Formula 
\[
\mu_{n}=\frac{\sigma_{0}^{2}\sum x_{i}+\sigma^{2}\mu_{0}}{\sigma^{2}+n\sigma_{0}^{2}}
\]

\end_inset

and
\begin_inset Formula 
\[
\sigma_{n}=\frac{\sigma^{2}\sigma_{0}^{2}}{\sigma^{2}+n\sigma_{0}^{2}}.
\]

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Example
The maximum likelihood estimate of 
\begin_inset Formula $p$
\end_inset

 for a Bernoulli r.v.
 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\[
p_{ML}=\frac{\sum x_{i}}{n}.
\]

\end_inset

 If we have 
\begin_inset Formula $K>2$
\end_inset

 outcomes, then we have the categorical distribution, also known as multinoulli
 or generalized Bernoulli, which has support 
\begin_inset Formula $\left\{ 1,...,K\right\} $
\end_inset

.
 Its parameters are 
\begin_inset Formula $p=\left(p_{1},...,p_{K}\right)$
\end_inset

, representing the probability of observing each of the possible outcomes,
 with 
\begin_inset Formula $p_{i}\in\left[0,1\right],\forall i$
\end_inset

 and 
\begin_inset Formula $\sum p_{i}=1$
\end_inset

.
\end_layout

\begin_layout Example
It is convenient to use the 
\emph on
one-of-K encoding 
\emph default
(also called one-hot encoding) for each outcome.
 Thus, the pmf of this distribution becomes
\begin_inset Formula 
\[
p\left(x\right)=\prod_{i=1}^{K}p_{i}^{x_{i}}.
\]

\end_inset

 Now, given a sample 
\begin_inset Formula $D=\left\{ x_{1},...,x_{n}\right\} $
\end_inset

 of possible outcomes for a multinoulli r.v.
 
\begin_inset Formula $X$
\end_inset

, the maximum likelihood estimate for 
\begin_inset Formula $p$
\end_inset

 is
\begin_inset Formula 
\[
\hat{p}_{k}=\frac{1}{n}\sum x_{ik},
\]

\end_inset

 for each 
\begin_inset Formula $k\in\left\{ 1,...,K\right\} $
\end_inset

.
 We can write this compactely as
\begin_inset Formula 
\[
\hat{p}=\frac{1}{n}\sum x_{i}.
\]

\end_inset

 If some category 
\begin_inset Formula $k$
\end_inset

 is not present in our sample, then its corresponding ML estimate is going
 to be 0.
 These 0-estimates are problematic in predictive applications, because unseen
 outcomes in the training data are considered to be impossible to happen,
 and thus can never be predicted.
 To avoid this, 
\series bold
pseudocounts
\series default
 are used instead.
 These represent prior knowledge in the form of (imagined) counts 
\begin_inset Formula $c_{k}$
\end_inset

 for each category 
\begin_inset Formula $k$
\end_inset

.
 The idea is to assume that the data is augmented with our pseudocounts,
 and then we estimate using maximum likelihood over the augmented data,
 namely
\begin_inset Formula 
\[
\hat{p}=\frac{c+\sum_{i}x_{i}}{n+\sum_{k}c_{k}},
\]

\end_inset

 so the 
\begin_inset Formula $k$
\end_inset

-th parameter is
\begin_inset Formula 
\[
\hat{p}_{k}=\frac{c_{k}+\sum_{i}x_{ik}}{n+\sum_{k}c_{k}}.
\]

\end_inset

 As an example, imagine that we obtain a sample from a die: 
\begin_inset Formula $\left\{ 1,3,5,4,4,6\right\} $
\end_inset

.
 If the vector of pseudocounts is 
\begin_inset Formula $c=\left(1,1,1,1,1,1\right),$
\end_inset

 then the estimates are 
\begin_inset Formula 
\[
\hat{p}_{1}=\frac{1+1}{6+6}=\frac{1}{6},
\]

\end_inset


\begin_inset Formula 
\[
\hat{p}_{2}=\frac{1}{6+6}=\frac{1}{12},
\]

\end_inset

 and so on.
 Notice that although 2 has not been observed in 
\begin_inset Formula $D$
\end_inset

, its probability estimate is not 0.
 This special case where all pseudocounts are 1 is known as 
\series bold
Laplace smoothing
\series default
.
\end_layout

\begin_layout Example
Prove that using maximum likelihood with pseudocounts corresponds to a MAP
 estimate with Dirichlet prior with parameters 
\begin_inset Formula $\left(c_{1}+1,...,c_{K}+1\right)$
\end_inset

.
\end_layout

\begin_layout Example
A Dirichlet distribution, 
\begin_inset Formula $Dir\left(c_{1}+1,...,c_{K}+1\right)$
\end_inset

 has the density function:
\begin_inset Formula 
\[
f\left(x\right)=\frac{\Gamma\left(\sum\left(c_{i}+1\right)\right)}{\prod\Gamma\left(c_{i}+1\right)}\prod x_{i}^{c_{i}}=\frac{\Gamma\left(\sum c_{i}+K\right)}{\prod c_{i}!}\prod x_{i}^{c_{i}}=\frac{\left(\sum c_{i}+K-1\right)!}{\prod c_{i}!}\prod x_{i}^{c_{i}}.
\]

\end_inset

 To compute the MAP estimate, we use
\begin_inset Formula 
\begin{align*}
P\left(D|p\right)P\left(p\right)= & \prod_{i}P\left(x_{i}|p\right)P\left(p\right)=\prod_{i}\prod_{k}p_{k}^{x_{ik}}\cdot\frac{\left(\sum c_{i}+K-1\right)!}{\prod c_{i}!}\prod_{k}p_{k}^{c_{k}}.
\end{align*}

\end_inset

 Thus, we want to minimize
\begin_inset Formula 
\[
f\left(p_{1},...,p_{K}\right)=\prod_{i,k}p_{k}^{x_{ik}}\prod_{k}p_{k}^{c_{k}},
\]

\end_inset

 or, equivalently, its log
\begin_inset Formula 
\[
g\left(p_{1},...,p_{K}\right)=\log f\left(p_{1},...,p_{K}\right)=\sum_{ik}x_{ik}\log\left(p_{k}\right)+\sum_{k}c_{k}\log\left(p_{k}\right).
\]

\end_inset

 We have
\begin_inset Formula 
\[
\begin{array}{cc}
\min & g\left(p_{1},...,p_{K}\right)\\
s.a. & \sum p_{i}=1\\
 & p_{i}\in\left[0,1\right],\forall i
\end{array}.
\]

\end_inset

 The lagrangian is
\begin_inset Formula 
\[
L=g-\lambda\left(\sum p_{i}-1\right),
\]

\end_inset

 so that
\begin_inset Formula 
\begin{align*}
\frac{\partial}{\partial p_{k}}L=\frac{\partial}{\partial p_{k}}g-\lambda= & \sum_{i}x_{ik}\frac{1}{p_{k}}+c_{k}\frac{1}{p_{k}}-\lambda=0\iff\lambda=\frac{\sum_{i}x_{ik}+c_{k}}{p_{k}}\iff p_{k}=\frac{\sum_{i}x_{ik}+c_{k}}{\lambda},
\end{align*}

\end_inset

 and then
\begin_inset Formula 
\[
1=\sum_{k}p_{k}=\sum_{k}\frac{\sum_{i}x_{ik}+c_{k}}{\lambda}=\frac{1}{\lambda}\left(n+\sum_{k}c_{k}\right)\iff\lambda=n+\sum_{k}c_{k}.
\]

\end_inset

 Finally, substituting back, we obtain
\begin_inset Formula 
\[
p_{k}=\frac{c_{k}+\sum_{i}x_{ik}}{n+\sum_{k}c_{k}},
\]

\end_inset

 as we wanted!
\end_layout

\begin_layout Subsection
Bayesian Learning
\end_layout

\begin_layout Standard
In the Bayesian Learning framework, instead of working with point estimates
 of our unknown parameter variables 
\begin_inset Formula $\theta$
\end_inset

, we work with the whole posterior distribution 
\begin_inset Formula $P\left(\theta|D\right)$
\end_inset

.
 In this case, learning is the process by which starting with some prior
 belief about the parameters and when facing some observations in the form
 of a dataset 
\begin_inset Formula $D$
\end_inset

, we update our belief about the possible values for our parameters in the
 form of the posterior distribution.
 This process is iterated over newly received data, so it can be viewed
 as a sequential process, in which Bayes is invoked each time we need a
 new posterior 
\begin_inset Formula $P\left(\theta|D_{1},D_{2},...\right)$
\end_inset

.
\end_layout

\begin_layout Example
Bayesian learning in Matlab
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
matlabtitle{Bayesian Learning}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

We are going to obtain data from a distribution $
\backslash
mathcal{N}(5,2)$.
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

mu_true = 5;
\end_layout

\begin_layout Plain Layout

sigma = 2;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

num_samples = 10;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

rng('default') % For reproducibility
\end_layout

\begin_layout Plain Layout

data = mu_true + sigma_true * randn(num_samples, 1);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% Set up the x-axis for plotting
\end_layout

\begin_layout Plain Layout

x = linspace(-5, 10, 1000);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% Plot the true distribution
\end_layout

\begin_layout Plain Layout

figure;
\end_layout

\begin_layout Plain Layout

hold on;
\end_layout

\begin_layout Plain Layout

plot(x, normpdf(x, mu_true, sigma_true), 'LineWidth', 2);
\end_layout

\begin_layout Plain Layout

title('Bayesian Learning');
\end_layout

\begin_layout Plain Layout

xlabel('x');
\end_layout

\begin_layout Plain Layout

ylabel('Probability Density');
\end_layout

\begin_layout Plain Layout

xlim([-5, 10]);
\end_layout

\begin_layout Plain Layout

legend('N(5, 2)');
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

Our prior is going to be a standard normal $
\backslash
mu 
\backslash
sim 
\backslash
mathcal{N}(0,1)$ and we are going to assume that $
\backslash
sigma$ is well approximated by the sample variance.
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

mu_prior = 0;
\end_layout

\begin_layout Plain Layout

sigma_prior = 1;
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

Now, we are going to update our beliefs with the obtained datapoints, and
 the formulas for updating are:
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout

$$
\backslash
mu_n =
\backslash
frac{
\backslash
sigma_0^2 
\backslash
sum x_i +
\backslash
sigma^2 
\backslash
mu_0 }{
\backslash
sigma^2 +n
\backslash
sigma_0^2 }$$ 
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

and
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

$
\backslash
sigma_n =
\backslash
frac{
\backslash
sigma^2 
\backslash
sigma_0^2 }{
\backslash
sigma^2 +n
\backslash
sigma_0^2 }$.
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

mu_posterior = (sigma_prior^2*(sum(data))+var(data)*mu_prior)/(var(data)+num_sam
ples*sigma_prior^2)
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

mu_posterior = 2.7734
\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

sigma_posterior = (var(data)*sigma_prior^2)/(var(data)+num_samples*sigma_prior^2
)
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

sigma_posterior = 0.5561
\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

plot(x, normpdf(x, mu_posterior, sqrt(var(data))), 'LineWidth', 2);
\end_layout

\begin_layout Plain Layout

xlabel('x');
\end_layout

\begin_layout Plain Layout

ylabel('Probability Density');
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

sigma_prior = sigma_posterior;
\end_layout

\begin_layout Plain Layout

mu_prior = mu_posterior;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

data = mu_true + sigma_true * randn(num_samples, 1);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

mu_posterior = (sigma_prior^2*(sum(data))+var(data)*mu_prior)/(var(data)+num_sam
ples*sigma_prior^2)
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

mu_posterior = 4.0179
\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

sigma_posterior = (var(data)*sigma_prior^2)/(var(data)+num_samples*sigma_prior^2
)
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

sigma_posterior = 0.2034
\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

plot(x, normpdf(x, mu_posterior, sqrt(var(data))), 'LineWidth', 2);
\end_layout

\begin_layout Plain Layout

xlabel('x');
\end_layout

\begin_layout Plain Layout

ylabel('Probability Density');
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

sigma_prior = sigma_posterior;
\end_layout

\begin_layout Plain Layout

mu_prior = mu_posterior;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

num_samples = 500;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

data = mu_true + sigma_true * randn(num_samples, 1);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

mu_posterior = (sigma_prior^2*(sum(data))+var(data)*mu_prior)/(var(data)+num_sam
ples*sigma_prior^2)
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

mu_posterior = 4.7770
\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

sigma_posterior = (var(data)*sigma_prior^2)/(var(data)+num_samples*sigma_prior^2
)
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

sigma_posterior = 0.0064
\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

plot(x, normpdf(x, mu_posterior, sqrt(var(data))), 'LineWidth', 2);
\end_layout

\begin_layout Plain Layout

xlabel('x');
\end_layout

\begin_layout Plain Layout

ylabel('Probability Density');
\end_layout

\begin_layout Plain Layout

leg = legend('True distribution', 'Learned distribution', 'New learned distribut
ion', 'New new');
\end_layout

\begin_layout Plain Layout

leg.Location = "northwest"
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename scripts/bayesian_learning_images/figure_0.eps
	scale 40

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

leg = 
\end_layout

\begin_layout Plain Layout

  Legend (True distribution, Learned distribution, New learned distribution,
 New new) with properties:
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

         String: {'True distribution'  'Learned distribution'  'New learned
 distribution'  'New new'}
\end_layout

\begin_layout Plain Layout

       Location: 'northwest'
\end_layout

\begin_layout Plain Layout

    Orientation: 'vertical'
\end_layout

\begin_layout Plain Layout

       FontSize: 9
\end_layout

\begin_layout Plain Layout

       Position: [0.1483 0.7446 0.3565 0.1545]
\end_layout

\begin_layout Plain Layout

          Units: 'normalized'
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

  Show all properties
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Subsubsection
Predictive posterior
\end_layout

\begin_layout Standard
When doing prediction in this framework, the whole distribution 
\begin_inset Formula $P\left(\theta|D\right)$
\end_inset

 is used.
 We view a prediction as a weighted average of all predictions each value
 for 
\begin_inset Formula $\theta$
\end_inset

 can make, weighted by its posterior probability (its expected value):
\begin_inset Formula 
\[
P\left(x'|D\right)=E_{\theta|D}\left[x\right]=\int_{\Theta}p\left(x'|\theta,D\right)P\left(\theta|D\right)d\theta.
\]

\end_inset


\end_layout

\begin_layout Example
An insightful example
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
matlabtitle{Bayesian Learning: an insightful example}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

Suppose we have a dataset with X and Y values representing the relationship
 between the number of hours studied and test scores.
 Our true model is $y=2X+1$.
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

X = [1,2,3,4,4.5]';
\end_layout

\begin_layout Plain Layout

y = [3,5,7,9,10]';
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

1)
\backslash
textbf{ Prior beliefs}: To start, we are searching for a linear relationship
 y=a*X+b.
 First, we need to define prior distributions for a and b.
 These distributions represent our initial beliefs about the values of a
 and b before seeing any data.
 A common choice is to use normal distributions with a mean of 0 and a large
 variance (e.g., 1000) to indicate a high level of uncertainty.
 For example, a
\backslash
textasciitilde{}N(0,1000) and b
\backslash
textasciitilde{}N(0,1000).
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

mu_prior = [0;0];
\end_layout

\begin_layout Plain Layout

sigma_prior = 1000;
\end_layout

\begin_layout Plain Layout

V_prior = sigma_prior^2*eye(2) %Covariance matrix for prior distributions
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

V_prior = 2x2    
\end_layout

\begin_layout Plain Layout

     1000000           0
\end_layout

\begin_layout Plain Layout

           0     1000000
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

X_design = [X, ones(size(X))] %Design matrix [X,1]
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

X_design = 5x2    
\end_layout

\begin_layout Plain Layout

    1.0000    1.0000
\end_layout

\begin_layout Plain Layout

    2.0000    1.0000
\end_layout

\begin_layout Plain Layout

    3.0000    1.0000
\end_layout

\begin_layout Plain Layout

    4.0000    1.0000
\end_layout

\begin_layout Plain Layout

    4.5000    1.0000
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

2) 
\backslash
textbf{Likelihood function}: We need to define a likelihood function, which
 describes the probability of the observed data given the parameters a and
 b.
 In linear regression, we typically assume that the errors are normally
 distributed.
 Therefore, the likelihood function can be represented as $y_i 
\backslash
sim 
\backslash
mathcal{N}(aX_i +b,
\backslash
sigma^2 )$, where $
\backslash
sigma^2$ is the variance of the error.
 We will assume it known, because the approaches to get it are a bit involved.
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

sigma_error = 1;
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

3) 
\backslash
textbf{Posterior distribution}: We update our beliefs about the coefficients
 a and b given the data by computing the posterior distribution.
 This is done by multiplying the prior distributions with the likelihood
 function and applying Bayes' theorem:
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

$P(a,b|X,y)=
\backslash
frac{P(y|X,a,b)*P(a,b)}{P(Y|X)}$.
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

Note that the denominator is a normalizing constant that ensures the posterior
 distribution sums to 1.
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout

$$V_{post} =(V_{prior}^{-1} +
\backslash
frac{1}{
\backslash
sigma_{error}^2 }
\backslash
cdot X_{design}^T *X_{design} )^{-1}$$
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout

$$
\backslash
mu_{post} =V_{post} 
\backslash
cdot (V_{prior}^{-1} 
\backslash
cdot 
\backslash
mu_{prior} +
\backslash
frac{1}{
\backslash
sigma_{error}^2 }
\backslash
cdot X_{design}^T 
\backslash
cdot y)$$
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

V_posterior = inv(inv(V_prior) + (1 / sigma_error^2) * (X_design' * X_design))
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

V_posterior = 2x2    
\end_layout

\begin_layout Plain Layout

    0.1220   -0.3537
\end_layout

\begin_layout Plain Layout

   -0.3537    1.2256
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

mu_posterior = V_posterior * (inv(V_prior) * mu_prior + (1 / sigma_error^2)
 * (X_design' * y))
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

mu_posterior = 2x1    
\end_layout

\begin_layout Plain Layout

    2.0000
\end_layout

\begin_layout Plain Layout

    1.0000
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

4)
\backslash
textbf{ Making predictions}: To make predictions for new X values, we can
 use the posterior predictive distribution, which accounts for the uncertainty
 in the estimates of a and b.
 
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

% Number of samples
\end_layout

\begin_layout Plain Layout

n_samples = 1000;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% Sample from the posterior distribution of a and b
\end_layout

\begin_layout Plain Layout

ab_samples = mvnrnd(mu_posterior, V_posterior, n_samples);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% Prediction for new X value
\end_layout

\begin_layout Plain Layout

X_new = 3.5;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% Generate predictions using the samples of a and b
\end_layout

\begin_layout Plain Layout

Y_new_samples = zeros(n_samples, 1);
\end_layout

\begin_layout Plain Layout

for i = 1:n_samples
\end_layout

\begin_layout Plain Layout

    a_sample = ab_samples(i, 1);
\end_layout

\begin_layout Plain Layout

    b_sample = ab_samples(i, 2);
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

    Y_new_mean_sample = [X_new, 1] * [a_sample; b_sample];
\end_layout

\begin_layout Plain Layout

    Y_new_samples(i) = Y_new_mean_sample + sigma_error * randn;
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% Compute summary statistics for the predictions
\end_layout

\begin_layout Plain Layout

Y_new_mean = mean(Y_new_samples);
\end_layout

\begin_layout Plain Layout

Y_new_std = std(Y_new_samples);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

fprintf('Prediction for X = %d: Y = %.2f +/- %.2f
\backslash
n', X_new, Y_new_mean, Y_new_std);
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

Prediction for X = 3.500000e+00: Y = 7.98 +/- 1.13
\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

The true value according to the model is 8, and we have obtained 7.98 +-
 1.13, which is pretty good!
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "upc"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
