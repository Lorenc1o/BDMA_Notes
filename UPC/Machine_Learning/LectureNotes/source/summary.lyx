#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{footmisc}
\usepackage{listings}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
%\usepackage{epstopdf}
\usepackage{matlab}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1,0.99,0.94}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,  
    frame=single,
    tabsize=2
}

\lstset{style=mystyle}
\end_preamble
\use_default_options true
\begin_modules
tcolorbox
customHeadersFooters
theorems-ams-bytype
theorems-sec-bytype
algorithm2e
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "linkcolor=blue, urlcolor=blue, citecolor=blue, pdfstartview={FitH}, unicode=true"
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\boxbgcolor #62a0ea
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 3cm
\rightmargin 2cm
\bottommargin 3cm
\headheight 2cm
\headsep 1cm
\footskip 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
ML-MDS - Machine Learning
\end_layout

\begin_layout Date
Spring 2023
\end_layout

\begin_layout Author
Jose Antonio Lorencio Abril
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../../../Big_Data_Mng/LectureNotes/source/upc-logo.png
	scale 50

\end_inset


\end_layout

\begin_layout Standard
\align right
Professor: Marta Arias
\end_layout

\begin_layout Standard
\align right
Student e-mail: jose.antonio.lorencio@estudiantat..upc.edu
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Address
This is a summary of the course 
\emph on
Machine Learning
\emph default
 taught at the Universitat PolitÃ¨cnica de Catalunya by Professor Marta Arias
 in the academic year 22/23.
 Most of the content of this document is adapted from the course notes by
 Arias, 
\begin_inset CommandInset citation
LatexCommand cite
key "Arias2022"
literal "false"

\end_inset

, so I won't be citing it all the time.
 Other references will be provided when used.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList figure

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList algorithm

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Linear regression
\end_layout

\begin_layout Subsection
Introduction
\end_layout

\begin_layout Standard
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Matlab's-accidents-dataset"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can observe a dataset of the population of different states plotted
 against the number of fatal accidents in each of the states.
 Here, each blue circle corresponds to a row of our data, and the coordinates
 are the 
\begin_inset Formula $\left(population,\#accidents\right)$
\end_inset

 values in the row.
 The red line is the linear regression model of this data.
 This means it is the line that 'best' approximates the data, where best
 refers to minimizing some kind of error: the squared error between each
 point to its projection on the 
\begin_inset Formula $y$
\end_inset

 axis of the line, in this case.
 This approach is called the 
\series bold
least squares method
\series default
.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado2.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Matlab's-accidents-dataset"

\end_inset

Matlab's 
\color blue
accidents
\color inherit
 dataset and best linear fit.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Least squares method
\end_layout

\begin_layout Subsubsection
Least squares in 2D
\end_layout

\begin_layout Standard
In 2D, we have a dataset 
\begin_inset Formula $\left\{ \left(x_{i},y_{i}\right),i=1,...,n\right\} $
\end_inset

 and we want to find the line that best approximates 
\begin_inset Formula $y$
\end_inset

 as a function of 
\begin_inset Formula $x$
\end_inset

.
 As we want a line, we need to specify its slope, 
\begin_inset Formula $\theta_{1}$
\end_inset

, and its intercept, 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 So, our estimations are:
\begin_inset Formula 
\[
\hat{y}\left(x_{i}\right)=\hat{y}_{i}=\theta_{0}+\theta_{1}x_{i}.
\]

\end_inset

 The least squares linear regression method chooses 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\theta_{1}$
\end_inset

 in such a way that the 
\series bold
error function
\series default

\begin_inset Formula 
\[
J\left(\theta_{0},\theta_{1}\right)=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}=\sum_{i=1}^{n}\left(y_{i}-\theta_{0}-\theta_{1}x_{i}\right)^{2}
\]

\end_inset

 is minimized.
 
\end_layout

\begin_layout Standard
Note that this function only depends on the parameters 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\theta_{1}$
\end_inset

, since the data is assumed to be fixed (they are observations).
\end_layout

\begin_layout Standard
To compute them, we just need to find the minimum of 
\begin_inset Formula $J$
\end_inset

, by taking partial derivatives and setting them to 0.
 Let's do this optimization.
 First, we can develop the square:
\begin_inset Formula 
\[
J\left(\theta_{0},\theta_{1}\right)=\sum_{i=1}^{n}y_{i}^{2}+\theta_{0}^{2}+\theta_{1}^{2}x_{i}^{2}-2\theta_{0}y_{i}-2\theta_{1}x_{i}y_{i}+2\theta_{0}\theta_{1}x_{i}.
\]

\end_inset

 Thus:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial J}{\partial\theta_{0}}=\sum_{i=1}^{n}2\theta_{0}-2y_{i}+2\theta_{1}x_{i}=2n\theta_{0}-2\sum_{i=1}^{n}y_{i}+2\theta_{1}\sum_{i=1}^{n}x_{i}
\]

\end_inset

and 
\begin_inset Formula 
\[
\frac{\partial J}{\partial\theta_{1}}=\sum_{i=1}^{n}2\theta_{1}x_{i}^{2}-2x_{i}y_{i}+2\theta_{0}x_{i}=2\theta_{1}\sum_{i=1}^{n}x_{i}^{2}-2\sum_{i=1}^{n}x_{i}y_{i}+2\theta_{0}\sum_{i=1}^{n}x_{i}.
\]

\end_inset

 We have now to solve the system given by
\begin_inset Formula 
\[
\begin{cases}
2n\theta_{0}-2\sum_{i=1}^{n}y_{i}+2\theta_{1}\sum_{i=1}^{n}x_{i}=0\\
2\theta_{1}\sum_{i=1}^{n}x_{i}^{2}-2\sum_{i=1}^{n}x_{i}y_{i}+2\theta_{0}\sum_{i=1}^{n}x_{i}=0
\end{cases}
\]

\end_inset

 which is equivalent to
\begin_inset Formula 
\[
\begin{cases}
n\theta_{0}-\sum_{i=1}^{n}y_{i}+\theta_{1}\sum_{i=1}^{n}x_{i}=0\\
\theta_{1}\sum_{i=1}^{n}x_{i}^{2}-\sum_{i=1}^{n}x_{i}y_{i}+\theta_{0}\sum_{i=1}^{n}x_{i}=0
\end{cases}.
\]

\end_inset

 We can isolate 
\begin_inset Formula $\theta_{0}$
\end_inset

 from the first equation:
\begin_inset Formula 
\[
\theta_{0}=\frac{\sum_{i=1}^{n}y_{i}-\theta_{1}\sum_{i=1}^{n}x_{i}}{n},
\]

\end_inset

 and substitute it in the second one
\begin_inset Formula 
\[
\theta_{1}\sum_{i=1}^{n}x_{i}^{2}-\sum_{i=1}^{n}x_{i}y_{i}+\frac{\sum_{i=1}^{n}y_{i}-\theta_{1}\sum_{i=1}^{n}x_{i}}{n}\sum_{i=1}^{n}x_{i}=0,
\]

\end_inset

 which is equivalent to
\begin_inset Formula 
\[
\theta_{1}\sum_{i=1}^{n}x_{i}^{2}-\sum_{i=1}^{n}x_{i}y_{i}+\frac{\sum_{i=1}^{n}y_{i}\sum_{i=1}^{n}x_{i}-\theta_{1}\left[\sum_{i=1}^{n}x_{i}\right]^{2}}{n}=0
\]

\end_inset

 or
\begin_inset Formula 
\[
\theta_{1}\left[\sum_{i=1}^{n}x_{i}^{2}-\frac{\left[\sum_{i=1}^{n}x_{i}\right]^{2}}{n}\right]-\sum_{i=1}^{n}x_{i}y_{i}+\frac{\sum_{i=1}^{n}y_{i}\sum_{i=1}^{n}x_{i}}{n}=0.
\]

\end_inset

 At this point, we can divide everything by 
\begin_inset Formula $n$
\end_inset

, yielding:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\theta_{1}\left[\frac{\sum_{i=1}^{n}x_{i}^{2}}{n}-\frac{\left[\sum_{i=1}^{n}x_{i}\right]^{2}}{n^{2}}\right]-\frac{\sum_{i=1}^{n}x_{i}y_{i}}{n}+\frac{\sum_{i=1}^{n}y_{i}\sum_{i=1}^{n}x_{i}}{n^{2}}=0.
\]

\end_inset


\end_layout

\begin_layout Standard
If we now assume that the observations are equiprobable, i.e., 
\begin_inset Formula $P\left(x_{i}\right)=\frac{1}{n}$
\end_inset

, and we call 
\begin_inset Formula $X$
\end_inset

 the random variable from which observations 
\begin_inset Formula $x_{i}$
\end_inset

 are obtained and the same for the observations 
\begin_inset Formula $y_{i}$
\end_inset

, obtained from 
\begin_inset Formula $Y$
\end_inset

, then:
\begin_inset Formula 
\[
\frac{\sum_{i=1}^{n}x_{i}^{2}}{n}=E\left[X^{2}\right],\ \frac{\left[\sum_{i=1}^{n}x_{i}\right]^{2}}{n^{2}}=E\left[X\right]^{2},\ \frac{\sum_{i=1}^{n}x_{i}y_{i}}{n}=E\left[XY\right],\ \frac{\sum_{i=1}^{n}y_{i}\sum_{i=1}^{n}x_{i}}{n^{2}}=E\left[X\right]E\left[Y\right].
\]

\end_inset

 This means that the previous equation can be rewritten as:
\begin_inset Formula 
\[
\theta_{1}\left(E\left[X^{2}\right]-E\left[X\right]^{2}\right)-\left(E\left[XY\right]-E\left[X\right]E\left[Y\right]\right)=0\iff\theta_{1}Var\left[X\right]-Cov\left[X,Y\right]=0
\]

\end_inset

 So
\begin_inset Formula 
\[
\theta_{1}=\frac{Cov\left[X,Y\right]}{Var\left[X\right]},
\]

\end_inset


\begin_inset Formula 
\[
\theta_{0}=E\left[Y\right]-\theta_{1}E\left[X\right].
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Least squares regression: multivariate case
\end_layout

\begin_layout Standard
Now, we can assume that we have 
\begin_inset Formula $m$
\end_inset

 independent variables 
\begin_inset Formula $X_{1},...,X_{m}$
\end_inset

 which we want to use to predict the dependent variable 
\begin_inset Formula $Y$
\end_inset

.
 Again, we have 
\begin_inset Formula $n$
\end_inset

 observations of each variable.
 Now, we want to construct an hyperplane in 
\begin_inset Formula $\mathbb{R}^{m+1}$
\end_inset

, whose predictions would be obtained as
\begin_inset Formula 
\[
\hat{y}\left(X_{i}\right)=\theta_{0}+\theta_{1}x_{i1}+...+\theta_{m}x_{im}=\theta_{0}+\sum_{j=1}^{m}\theta_{j}x_{ij}=\sum_{j=0}^{m}\theta_{j}x_{ij},
\]

\end_inset

 where we define 
\begin_inset Formula $x_{i0}=1,$
\end_inset

 for all 
\begin_inset Formula $i$
\end_inset

.
 We can represent
\begin_inset Formula 
\[
X=\left(x_{ij}\right)_{i=1,...,n;\ j=1,...,m},\ Y=\left[\begin{array}{c}
y_{1}\\
\vdots\\
y_{n}
\end{array}\right],\ \theta=\left[\begin{array}{c}
\theta_{0}\\
\vdots\\
\theta_{m}
\end{array}\right],
\]

\end_inset

 so that we can write
\begin_inset Formula 
\[
\hat{Y}=X\theta.
\]

\end_inset

 The error function is defined as in the simple case
\begin_inset Formula 
\[
J\left(\theta\right)=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2},
\]

\end_inset

 but now we can rewrite this as
\begin_inset Formula 
\[
J\left(\theta\right)=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}=\left(Y-\hat{Y}\right)^{T}\left(Y-\hat{Y}\right)=\left(Y-X\theta\right)^{T}\left(Y-X\theta\right).
\]

\end_inset

 Again, to obtain 
\begin_inset Formula $\theta$
\end_inset

 we need to optimize this function using matrix calculus.
\end_layout

\begin_layout Lemma
If 
\begin_inset Formula $A=\left[\begin{array}{ccc}
a_{11} & ... & a_{1m}\\
\vdots & \ddots & \vdots\\
a_{n1} & ... & a_{nm}
\end{array}\right]\in\mathcal{M}_{n\times m}\left(\mathbb{R}\right)$
\end_inset

 , 
\begin_inset Formula $\theta=\left[\begin{array}{c}
\theta_{1}\\
\vdots\\
\theta_{m}
\end{array}\right]\in\mathbb{R}^{m}$
\end_inset

 and 
\begin_inset Formula $B=\left[\begin{array}{ccc}
b_{11} & ... & b_{1n}\\
\vdots & \ddots & \vdots\\
b_{n1} & ... & b_{nn}
\end{array}\right]\in\mathcal{M}_{m\times m}\left(\mathbb{R}\right)$
\end_inset

 is a symmetric matrix, it holds:
\begin_inset Formula 
\[
\frac{\partial A\theta}{\partial\theta}=A,
\]

\end_inset


\begin_inset Formula 
\[
\frac{\partial\theta^{T}A^{T}}{\partial\theta}=A,
\]

\end_inset

 and
\begin_inset Formula 
\[
\frac{\partial\theta^{T}B\theta}{\partial\theta}=2\theta^{T}B^{T}.
\]

\end_inset


\end_layout

\begin_layout Proof
First, notice that 
\begin_inset Formula $A\theta=\left[\begin{array}{c}
\sum_{j=1}^{m}a_{1j}\theta_{j}\\
\vdots\\
\sum_{j=1}^{m}a_{nj}\theta_{j}
\end{array}\right]\in\mathbb{R}^{n},$
\end_inset

so it is
\begin_inset Formula 
\[
\frac{\partial A\theta}{\partial\theta}=\left[\begin{array}{ccc}
\frac{\partial\sum_{j=1}^{m}a_{1j}\theta_{j}}{\partial\theta_{1}} & ... & \frac{\partial\sum_{j=1}^{m}a_{1j}\theta_{j}}{\partial\theta_{m}}\\
\vdots & \ddots & \vdots\\
\frac{\partial\sum_{j=1}^{m}a_{nj}\theta_{j}}{\partial\theta_{1}} & ... & \frac{\partial\sum_{j=1}^{m}a_{nj}\theta_{j}}{\partial\theta_{m}}
\end{array}\right]=\left[\begin{array}{ccc}
a_{11} & ... & a_{1m}\\
\vdots & \ddots & \vdots\\
a_{n1} & ... & a_{nm}
\end{array}\right]=A.
\]

\end_inset

 For the second result, the procedure is the same.
\end_layout

\begin_layout Proof
Lastly, notice that 
\begin_inset Formula $\theta^{T}B\theta=\sum_{k=1}^{m}\sum_{j=1}^{m}\theta_{k}b_{kj}\theta_{j}\in\mathbb{R}$
\end_inset

, so
\begin_inset Formula 
\[
\frac{\partial\theta^{T}B\theta}{\partial\theta}=\left[\begin{array}{ccc}
\frac{\partial\sum_{k=1}^{m}\sum_{j=1}^{m}\theta_{k}b_{kj}\theta_{j}}{\partial\theta_{1}} & ... & \frac{\partial\sum_{k=1}^{m}\sum_{j=1}^{m}\theta_{k}b_{kj}\theta_{j}}{\partial\theta_{m}}\end{array}\right]=\left[\begin{array}{ccc}
2\sum_{j=1}^{m}b_{1j}\theta_{j} & ... & 2\sum_{j=1}^{m}b_{mj}\theta_{j}\end{array}\right]=2\left[B\theta\right]^{T}=2\theta^{T}B^{T}.
\]

\end_inset


\end_layout

\begin_layout Standard
Now, we can proceed and minimize 
\begin_inset Formula $J$
\end_inset

:
\begin_inset Formula 
\begin{align*}
\frac{\partial J\left(\theta\right)}{\partial\theta}= & \frac{\partial\left(Y-X\theta\right)^{T}\left(Y-X\theta\right)}{\theta}\\
= & \frac{\partial}{\partial\theta}\left[Y^{T}Y-Y^{T}X\theta-\theta^{T}X^{T}Y+\theta^{T}X^{T}X\theta\right]\\
= & 0-Y^{T}X-Y^{T}X+2X^{T}X\theta\\
= & -2Y^{T}X+2\theta^{T}X^{T}X,
\end{align*}

\end_inset

 setting this to be 0, we get
\begin_inset Formula 
\[
\theta^{T}X^{T}X=Y^{T}X\iff X^{T}X\theta=X^{T}Y\iff\theta=\left(X^{T}X\right)^{-1}X^{T}Y.
\]

\end_inset

 Thus, the 'best' linear model is given by
\begin_inset Formula 
\[
\theta_{lse}=\left(X^{T}X\right)^{-1}X^{T}Y.
\]

\end_inset

 Once we have this model, if we have an observation of 
\begin_inset Formula $X$
\end_inset

, 
\begin_inset Formula $x'=\left(x'_{1},...,x'_{m}\right)$
\end_inset

 and we want to make a prediction, we compute
\begin_inset Formula 
\[
y'=x'\theta_{lse}.
\]

\end_inset

 The approach that we have followed here is the 
\series bold
optimization
\series default
 view of learning, which basically consists of the steps:
\end_layout

\begin_layout Enumerate
Set up an error function as a function of some parameters.
\end_layout

\begin_layout Enumerate
Optimize this function to find the suitable values for this parameters,
 assuming the data as given.
\end_layout

\begin_layout Enumerate
Use incoming values to make predictions.
\end_layout

\begin_layout Subsubsection
Computation of least squares solution via the singular values decomposition
 (SVD)
\end_layout

\begin_layout Standard
Inverting 
\begin_inset Formula $X^{T}X$
\end_inset

 can entail numerical problems, so the SVD can be used instead.
\end_layout

\begin_layout Standard
\begin_inset Flex Color Box
status open

\begin_layout Theorem
Any matrix 
\begin_inset Formula $A\in\mathbb{R}^{m\times n}$
\end_inset

, 
\begin_inset Formula $m>n$
\end_inset

, can be expressed as
\begin_inset Formula 
\[
A=U\Sigma V^{T},
\]

\end_inset

 where 
\begin_inset Formula $U\in\mathbb{R}^{m\times n}$
\end_inset

 has orthonormal columns 
\begin_inset Formula $\left(U^{T}U=I\right)$
\end_inset

, 
\begin_inset Formula $\Sigma\in\mathbb{R}^{n\times n}$
\end_inset

 is diagonal and contains the singular values in its diagonal and 
\begin_inset Formula $V\in\mathbb{R}^{n\times n}$
\end_inset

 is orthonormal 
\begin_inset Formula $\left(V^{-1}=V^{T}\right)$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $C=A^{T}A\in\mathbb{R}^{n\times n}$
\end_inset

.
 
\begin_inset Formula $C$
\end_inset

 is square, symmetric and positive semidefinite.
 Therefore, 
\begin_inset Formula $C$
\end_inset

 is diagonalizable, so it can be written as
\begin_inset Formula 
\[
C=V\Lambda V^{T},
\]

\end_inset

 where 
\begin_inset Formula $V=\left(v_{i}\right)_{i=1,...,n}$
\end_inset

 is orthogonal and 
\begin_inset Formula $\Lambda=diag\left(\lambda_{1},...,\lambda_{d}\right)$
\end_inset

 with 
\begin_inset Formula $\lambda_{1}\geq...\geq\lambda_{r}>0=\lambda_{r+1}=...=\lambda_{n}$
\end_inset

 and 
\begin_inset Formula $r$
\end_inset

 is 
\begin_inset Formula $rank\left(A\right)\leq n$
\end_inset

.
 
\end_layout

\begin_layout Proof
Now, define 
\begin_inset Formula $\sigma_{i}=\sqrt{\lambda_{i}}$
\end_inset

, and form the matrix
\begin_inset Formula 
\[
\Sigma=\left[\begin{array}{cc}
diag\left(\sigma_{1},...,\sigma_{r}\right) & 0_{r\times\left(n-r\right)}\\
0_{\left(m-r\right)\times r} & 0_{\left(m-r\right)\times\left(n-r\right)}
\end{array}\right].
\]

\end_inset

 Define also
\begin_inset Formula 
\[
u_{i}=\frac{1}{\sigma_{i}}Xv_{i}\in\mathbb{R}^{m},i=1,...,r.
\]

\end_inset

 Then, this vectors are orthonormal:
\begin_inset Formula 
\[
u_{i}^{T}u_{j}=\left(\frac{1}{\sigma_{i}}Xv_{i}\right)^{T}\left(\frac{1}{\sigma_{j}}Xv_{j}\right)=\frac{1}{\sigma_{i}\sigma_{j}}v_{i}^{T}X^{T}Xv_{j}=\frac{1}{\sigma_{i}\sigma_{j}}v_{i}^{T}Cv_{j}\overset{*}{=}\frac{1}{\sigma_{i}\sigma_{j}}v_{i}^{T}\left(\lambda_{j}v_{j}\right)\overset{\left(\lambda_{j}=\sigma_{j}^{2}\right)}{=}\frac{\sigma_{j}}{\sigma_{i}}v_{i}^{T}v_{j}\overset{**}{=}0,
\]

\end_inset

 where 
\begin_inset Formula $\left(*\right)$
\end_inset

 is because 
\begin_inset Formula $V$
\end_inset

 is formed with the eigenvectors of 
\begin_inset Formula $C$
\end_inset

, and 
\begin_inset Formula $\left(**\right)$
\end_inset

 is because 
\begin_inset Formula $V$
\end_inset

 is orthonormal.
\end_layout

\begin_layout Proof
Now, we can complete the base with 
\begin_inset Formula $u_{r+1},...,u_{n}$
\end_inset

 (using Gram-Schmidt) in such a way that
\begin_inset Formula 
\[
U=\left[u_{1},...,u_{r},u_{r+1},...,u_{n}\right]\in\mathbb{R}^{n\times n}
\]

\end_inset

 is column orthonormal.
\end_layout

\begin_layout Proof
Now, if it is the case that 
\begin_inset Formula $XV=U\Sigma$
\end_inset

, then 
\begin_inset Formula 
\[
X=XVV^{T}=U\Sigma V^{T},
\]

\end_inset

 so it is only left to see that indeed this holds.
 Consider two cases:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $1\leq i\leq r:$
\end_inset

 
\begin_inset Formula $Xv_{i}=u_{i}\Sigma$
\end_inset

 by the definition of 
\begin_inset Formula $u_{i}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $r+1\leq i\leq n:$
\end_inset

 It is 
\begin_inset Formula $Xv_{i}=0,$
\end_inset

 because 
\begin_inset Formula $X^{T}Xv_{i}=Cv_{i}=\lambda_{i}v_{i}\overset{i>r}{=}0$
\end_inset

.
 As 
\begin_inset Formula $X,v_{i}\neq0$
\end_inset

, it must be 
\begin_inset Formula $Xv_{i}=0$
\end_inset

.
 On the other side of the equation we also have 0 because 
\begin_inset Formula $u_{i}\Sigma=u_{i}\sigma_{i}=0$
\end_inset

 as 
\begin_inset Formula $i>r$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
This, added to the fact that if 
\begin_inset Formula $X$
\end_inset

 has full rank, 
\begin_inset Formula $X^{T}X$
\end_inset

 is invertible and all its eigenvalues non null, gives us: 
\begin_inset Formula 
\begin{align*}
\theta_{lse}= & \left(X^{T}X\right)^{-1}X^{T}y\\
= & \left(\left(U\Sigma V^{T}\right)U\Sigma V^{T}\right)^{-1}\left(U\Sigma V^{T}\right)^{T}y\\
= & \left(V\Sigma U^{T}U\Sigma V^{T}\right)^{-1}V\Sigma U^{T}y\\
= & \Sigma^{-2}V\Sigma U^{T}y=V\Sigma^{-1}U^{T}y\\
= & V\cdot diag\left(\frac{1}{\sigma_{1}},...,\frac{1}{\sigma_{n}}\right)\cdot U^{T}y.
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection*
Intuitive interpretation
\end_layout

\begin_layout Standard
The intuition behind the SVD is summarized in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SVD-visual."
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Basically, every linear transformation can be decomposed into a rotation,
 a scaling and a simpler transformation (column orthogonal).
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename SVD.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
SVD visual.
\begin_inset CommandInset label
LatexCommand label
name "fig:SVD-visual."

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The intuition behind SVD lies in the idea of finding a low-rank approximation
 of a given matrix.
 The rank of a matrix is the number of linearly independent rows or columns
 it contains.
 A high-rank matrix has many linearly independent rows or columns, which
 makes it complex and difficult to analyze.
 On the other hand, a low-rank matrix has fewer linearly independent rows
 or columns, which makes it simpler and easier to analyze.
\end_layout

\begin_layout Standard
SVD provides a way to find the best possible low-rank approximation of a
 given matrix by decomposing it into three components.
 The left singular vectors represent the 
\series bold
direction of maximum variance
\series default
 in the data, while the right singular vectors represent the 
\series bold
direction of maximum correlation
\series default
 between the variables.
 The singular values represent the 
\series bold
magnitude of the variance or correlation
\series default
 in each direction.
\end_layout

\begin_layout Standard
By truncating the diagonal matrix of singular values to keep only the top-k
 values, we can obtain a low-rank approximation of the original matrix that
 retains most of the important information.
 This is useful for reducing the dimensionality of data, compressing images,
 and solving linear equations, among other applications.
\end_layout

\begin_layout Example
How to use SVD in Python and Matlab.
\end_layout

\begin_layout Example
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset listings
lstparams "language=Python"
inline false
status open

\begin_layout Plain Layout

import numpy as np
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

U, d, Vt = np.linalg.svd(X, full_matrices=False)
\end_layout

\begin_layout Plain Layout

D = np.diag(1/d)
\end_layout

\begin_layout Plain Layout

theta = Vt.T @ D @ U.T @ y
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
SVD in Python.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Example
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset listings
lstparams "language=Matlab"
inline false
status open

\begin_layout Plain Layout

[U, d, V] = svd(X)
\end_layout

\begin_layout Plain Layout

D = diag(diag(1./d))
\end_layout

\begin_layout Plain Layout

theta = V'*D*U'*y
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
SVD in Matlab.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Things that could go wrong when using linear regression
\end_layout

\begin_layout Subsubsection
Our independent variable is not enough
\end_layout

\begin_layout Standard
It is possible that our variable 
\begin_inset Formula $X$
\end_inset

 does not provide enough information to predict 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado7.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The independent variable does not provide enough information.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
The relationship between the variables is not linear (underfitting)
\begin_inset CommandInset label
LatexCommand label
name "subsec:The-relationship-between"

\end_inset


\end_layout

\begin_layout Standard
It is also possible that the variables are related in non-linear ways.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado6.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The variables are not linearly related.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Outliers affect the fit
\end_layout

\begin_layout Standard
In the presence of outliers, the model obtained can be distorted, leading
 to bad results.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado8.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The outlier distort the fit.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Basis Functions
\end_layout

\begin_layout Standard
In order to fix the second problem (Subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:The-relationship-between"
plural "false"
caps "false"
noprefix "false"

\end_inset

), we can make use of basis functions.
 The idea is to apply different transformations to the data, so that we
 can extend the expressive power of our model.
 
\end_layout

\begin_layout Standard
\begin_inset Flex Color Box
status open

\begin_layout Definition
A 
\series bold
feature mapping
\series default
 is a non-linear transformation of the inputs 
\begin_inset Formula $\phi:\mathbb{R}^{m}\rightarrow\mathbb{R}^{k}$
\end_inset

.
\end_layout

\begin_layout Definition
The resulting 
\series bold
predictive function
\series default
 or model is 
\begin_inset Formula $y=\phi\left(x\right)\theta$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Example
For example, we can consider the 
\series bold
polynomial expansion of degree 
\begin_inset Formula $k$
\end_inset


\series default
, which is a commonly used feature mapping that approximates the relationship
 between the independent variable 
\begin_inset Formula $x$
\end_inset

 and the dependent variable 
\begin_inset Formula $y$
\end_inset

 to be polynomial of degree 
\begin_inset Formula $k$
\end_inset

, i.e.:
\begin_inset Formula 
\[
y=\theta_{0}+\theta_{1}x+\theta_{2}x^{2}+...+\theta_{k}x^{k}.
\]

\end_inset

 The feature mapping is 
\begin_inset Formula $\phi\left(x\right)=\left(\begin{array}{ccccc}
1 & x & x^{2} & ... & x^{k}\end{array}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Note that the idea is to transform the data so that the fit is still linear,
 even if the relationship is not.
 Of course, this requires to apply the same transformation whenever we receive
 an input for which we want to make predictions.
 Also, the resulting model is more complex, so 
\series bold
complexity control
\series default
 is necessary to avoid overfitting.
\end_layout

\begin_layout Standard
When we apply 
\begin_inset Formula $\phi$
\end_inset

 to the input matrix 
\begin_inset Formula $X$
\end_inset

, we get a new input matrix, given by
\begin_inset Formula 
\[
\Phi=\left(\begin{array}{c}
\phi\left(x_{1}\right)\\
\phi\left(x_{2}\right)\\
\vdots\\
\phi\left(x_{n}\right)
\end{array}\right)=\left(\begin{array}{cccc}
\phi_{1}\left(x_{1}\right) & \phi_{2}\left(x_{1}\right) & \dots & \phi_{m}\left(x_{1}\right)\\
\phi_{1}\left(x_{2}\right) & \phi_{2}\left(x_{2}\right) & \dots & \phi_{m}\left(x_{2}\right)\\
\vdots & \vdots & \ddots & \vdots\\
\phi_{1}\left(x_{n}\right) & \phi_{2}\left(x_{n}\right) & \dots & \phi_{m}\left(x_{n}\right)
\end{array}\right),
\]

\end_inset

 and we obtain the optimal solution as before:
\begin_inset Formula 
\[
\theta_{min}=\arg\min_{\theta}\left(y-\Phi\theta\right)^{T}\left(y-\Phi\theta\right)=\left(\Phi^{T}\Phi\right)^{-1}\Phi^{T}y.
\]

\end_inset


\end_layout

\begin_layout Example
A MATLAB example
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

% This LaTeX was auto-generated from MATLAB code.
\end_layout

\begin_layout Plain Layout

% To make changes, update the MATLAB code and export to LaTeX again.
\end_layout

\begin_layout Plain Layout


\backslash
sloppy
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
matlabtitle{Example 2.3}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

First, we define the dataset.
 In this case $y=e^x$.
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

x=linspace(1,5,15)';
\end_layout

\begin_layout Plain Layout

y=exp(x);
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

Now, we first see what happens with linear regression:
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

b1 = X
\backslash
y;
\end_layout

\begin_layout Plain Layout

yCalc1 = X*b1;
\end_layout

\begin_layout Plain Layout

figure;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

scatter(x,y,'filled')
\end_layout

\begin_layout Plain Layout

hold on 
\end_layout

\begin_layout Plain Layout

plot(x,yCalc1,'LineWidth',2)
\end_layout

\begin_layout Plain Layout

xlabel('X')
\end_layout

\begin_layout Plain Layout

ylabel('Y')
\end_layout

\begin_layout Plain Layout

title('Linear Regression Relation Between X & Y')
\end_layout

\begin_layout Plain Layout

legend('Data','Linear regression')
\end_layout

\begin_layout Plain Layout

grid on
\end_layout

\begin_layout Plain Layout

hold off
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename scripts/example_2_3_images/figure_0.eps
	scale 50

\end_inset


\end_layout

\end_deeper
\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

As we can see, the model does not fit the data addecuately.
 We can use a feature mapping and repeat the process with the transformed
 input:
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

X = feat_map(x)
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlaboutput}
\end_layout

\begin_layout Plain Layout

X = 15x3    
\end_layout

\begin_layout Plain Layout

    1.0000    1.0000    2.7183
\end_layout

\begin_layout Plain Layout

    1.0000    1.2857    3.6173
\end_layout

\begin_layout Plain Layout

    1.0000    1.5714    4.8135
\end_layout

\begin_layout Plain Layout

    1.0000    1.8571    6.4054
\end_layout

\begin_layout Plain Layout

    1.0000    2.1429    8.5238
\end_layout

\begin_layout Plain Layout

    1.0000    2.4286   11.3427
\end_layout

\begin_layout Plain Layout

    1.0000    2.7143   15.0938
\end_layout

\begin_layout Plain Layout

    1.0000    3.0000   20.0855
\end_layout

\begin_layout Plain Layout

    1.0000    3.2857   26.7281
\end_layout

\begin_layout Plain Layout

    1.0000    3.5714   35.5674
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{matlaboutput}
\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

b2 = X
\backslash
y;
\end_layout

\begin_layout Plain Layout

yCalc2 = X*b2;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

scatter(x,y,'filled')
\end_layout

\begin_layout Plain Layout

hold on 
\end_layout

\begin_layout Plain Layout

plot(x,yCalc2,'LineWidth',2)
\end_layout

\begin_layout Plain Layout

xlabel('X')
\end_layout

\begin_layout Plain Layout

ylabel('Y')
\end_layout

\begin_layout Plain Layout

title('Linear Regression Relation Between exp(X) & Y')
\end_layout

\begin_layout Plain Layout

legend('Data','Linear regression','Location','north')
\end_layout

\begin_layout Plain Layout

grid on
\end_layout

\begin_layout Plain Layout

hold off
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename scripts/example_2_3_images/figure_1.eps
	scale 50

\end_inset


\end_layout

\end_deeper
\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{par}
\end_layout

\begin_layout Plain Layout


\backslash
begin{flushleft}
\end_layout

\begin_layout Plain Layout

As we can see, the model is now perfectly fitting the data!
\end_layout

\begin_layout Plain Layout


\backslash
end{flushleft}
\end_layout

\begin_layout Plain Layout


\backslash
end{par}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{matlabcode}
\end_layout

\begin_layout Plain Layout

function X=feat_map(x)
\end_layout

\begin_layout Plain Layout

    X = [ones(size(x)) x exp(x)];
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout


\backslash
end{matlabcode}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Probabilistic approach
\end_layout

\begin_layout Standard
A review on probability basics in Appendix 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\start_of_appendix
Notes on probability theory, Bayes theorem and Bayesian learning
\end_layout

\begin_layout Standard
These notes are adapted from 
\begin_inset CommandInset citation
LatexCommand cite
key "AriasProbability"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Probability theory basic
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\Omega$
\end_inset

 be a sample space, i.e., the set of possible outcomes of an event, and 
\begin_inset Formula $A\subset\Omega$
\end_inset

 an event.
 A probability measure is a function
\begin_inset Formula 
\[
P:\mathcal{P}\left(\Omega\right)\rightarrow\mathbb{R},
\]

\end_inset

 that assigns a real number to every event, 
\begin_inset Formula $P\left(A\right)$
\end_inset

.
 This represents how likely it is that the experiment's outcome is in 
\begin_inset Formula $A$
\end_inset

.
\end_layout

\begin_layout Standard
For 
\begin_inset Formula $\left(\Omega,P\right)$
\end_inset

 to be a probability space
\begin_inset Foot
status open

\begin_layout Plain Layout
In fact, we need one more ingredient, 
\begin_inset Formula $\mathcal{F}$
\end_inset

, which is a 
\begin_inset Formula $\sigma$
\end_inset

-algebra of subsets of 
\begin_inset Formula $\Omega$
\end_inset

.
 This is just a formalization, but we can abuse notation here to simplify
 some things.
 
\end_layout

\end_inset

 we have to impose three axioms:
\end_layout

\begin_layout Enumerate
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
A1)
\end_layout

\end_inset

 
\begin_inset Formula $P\left(A\right)\geq0,\forall A\subset\Omega$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
A2)
\end_layout

\end_inset

 
\begin_inset Formula $P\left(\Omega\right)=1$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
A3)
\end_layout

\end_inset

 
\begin_inset Formula $P\left(A\cup B\right)=P\left(A\right)+P\left(B\right)$
\end_inset

 if 
\begin_inset Formula $A\cap B=\emptyset$
\end_inset

.
\end_layout

\begin_layout Standard
From these axioms, some consequences can be derived:
\end_layout

\begin_layout Itemize
\begin_inset Formula $P\left(\overline{A}\right)\overset{def}{:=}P\left(\Omega\setminus A\right)=1-P\left(A\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Proof
We can write 
\begin_inset Formula $\Omega=A\cup\left(\Omega\setminus A\right)$
\end_inset

 and 
\begin_inset Formula $A\cap\left(\Omega\setminus A\right)=\emptyset$
\end_inset

, so we have
\begin_inset Formula 
\[
1\overset{A2}{=}P\left(\Omega\right)=P\left(A\cup\left(\Omega\setminus A\right)\right)\overset{A3}{=}P\left(A\right)+P\left(\Omega\setminus A\right),
\]

\end_inset

 thus:
\begin_inset Formula 
\[
P\left(\overline{A}\right)=P\left(\Omega\setminus A\right)=1-P\left(A\right).
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $P\left(\emptyset\right)=0$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Proof
\begin_inset Formula $P\left(\emptyset\right)=P\left(\overline{\Omega}\right)=1-P\left(\Omega\right)=0$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
If 
\begin_inset Formula $A\subset B$
\end_inset

, then 
\begin_inset Formula $P\left(A\right)\leq P\left(B\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Proof
In this case, we can write 
\begin_inset Formula $B=A\cup\left(B\setminus A\right)$
\end_inset

, so 
\begin_inset Formula $P\left(B\right)=P\left(A\right)+P\left(B\setminus A\right)$
\end_inset

 and we have the inequality.
\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $P\left(A\cup B\right)=P\left(A\right)+P\left(B\right)-P\left(A\cap B\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Proof
We have 
\begin_inset Formula $A=\left(A\setminus B\right)\cup\left(A\cap B\right)$
\end_inset

, 
\begin_inset Formula $B=\left(B\setminus A\right)\cup\left(A\cap B\right)$
\end_inset

 and 
\begin_inset Formula $A\cup B=\left(A\setminus B\right)\cup\left(A\cap B\right)\cup\left(B\setminus A\right)$
\end_inset

, so
\begin_inset Formula 
\begin{align*}
P\left(A\cup B\right)= & P\left(A\setminus B\right)+P\left(A\cap B\right)+P\left(B\setminus A\right)\\
= & P\left(A\right)-P\left(A\cap B\right)+P\left(A\cap B\right)+P\left(B\right)-P\left(A\cap B\right)\\
= & P\left(A\right)+P\left(B\right)-P\left(A\cap B\right).
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $P\left(A\cup B\right)\leq P\left(A\right)+P\left(B\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Proof
This is obvious from the previous result.
\end_layout

\end_deeper
\begin_layout Subsubsection
Joint probability
\end_layout

\begin_layout Standard
It is usal to be interested in the probability of two events happening simultane
ously.
 This is called the 
\series bold
joint probability
\series default
 of events 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

:
\begin_inset Formula 
\[
P\left(A,B\right)\overset{def}{:=}P\left(A\cap B\right).
\]

\end_inset

 The join probability is useful when an event can be decomposed in simpler
 disjoint events, i.e., we have 
\begin_inset Formula $A\subset B_{1}\cup...\cup B_{n}$
\end_inset

, with 
\begin_inset Formula $B_{i}\cap B_{j}=\emptyset,\forall i\neq j$
\end_inset

.
 In this situation, we can use the 
\series bold
sum rule
\series default
:
\begin_inset Formula 
\[
P\left(A\right)=\sum_{i=1}^{n}P\left(A,B_{i}\right).
\]

\end_inset

 This is also known as 
\series bold
marginalization
\series default
: when we know the joint probability 
\begin_inset Formula $p\left(x,y\right)$
\end_inset

 and we want to compute 
\begin_inset Formula $p\left(x\right)$
\end_inset

, we marginalize out 
\begin_inset Formula $y$
\end_inset

.
 This basically means that if we know the probabilities of all possible
 pairs 
\begin_inset Formula $\left(x,y\right)$
\end_inset

, we can know the probability of 
\begin_inset Formula $x$
\end_inset

 by exploring all the possibilities.
 Here, 'exploring' is using the sum rule:
\begin_inset Formula 
\[
p\left(x\right)=\sum_{y}p\left(x,y\right)
\]

\end_inset

 or
\begin_inset Formula 
\[
p\left(x\right)=\int_{y}p\left(x,y\right)dy,
\]

\end_inset

 if 
\begin_inset Formula $y$
\end_inset

 is continuous.
\end_layout

\begin_layout Example
We have two events:
\end_layout

\begin_layout Itemize
\begin_inset Formula $x$
\end_inset

: earns more than 100k or earns less than 100k.
\end_layout

\begin_layout Itemize
\begin_inset Formula $y$
\end_inset

: is a professor, a software engineer or a data scientist.
\end_layout

\begin_layout Standard
We have some sources of information, and are able to determine that
\begin_inset Formula 
\[
p\left(>100,prof\right)=0.05,\ p\left(>100,seng\right)=0.1,\ p\left(>100,dsci\right)=0.2,
\]

\end_inset

 then we can conclude that
\begin_inset Formula 
\[
p\left(>100\right)=0.05+0.1+0.2=0.35.
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Conditional probability
\end_layout

\begin_layout Standard
The 
\series bold
conditional probability
\series default
 of 
\begin_inset Formula $B$
\end_inset

 given 
\begin_inset Formula $A$
\end_inset

 is the probability that 
\begin_inset Formula $B$
\end_inset

 occurs, knowing that 
\begin_inset Formula $A$
\end_inset

 has occurred.
 This means that we have to restrict the space of possible outcomes, from
 
\begin_inset Formula $\Omega$
\end_inset

 to 
\begin_inset Formula $A$
\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
Note that 
\begin_inset Formula $P\left(A\right)>0$
\end_inset

 is needed.
\end_layout

\end_inset

:
\begin_inset Formula 
\[
P\left(B|A\right)=\frac{P\left(A\cap B\right)}{P\left(A\right)}.
\]

\end_inset

 If we rearrange the terms, we can obtain the 
\series bold
product rule
\series default
:
\begin_inset Formula 
\[
P\left(A,B\right)=P\left(B|A\right)P\left(A\right).
\]

\end_inset

 This formula can be generalized to an arbitrary number of events, the 
\series bold
general product rule
\series default
, given by
\begin_inset Foot
status open

\begin_layout Plain Layout
Note that here it is needed that all the intersections 
\begin_inset Formula $A_{1}\cap...\cap A_{i}$
\end_inset

 have non-zero probability, for 
\begin_inset Formula $i=1,...,n-1$
\end_inset

.
\end_layout

\end_inset


\begin_inset Formula 
\[
P\left(A_{1},...,A_{n}\right)=\prod_{i=1}^{n}P\left(A_{i}|A_{1},...,A_{i-1}\right).
\]

\end_inset


\end_layout

\begin_layout Exercise
Prove that
\begin_inset Formula 
\[
P\left(A,B,C\right)=P\left(A\right)P\left(B|A\right)P\left(C|A,B\right)=P\left(C\right)P\left(B|C\right)P\left(A|B,C\right).
\]

\end_inset


\end_layout

\begin_layout Exercise
Then, prove the general product rule.
\end_layout

\begin_layout Standard
Basically, we are asked to prove the general product rule by induction.
\end_layout

\begin_layout Standard
For 
\begin_inset Formula $n=2$
\end_inset

, we have already proven it.
\end_layout

\begin_layout Standard
For 
\begin_inset Formula $n=3$
\end_inset

, we hav
\begin_inset Formula 
\begin{align*}
P\left(\left(A_{1}\cap A_{2}\right)\cap A_{3}\right)= & P\left(A_{3}|A_{1}\cap A_{2}\right)P\left(A_{1}\cap A_{2}\right)\\
= & P\left(A_{3}|A_{1}\cap A_{2}\right)P\left(A_{2}|A_{1}\right)P\left(A_{1}\right),
\end{align*}

\end_inset

 which is what we wanted.
\end_layout

\begin_layout Standard
Now, assume it is true for 
\begin_inset Formula $n-1$
\end_inset

.
 Then, we have
\begin_inset Formula 
\begin{align*}
P\left(A_{1}\cap...\cap A_{n}\right)= & P\left(\left(A_{1}\cap...\cap A_{n-1}\right)\cap A_{n}\right)\\
= & P\left(A_{n}|A_{1}\cap...\cap A_{n-1}\right)P\left(A_{1}\cap...\cap A_{n-1}\right)\\
\overset{induction}{=} & P\left(A_{n}|A_{1}\cap...\cap A_{n-1}\right)\prod_{i=1}^{n-1}P\left(A_{i}|A_{1},...,A_{i-1}\right)\\
= & \prod_{i=1}^{n}P\left(A_{i}|A_{1},...,A_{i-1}\right).
\end{align*}

\end_inset

 
\end_layout

\begin_layout Subsubsection
Bayes rule
\end_layout

\begin_layout Standard
Bayes theorem gives an alternative formula for the conditional probability:
\begin_inset Formula 
\[
P\left(A|B\right)=\frac{P\left(B|A\right)P\left(A\right)}{P\left(B\right)}.
\]

\end_inset


\end_layout

\begin_layout Proof
Assuming all involved probabilities are non-zero:
\begin_inset Formula 
\[
P\left(A|B\right)=\frac{P\left(A\cap B\right)}{P\left(B\right)}\overset{prod\ rule}{=}\frac{P\left(B|A\right)P\left(A\right)}{P\left(B\right)}.
\]

\end_inset


\end_layout

\begin_layout Standard
This rule is known to be useful to update the probability of an event happening,
 when we are able to gather new information of related events.
 
\begin_inset Formula $P\left(A\right)$
\end_inset

 is usually called the 
\series bold
prior probability
\series default
, and 
\begin_inset Formula $P\left(A|B\right)$
\end_inset

 is the 
\series bold
a posteriori probability
\series default
, which means that we have observed 
\begin_inset Formula $B$
\end_inset

, and want to update the probability estimate for 
\begin_inset Formula $A$
\end_inset

.
\end_layout

\begin_layout Example
Example of the Bayes rule in action
\end_layout

\begin_layout Example
An English-speaking tourist visits a city whose language is not English.
 A local friend tells him that 1 in 10 natives speak English, 1 in 5 people
 in the streets are tourists and that half of the tourists speak English.
 Our visitor stops comeone in the street and finds that this person speaks
 English.
 What is the probability that this person is a tourist?
\end_layout

\begin_layout Example
We have
\begin_inset Formula 
\[
P\left(EN|Tourist\right)=\frac{1}{2},\ P\left(EN|Local\right)=\frac{1}{10},\ P\left(Tourist\right)=\frac{1}{5}
\]

\end_inset


\end_layout

\begin_layout Example
We want to update our knowledge about the event of this person being a tourist.
 The prior probability is 
\begin_inset Formula $\frac{1}{5}$
\end_inset

, but since we know that this person speaks english, we have new information
 useful for updating the probability.
 
\end_layout

\begin_layout Example
First, the total probability of someone speaking english is
\begin_inset Formula 
\[
P\left(EN\right)\overset{sum\ rule+product\ rule}{=}P\left(EN|Tourist\right)P\left(Tourist\right)+P\left(EN|Local\right)P\left(Local\right)=\frac{1}{2}\frac{1}{5}+\frac{1}{10}\frac{4}{5}=\frac{9}{50}.
\]

\end_inset


\end_layout

\begin_layout Example
Now, the a posteriori probability of the person being a tourist, now that
 we know that he speaks english is
\begin_inset Formula 
\[
P\left(tourist|EN\right)=\frac{P\left(EN|Tourist\right)P\left(Tourist\right)}{P\left(EN\right)}=\frac{\frac{1}{2}\frac{1}{5}}{\frac{9}{50}}=\frac{5}{9}.
\]

\end_inset

 As we can see (and as we should expect), knowing that the person speaks
 english, our confidence that he is a tourist increases.
\end_layout

\begin_layout Subsection
Bayes rule in the context of learning
\end_layout

\begin_layout Standard
As have been explained, Bayes rule allows us to reason about hypotheses
 from data:
\begin_inset Formula 
\[
P\left(hypothesis|data\right)=\frac{P\left(data|hypothesis\right)P\left(hypothesis\right)}{P\left(data\right)}.
\]

\end_inset

 In the jargon of parameters and datasets, this is: 
\emph on
let 
\begin_inset Formula $\theta$
\end_inset

 be a random variable with support 
\begin_inset Formula $\Theta$
\end_inset

, and let 
\begin_inset Formula $D$
\end_inset

 be the data that has been observed.
 Then, it is
\begin_inset Formula 
\[
P\left(\theta|D\right)=\frac{P\left(D|\theta\right)P\left(\theta\right)}{P\left(D\right)}=\frac{P\left(D|\theta\right)P\left(\theta\right)}{\int_{\Theta}P\left(D|\theta\right)P\left(\theta\right)d\theta}.
\]

\end_inset

 
\emph default
Here, 
\begin_inset Formula $P\left(\theta\right)$
\end_inset

 is the 
\series bold
prior distribution of 
\begin_inset Formula $\theta$
\end_inset


\series default
.
 This means it is the distribution that we assume, before observing 
\begin_inset Formula $D$
\end_inset

.
 
\begin_inset Formula $P\left(D|\theta\right)$
\end_inset

 is the 
\series bold
likelihood
\series default
 of 
\begin_inset Formula $\theta$
\end_inset

: the probability of observing 
\begin_inset Formula $D$
\end_inset

 if the parameters are 
\begin_inset Formula $\theta$
\end_inset

.
 
\begin_inset Formula $P\left(D\right)$
\end_inset

 is the 
\series bold
evidence
\series default
 or expected likelihood.
 
\begin_inset Formula $P\left(\theta|D\right)$
\end_inset

 is the 
\series bold
posterior distribution of 
\series default

\begin_inset Formula $\theta$
\end_inset

, our quantity of interest, expressing what we know about 
\begin_inset Formula $\theta$
\end_inset

 after having observed 
\begin_inset Formula $D$
\end_inset

.
\end_layout

\begin_layout Standard
Thus, we can continue this line of thought to tackle a new way of creating
 a model: given some data 
\begin_inset Formula $D$
\end_inset

, find the best possible values for the unknown parameters 
\begin_inset Formula $\theta$
\end_inset

, so that the posterior or its likelihood is maximized.
\end_layout

\begin_layout Standard
There are, basically, two different approaches:
\end_layout

\begin_layout Itemize

\series bold
Maximum likelihood
\series default
: in this case, we want to choose 
\begin_inset Formula $\theta$
\end_inset

 in order to maximize its likelihood:
\begin_inset Formula 
\[
\theta_{ML}=\arg\max_{\theta}P\left(D|\theta\right).
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Maximum a posteriori
\series default
: in this case, we take into account a prior distribution for 
\begin_inset Formula $\theta$
\end_inset

 and estimate its value maximizing its posterior distribution:
\begin_inset Formula 
\[
\theta_{MAP}=\arg\max_{\theta}P\left(D|\theta\right)P\left(\theta\right).
\]

\end_inset


\end_layout

\begin_layout Subsection
Maximum likelihood estimation
\end_layout

\begin_layout Standard
Given a sample 
\begin_inset Formula $D=\left\{ x_{1},...,x_{n}\right\} $
\end_inset

, where 
\begin_inset Formula $x_{i}$
\end_inset

 are independent are identically distributed observations from a random
 variable 
\begin_inset Formula $X$
\end_inset

, following a distribution 
\begin_inset Formula $p\left(X;\theta\right)$
\end_inset

, with 
\begin_inset Formula $\theta$
\end_inset

 the paremeters of the distribution.
 Our objective will be to obtain the best values for 
\begin_inset Formula $\theta$
\end_inset

, according to our data, assuming some special form for 
\begin_inset Formula $p$
\end_inset

.
 
\end_layout

\begin_layout Standard
For this, the likelihood can be used: since the 
\begin_inset Formula $x_{i}$
\end_inset

 are independent, the probability of having the sample 
\begin_inset Formula $D$
\end_inset

 is
\begin_inset Formula 
\[
p\left(D;\theta\right)=\prod_{i=1}^{n}p\left(x;\theta\right).
\]

\end_inset

 The 
\series bold
likelihood function
\series default
 is thus defined as
\begin_inset Formula 
\[
L\left(\theta\right)=p\left(D;\theta\right).
\]

\end_inset

 Note that this is done with a fixed data 
\begin_inset Formula $D$
\end_inset

, so it is not a probability distribution, but a function of 
\begin_inset Formula $\theta$
\end_inset

.
 This way, the maximum likelihood estimator for 
\begin_inset Formula $\theta$
\end_inset

 is given by
\begin_inset Formula 
\[
\theta_{ML}=\arg\max_{\theta}L\left(\theta\right).
\]

\end_inset

 There is a numerical issue here, though: as we are multiplying probabilities,
 which are values between 0 and 1, and we are likely to be multiplying many
 of them, we expect to obtain values very close to 0, which can lead to
 underflow in computations.
 Thus, it is convenient to use the 
\series bold
log-likelihood
\series default
:
\begin_inset Formula 
\[
\theta_{ML}=\arg\max_{\theta}L\left(\theta\right)=\arg\max_{\theta}\left[\log L\left(\theta\right)\right]=\arg\min_{\theta}\left[-\log L\left(\theta\right)\right].
\]

\end_inset


\end_layout

\begin_layout Example
Compute the maximum likelihood estimator (MLE) for univariate Gaussian distribut
ion.
 
\end_layout

\begin_layout Example
For this, we assume the data 
\begin_inset Formula $D=\left\{ x_{1},...,x_{n}\right\} $
\end_inset

, where each 
\begin_inset Formula $x_{i}\sim\mathcal{N}\left(\mu,\sigma^{2}\right).$
\end_inset

 In this situation, the parameters are 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 The likelihood function is
\begin_inset Formula 
\begin{align*}
L\left(D;\mu,\sigma^{2}\right)= & \prod_{i=1}^{n}f\left(x;\mu,\sigma^{2}\right)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{1}{2}\frac{\left(x_{i}-\mu\right)^{2}}{\sigma^{2}}}\\
= & \frac{1}{\left(2\pi\sigma^{2}\right)^{\frac{n}{2}}}e^{-\frac{1}{2\sigma^{2}}\sum\left(x_{i}-\mu\right)^{2}}.
\end{align*}

\end_inset

 Now, the log-likelihood is
\begin_inset Formula 
\begin{align*}
logL\left(D;\mu,\sigma^{2}\right)=\log\left(L\left(D;\mu,\sigma^{2}\right)\right)= & \log\left(\frac{1}{\left(2\pi\sigma^{2}\right)^{\frac{n}{2}}}e^{-\frac{1}{2\sigma^{2}}\sum\left(x_{i}-\mu\right)^{2}}\right)\\
= & \log\left(\frac{1}{\left(2\pi\sigma^{2}\right)^{\frac{n}{2}}}\right)+\log\left(e^{-\frac{1}{2\sigma^{2}}\sum\left(x_{i}-\mu\right)^{2}}\right)\\
= & \log\left(1\right)-\log\left(\left(2\pi\sigma^{2}\right)^{\frac{n}{2}}\right)-\frac{1}{2\sigma^{2}}\sum\left(x_{i}-\mu\right)^{2}\log\left(e\right)\\
= & 0-\frac{n}{2}\log\left(2\pi\sigma^{2}\right)-\frac{1}{2\sigma^{2}}\sum\left(x_{i}-\mu\right)^{2}.
\end{align*}

\end_inset

 At this point, to obtain the MLE, we need to maximize this function with
 respect to 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}$
\end_inset

:
\begin_inset Formula 
\[
\frac{\partial}{\partial\mu}logL=-\frac{1}{\sigma^{2}}\sum\left(x_{i}-\mu\right)=0\iff\sum\left(x_{i}-\mu\right)=0\iff\sum x_{i}-n\mu=0\iff\mu_{MLE}=\frac{\sum x_{i}}{n},
\]

\end_inset


\begin_inset Formula 
\[
\frac{\partial}{\partial\sigma^{2}}logL=-\frac{n}{2}\frac{1}{2\pi\sigma^{2}}2\pi+\frac{1}{2\sigma^{4}}\sum\left(x_{i}-\mu\right)^{2}=-\frac{n}{2\sigma^{2}}+\frac{1}{2\sigma^{4}}\sum\left(x_{i}-\mu\right)^{2}=0
\]

\end_inset


\begin_inset Formula 
\[
\iff\frac{1}{2\sigma^{4}}\sum\left(x_{i}-\mu\right)^{2}=\frac{n}{2\sigma^{2}}\iff\sum\left(x_{i}-\mu\right)^{2}=n\sigma^{2}\iff\sigma^{2}=\frac{\sum\left(x_{i}-\mu\right)^{2}}{n}.
\]

\end_inset

 Now, we substitute the value obtained for 
\begin_inset Formula $\mu$
\end_inset

.
\begin_inset Formula 
\[
\sigma_{MLE}^{2}=\frac{\sum\left(x_{i}-\mu_{MLE}\right)^{2}}{n}.
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Example
Compute the MLE for a Bernoulli distribution.
 
\end_layout

\begin_layout Example
Now the observations are the results of 
\begin_inset Formula $n$
\end_inset

 coin tosses.
 We have to compute the parameter 
\begin_inset Formula $p_{ML}$
\end_inset

 of the Bernoulli random variable, whose probability function is given by
 
\begin_inset Formula 
\[
f\left(x\right)=p^{x}\left(1-p\right)^{1-x},\ p\in\left(0,1\right),\ x\in\left\{ 0,1\right\} .
\]

\end_inset

 Now, we have 
\begin_inset Formula $D=\left\{ x_{1},...,x_{n}\right\} \subset\left\{ 0,1\right\} ^{n}$
\end_inset

.
 The likelihood function is
\begin_inset Formula 
\begin{align*}
L\left(D;p\right)= & \prod_{i=1}^{n}f\left(x_{i}\right)=\prod_{i=1}^{n}p^{x_{i}}\left(1-p\right)^{1-x_{i}}=p^{\sum x_{i}}\left(1-p\right)^{n-\sum x_{i}}.
\end{align*}

\end_inset

 We differentiate:
\begin_inset Formula 
\begin{align*}
\frac{\partial}{\partial p}L= & \sum x_{i}p^{\sum x_{i}-1}\left(1-p\right)^{n-\sum x_{i}}-p^{\sum x_{i}}\left[n-\sum x_{i}\right]\left(1-p\right)^{n-\sum x_{i}-1}\\
= & p^{\sum x_{i}-1}\left(1-p\right)^{n-\sum x_{i}-1}\left[\sum x_{i}\left(1-p\right)-p\left(n-\sum x_{i}\right)\right]\\
= & p^{\sum x_{i}-1}\left(1-p\right)^{n-\sum x_{i}-1}\left[\sum x_{i}-\cancel{p\sum x_{i}}-pn+\cancel{p\sum x_{i}}\right]\\
= & p^{\sum x_{i}-1}\left(1-p\right)^{n-\sum x_{i}-1}\left[\sum x_{i}-pn\right].
\end{align*}

\end_inset

 This derivative is zero if and only if
\begin_inset Formula 
\[
\sum x_{i}-pn=0\iff p=\frac{\sum x_{i}}{n}=\overline{x}.
\]

\end_inset


\end_layout

\begin_layout Subsection
Properties of estimators
\end_layout

\begin_layout Standard
\begin_inset Flex Color Box
status open

\begin_layout Definition
If we have a dataset 
\begin_inset Formula $D=\left\{ x_{1},...,x_{n}\right\} $
\end_inset

 where 
\begin_inset Formula $x_{i}\sim X$
\end_inset

 and 
\begin_inset Formula $X$
\end_inset

 is a random variable, we call 
\series bold
estimator
\series default
 a function 
\begin_inset Formula $h:\mathbb{R}^{n}\rightarrow\mathbb{R}^{k}$
\end_inset

.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Usually, we focus on estimators that tell us something about the underlying
 distribution 
\begin_inset Formula $X$
\end_inset

.
 For example, it is usual to assume that 
\begin_inset Formula $X$
\end_inset

 belongs to a certain family of random variables 
\begin_inset Formula $X\in F\left(\theta\right)$
\end_inset

, so that we need to estimate the parameters 
\begin_inset Formula $\theta=\left(\theta_{1},...,\theta_{k}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
There are some properties that are considered desirable for an estimator
 to have:
\end_layout

\begin_layout Itemize

\series bold
Unbiasedness
\series default
: an estimator 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is unbiased if in the long run it takes the value of estimated parameter.
 If we define the 
\series bold
bias
\series default
 of an estimator as 
\begin_inset Formula 
\[
Bias\left[\hat{\theta}\right]=E\left[\hat{\theta}\right]-\theta,
\]

\end_inset

 then, the estimator is unbiased if
\begin_inset Formula 
\[
Bias\left[\hat{\theta}\right]=0.
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Low variance
\series default
: the variance of an estimator tells us how sensitive it is to variations
 in the input data 
\begin_inset Formula $D$
\end_inset

:
\begin_inset Formula 
\[
Var\left[\hat{\theta}\right]=E\left[\left(\hat{\theta}-E\left[\hat{\theta}\right]\right)^{2}\right]=E\left[\hat{\theta}^{2}\right]-E\left[\hat{\theta}\right]^{2}.
\]

\end_inset

 In the case that
\begin_inset Formula 
\[
Var\left[\hat{\theta}_{1}\right]<Var\left[\hat{\theta}_{2}\right],
\]

\end_inset

 we say that 
\begin_inset Formula $\hat{\theta}_{1}$
\end_inset

 is 
\series bold
more efficient
\series default
 than 
\begin_inset Formula $\hat{\theta}_{2}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Standard
The 
\series bold
Cramer-Rao bound
\series default
 gives a theoretical lower bound to the variance of an estimator, under
 some hypotheses that are usually assumed true
\begin_inset Foot
status open

\begin_layout Plain Layout
For further information one could read my notes from a course in Statistics
 (in Spanish), 
\begin_inset CommandInset citation
LatexCommand cite
key "lorencio2021"
literal "false"

\end_inset

.
\end_layout

\end_inset

:
\begin_inset Formula 
\[
Var\left(\hat{\theta}\right)\geq\frac{\left(\frac{\partial E\left[\hat{\theta}\right]}{\partial\theta}\right)^{2}}{E\left[\left(\frac{\partial\ln L}{\partial\theta}\right)^{2}\right]}.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Efficiency
\series default
: an estimator is efficient if:
\end_layout

\begin_deeper
\begin_layout Itemize
It is unbiased.
\end_layout

\begin_layout Itemize
There is no other unbiased estimator with lower variance.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Consistency
\series default
: a sequence of estimators 
\begin_inset Formula $\left\{ \hat{\theta}_{n}\right\} _{n\in\mathbb{N}}$
\end_inset

 is consistent if it converges in probability to the true value of the parameter
 
\begin_inset Formula $\theta$
\end_inset

:
\begin_inset Formula 
\[
\forall\varepsilon>0,\lim_{n\rightarrow\infty}P\left(\left|\theta-\hat{\theta}\right|<\varepsilon\right)=1.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Remark
If the bias and the variance of an estimator tend to 0 with 
\begin_inset Formula $n$
\end_inset

, then it is consistent.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Mean squeared error
\series default
: the mean squared error of an estimator is
\begin_inset Formula 
\[
MSE\left(\hat{\theta}\right)=E\left[\left(\theta-\hat{\theta}\right)^{2}\right],
\]

\end_inset

 which is a value that we seek to minimize.
\end_layout

\begin_layout Example
Show that
\begin_inset Formula 
\[
MSE\left(\hat{\theta}\right)=Bias\left[\hat{\theta}\right]^{2}+Var\left[\hat{\theta}\right].
\]

\end_inset


\end_layout

\begin_layout Example
Let's start from the definition:
\begin_inset Formula 
\begin{align*}
MSE\left(\hat{\theta}\right)=E\left[\left(\theta-\hat{\theta}\right)^{2}\right]= & E\left[\theta^{2}-2\theta\hat{\theta}+\hat{\theta}^{2}\right]=E\left[\theta^{2}\right]-2\theta E\left[\hat{\theta}\right]+E\left[\hat{\theta}^{2}\right]\\
= & \theta^{2}-\theta E\left[\hat{\theta}\right]-\theta E\left[\hat{\theta}\right]+E\left[\hat{\theta}^{2}\right]-{\color{teal}E\left[\hat{\theta}\right]^{2}+E\left[\hat{\theta}\right]^{2}}\\
= & \theta\left({\color{red}\theta-E\left[\hat{\theta}\right]}\right)-\theta E\left[\hat{\theta}\right]+{\color{blue}E\left[\hat{\theta}^{2}\right]-E\left[\hat{\theta}\right]^{2}}+E\left[\hat{\theta}\right]^{2}\\
= & {\color{blue}Var\left[\hat{\theta}\right]}-\theta\cdot{\color{red}Bias\left[\hat{\theta}\right]}+E\left[\hat{\theta}\right]\left({\color{red}E\left[\hat{\theta}\right]-\theta}\right)\\
= & {\color{blue}Var\left[\hat{\theta}\right]}+{\color{red}Bias\left[\hat{\theta}\right]}\left({\color{red}E\left[\hat{\theta}\right]-\theta}\right)\\
= & {\color{blue}Var\left[\hat{\theta}\right]}+{\color{red}Bias\left[\hat{\theta}\right]}^{{\color{red}2}}.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Example
Compute the bias and the variance of the ML estimates 
\begin_inset Formula $\left(\mu,\sigma^{2}\right)$
\end_inset

 of an univariate Gaussian.
 Show that 
\begin_inset Formula $\sigma_{ML}$
\end_inset

 is biased and that we can correct its biasedness by using a different estimator
\begin_inset Formula 
\[
\hat{\sigma}^{2}=\frac{n}{n-1}\sigma_{ML}^{2}.
\]

\end_inset

 Compute the bias and the variance of this new estimator.
\end_layout

\begin_layout Example
Let's start with 
\begin_inset Formula $\mu$
\end_inset

:
\end_layout

\begin_layout Example
\begin_inset Formula 
\begin{align*}
Bias\left(\mu_{MLE}\right)= & Bias\left(\frac{\sum x_{i}}{n}\right)=E\left[\frac{\sum x_{i}}{n}\right]-E\left[X\right]=\frac{1}{n}E\left[\sum x_{i}\right]-E\left[X\right]=\frac{1}{n}\sum E\left[x_{i}\right]-E\left[X\right]\\
= & \frac{1}{n}\sum E\left[X\right]-E\left[X\right]=\frac{n}{n}E\left[X\right]-E\left[X\right]=0,
\end{align*}

\end_inset


\begin_inset Formula 
\begin{align*}
Variance\left(\mu_{MLE}\right)= & E\left[\left(\frac{\sum x_{i}}{n}\right)^{2}\right]-E\left[\frac{\sum x_{i}}{n}\right]^{2}\\
= & \frac{1}{n^{2}}E\left[\left(\sum x_{i}\right)^{2}\right]-\frac{1}{n^{2}}E\left[\sum x_{i}\right]^{2}\\
= & \frac{1}{n^{2}}E\left[\sum x_{i}^{2}+\sum_{i\neq j}x_{i}x_{j}\right]-\frac{1}{n^{2}}\left(\sum E\left[x_{i}\right]\right)^{2}\\
= & \frac{1}{n^{2}}\left(\sum E\left[x_{i}^{2}\right]+\sum_{i\neq j}E\left[x_{i}x_{j}\right]\right)-\frac{1}{n^{2}}\left(\sum E\left[X\right]\right)^{2}\\
= & \frac{1}{n^{2}}\left(\sum E\left[X^{2}\right]+\sum_{i\neq j}E\left[x_{i}\right]E\left[x_{j}\right]\right)-\frac{1}{n^{2}}n^{2}E\left[X\right]^{2}\\
= & \frac{1}{nÂ²}\left(nE\left[X^{2}\right]+\sum_{i\neq j}E\left[X\right]^{2}\right)-E\left[X\right]^{2}\\
= & \frac{E\left[X^{2}\right]+n\left(n-1\right)E\left[X\right]^{2}-n^{2}E\left[X\right]^{2}}{n^{2}}\\
= & \frac{E\left[X^{2}\right]-E\left[X\right]^{2}}{n^{2}}=\frac{Var\left[X\right]}{n^{2}}.
\end{align*}

\end_inset

 Now 
\begin_inset Formula $\sigma_{MLE}^{2}$
\end_inset

:
\begin_inset Formula 
\begin{align*}
Bias\left(\sigma_{MLE}^{2}\right)= & Bias\left(\frac{\sum\left(x_{i}-\mu_{MLE}\right)^{2}}{n}\right)=E\left[\frac{\sum\left(x_{i}-\mu_{MLE}\right)^{2}}{n}\right]-Var\left[X\right]\\
= & \frac{1}{n}E\left[\sum x_{i}^{2}-2\mu_{MLE}\sum x_{i}+n\mu_{MLE}^{2}\right]-Var\left[X\right]\\
= & E\left[X^{2}\right]-\frac{2}{n}E\left[\mu_{MLE}\right]nE\left[X\right]+E\left[\mu_{MLE}^{2}\right]-Var\left[X\right]\\
= & E\left[X^{2}\right]-2E\left[X\right]^{2}+E\left[\left(\frac{\sum x_{i}}{n}\right)^{2}\right]-Var\left[X\right]\\
= & \cancel{E\left[X^{2}\right]-E\left[X\right]^{2}}-E\left[X\right]^{2}+E\left[\left(\frac{\sum x_{i}}{n}\right)^{2}\right]-\cancel{Var\left[X\right]}\\
= & \frac{1}{n^{2}}\left(nE\left[X^{2}\right]+\sum_{i\neq j}E\left[X\right]^{2}\right)-E\left[X\right]^{2}\\
= & \frac{nE\left[X^{2}\right]+n\left(n-1\right)E\left[X\right]^{2}-n^{2}E\left[X\right]^{2}}{n^{2}}\\
= & \frac{nE\left[X^{2}\right]-nE\left[X\right]^{2}}{n^{2}}=\frac{Var\left[X\right]}{n}.
\end{align*}

\end_inset


\begin_inset Formula 
\begin{align*}
Bias\left(\hat{\sigma}^{2}\right)=Bias\left(\frac{n}{n-1}\sigma_{MLE}^{2}\right)= & Bias\left(\frac{\sum\left(x_{i}-\mu_{MLE}\right)^{2}}{n-1}\right)=E\left[\frac{\sum\left(x_{i}-\mu_{MLE}\right)^{2}}{n-1}\right]-Var\left[X\right]\\
= & \frac{1}{n-1}E\left[\sum x_{i}^{2}-2\mu_{MLE}\sum x_{i}+n\mu_{MLE}^{2}\right]-Var\left[X\right]\\
= & \frac{n}{n-1}E\left[X^{2}\right]-\frac{2}{n-1}E\left[\mu_{MLE}\right]nE\left[X\right]+\frac{n}{n-1}E\left[\mu_{MLE}^{2}\right]-Var\left[X\right]\\
= & \frac{n}{n-1}E\left[X^{2}\right]-\frac{2n}{n-1}E\left[X\right]^{2}+\frac{\cancel{n}}{n-1}\frac{1}{n^{\cancel{2}}}\left(nE\left[X^{2}\right]+\sum_{i\neq j}E\left[X\right]^{2}\right)-Var\left[X\right]\\
= & \frac{n^{2}E\left[X^{2}\right]-2n^{2}E\left[X\right]^{2}+nE\left[X^{2}\right]+n\left(n-1\right)E\left[X\right]^{2}-n\left(n-1\right)Var\left[X\right]}{n\left(n-1\right)}\\
= & \frac{{\color{red}n^{2}Var\left[X\right]}-{\color{blue}n^{2}E\left[X\right]^{2}}+{\color{teal}nE\left[X^{2}\right]}{\color{blue}+n^{2}E\left[X\right]^{2}}-{\color{teal}nE\left[X\right]^{2}}-{\color{red}n^{2}Var\left[X\right]}+{\color{teal}nVar\left[X\right]}}{n\left(n-1\right)}\\
= & 0.
\end{align*}

\end_inset

 And we see how this one is, in fact, unbiased.
\end_layout

\begin_layout Subsection
Maximum a posteriori estimation
\end_layout

\begin_layout Standard
MAP (maximum a posteriori) estimation is a method of estimating the parameters
 of a statistical model by finding the parameter values that maximize the
 posterior probability distribution of the parameters, given the observed
 data and a prior probability distribution over the parameters.
 In contexts where the amount of data is limited or noisy, incorporating
 prior knowledge or beliefs can help to produce more stable and accurate
 estimates.
 The prior distribution serves as a regularization term, allowing us to
 control the degree of influence that the prior has on the estimate:
\begin_inset Formula 
\[
\hat{\theta}_{MAP}=\arg\max_{\theta}P\left(\theta|D\right)=\arg\max_{\theta}\frac{P\left(D|\theta\right)P\left(\theta\right)}{P\left(D\right)}=\arg\max_{\theta}P\left(D|\theta\right)P\left(\theta\right),
\]

\end_inset

 where the denominator can be ignored because it is constant for all possible
 
\begin_inset Formula $\theta$
\end_inset

, so it does not change the 
\begin_inset Formula $\arg\max$
\end_inset

.
\end_layout

\begin_layout Example
Find the MAP estimate for 
\begin_inset Formula $\mu$
\end_inset

 of an univariate Gaussian 
\begin_inset Formula $X\sim\mathcal{N}\left(\mu,\sigma^{2}\right)$
\end_inset

 with Gaussian prior distribution for 
\begin_inset Formula $\mu\sim\mathcal{N}\left(\mu_{0},\sigma_{0}^{2}\right)$
\end_inset

, where 
\begin_inset Formula $\sigma,\sigma_{0}$
\end_inset

 and 
\begin_inset Formula $\mu_{0}$
\end_inset

 are assumed to be known.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $D=\left\{ x_{1},...,x_{n}\right\} $
\end_inset

 where 
\begin_inset Formula $x_{i}\sim X$
\end_inset

, then
\begin_inset Formula 
\begin{align*}
P\left(\mu|D\right)= & \frac{P\left(D|\mu\right)P\left(\mu\right)}{P\left(D\right)}=\frac{\prod_{i=1}^{n}P\left(x_{i}|\mu\right)P\left(\mu\right)}{P\left(D\right)}=\frac{\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{1}{2}\frac{\left(x_{i}-\mu\right)^{2}}{\sigma^{2}}}\cdot\frac{1}{\sqrt{2\pi\sigma_{0}^{2}}}e^{-\frac{1}{2}\frac{\left(\mu-\mu_{0}\right)^{2}}{\sigma_{0}^{2}}}}{P\left(D\right)}\\
= & \frac{\frac{1}{\left(2\pi\sigma^{2}\right)^{\frac{n}{2}}}\frac{1}{\sqrt{2\pi\sigma_{0}^{2}}}e^{-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}}\cdot e^{-\frac{1}{2}\frac{\left(\mu-\mu_{0}\right)^{2}}{\sigma_{0}^{2}}}}{P\left(D\right)},
\end{align*}

\end_inset

 so we want to maximize
\begin_inset Formula 
\[
f\left(\mu\right)=e^{-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}}\cdot e^{-\frac{1}{2}\frac{\left(\mu-\mu_{0}\right)^{2}}{\sigma_{0}^{2}}},
\]

\end_inset

 or, its log function
\begin_inset Formula 
\[
g\left(\mu\right)=\log f\left(\mu\right)=-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}-\frac{1}{2}\frac{\left(\mu-\mu_{0}\right)^{2}}{\sigma_{0}^{2}}.
\]

\end_inset

 Thus,
\begin_inset Formula 
\begin{align*}
\frac{d}{d\mu}g\left(\mu\right)= & \frac{1}{2\sigma^{2}}\sum_{i=1}^{n}2\left(x_{i}-\mu\right)-\frac{1}{2}\frac{2\left(\mu-\mu_{0}\right)}{\sigma_{0}^{2}}\\
= & \frac{\sum\left(x_{i}-\mu\right)}{\sigma^{2}}-\frac{\mu-\mu_{0}}{\sigma_{0}^{2}}\\
= & \frac{\sum x_{i}-n\mu}{\sigma^{2}}-\frac{\mu-\mu_{0}}{\sigma_{0}^{2}}\\
= & \frac{\sum x_{i}}{\sigma^{2}}-\frac{n\mu\sigma_{0}^{2}+\sigma^{2}\mu}{\sigma^{2}\sigma_{0}^{2}}+\frac{\mu_{0}}{\sigma_{0}^{2}}\\
= & \frac{\sum x_{i}}{\sigma^{2}}-\mu\frac{n\sigma_{0}^{2}+\sigma^{2}}{\sigma^{2}\sigma_{0}^{2}}+\frac{\mu_{0}}{\sigma_{0}^{2}}
\end{align*}

\end_inset

 and this is zero if and only if
\begin_inset Formula 
\[
\mu_{MAP}=\frac{\sigma^{2}\sigma_{0}^{2}}{\sigma^{2}+n\sigma_{0}^{2}}\left(\frac{\sum x_{i}}{\sigma^{2}}+\frac{\mu_{0}}{\sigma_{0}^{2}}\right)=\frac{\sigma^{2}\sigma_{0}^{2}}{\sigma^{2}+n\sigma_{0}^{2}}\left(\frac{\sigma_{0}^{2}\sum x_{i}+\sigma^{2}\mu_{0}}{\sigma^{2}\sigma_{0}^{2}}\right)=\frac{\sigma_{0}^{2}\sum x_{i}+\sigma^{2}\mu_{0}}{\sigma^{2}+n\sigma_{0}^{2}}.
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Example
The maximum likelihood estimate of 
\begin_inset Formula $p$
\end_inset

 for a Bernoulli r.v.
 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\[
p_{ML}=\frac{\sum x_{i}}{n}.
\]

\end_inset

 If we have 
\begin_inset Formula $K>2$
\end_inset

 outcomes, then we have the categorical distribution, also known as multinoulli
 or generalized Bernoulli, which has support 
\begin_inset Formula $\left\{ 1,...,K\right\} $
\end_inset

.
 Its parameters are 
\begin_inset Formula $p=\left(p_{1},...,p_{K}\right)$
\end_inset

, representing the probability of observing each of the possible outcomes,
 with 
\begin_inset Formula $p_{i}\in\left[0,1\right],\forall i$
\end_inset

 and 
\begin_inset Formula $\sum p_{i}=1$
\end_inset

.
\end_layout

\begin_layout Example
It is convenient to use the 
\emph on
one-of-K encoding 
\emph default
(also called one-hot encoding) for each outcome.
 Thus, the pmf of this distribution becomes
\begin_inset Formula 
\[
p\left(x\right)=\prod_{i=1}^{K}p_{i}^{x_{i}}.
\]

\end_inset

 Now, given a sample 
\begin_inset Formula $D=\left\{ x_{1},...,x_{n}\right\} $
\end_inset

 of possible outcomes for a multinoulli r.v.
 
\begin_inset Formula $X$
\end_inset

, the maximum likelihood estimate for 
\begin_inset Formula $p$
\end_inset

 is
\begin_inset Formula 
\[
\hat{p}_{k}=\frac{1}{n}\sum x_{ik},
\]

\end_inset

 for each 
\begin_inset Formula $k\in\left\{ 1,...,K\right\} $
\end_inset

.
 We can write this compactely as
\begin_inset Formula 
\[
\hat{p}=\frac{1}{n}\sum x_{i}.
\]

\end_inset

 If some category 
\begin_inset Formula $k$
\end_inset

 is not present in our sample, then its corresponding ML estimate is going
 to be 0.
 These 0-estimates are problematic in predictive applications, because unseen
 outcomes in the training data are considered to be impossible to happen,
 and thus can never be predicted.
 To avoid this, 
\series bold
pseudocounts
\series default
 are used instead.
 These represent prior knowledge in the form of (imagined) counts 
\begin_inset Formula $c_{k}$
\end_inset

 for each category 
\begin_inset Formula $k$
\end_inset

.
 The idea is to assume that the data is augmented with our pseudocounts,
 and then we estimate using maximum likelihood over the augmented data,
 namely
\begin_inset Formula 
\[
\hat{p}=\frac{c+\sum_{i}x_{i}}{n+\sum_{k}c_{k}},
\]

\end_inset

 so the 
\begin_inset Formula $k$
\end_inset

-th parameter is
\begin_inset Formula 
\[
\hat{p}_{k}=\frac{c_{k}+\sum_{i}x_{ik}}{n+\sum_{k}c_{k}}.
\]

\end_inset

 As an example, imagine that we obtain a sample from a die: 
\begin_inset Formula $\left\{ 1,3,5,4,4,6\right\} $
\end_inset

.
 If the vector of pseudocounts is 
\begin_inset Formula $c=\left(1,1,1,1,1,1\right),$
\end_inset

 then the estimates are 
\begin_inset Formula 
\[
\hat{p}_{1}=\frac{1+1}{6+6}=\frac{1}{6},
\]

\end_inset


\begin_inset Formula 
\[
\hat{p}_{2}=\frac{1}{6+6}=\frac{1}{12},
\]

\end_inset

 and so on.
 Notice that although 2 has not been observed in 
\begin_inset Formula $D$
\end_inset

, its probability estimate is not 0.
 This special case where all pseudocounts are 1 is known as 
\series bold
Laplace smoothing
\series default
.
\end_layout

\begin_layout Example
Prove that using maximum likelihood with pseudocounts corresponds to a MAP
 estimate with Dirichlet prior with parameters 
\begin_inset Formula $\left(c_{1}+1,...,c_{K}+1\right)$
\end_inset

.
\end_layout

\begin_layout Example
A Dirichlet distribution, 
\begin_inset Formula $Dir\left(c_{1}+1,...,c_{K}+1\right)$
\end_inset

 has the density function:
\begin_inset Formula 
\[
f\left(x\right)=\frac{\Gamma\left(\sum\left(c_{i}+1\right)\right)}{\prod\Gamma\left(c_{i}+1\right)}\prod x_{i}^{c_{i}}=\frac{\Gamma\left(\sum c_{i}+K\right)}{\prod c_{i}!}\prod x_{i}^{c_{i}}=\frac{\left(\sum c_{i}+K-1\right)!}{\prod c_{i}!}\prod x_{i}^{c_{i}}.
\]

\end_inset

 To compute the MAP estimate, we use
\begin_inset Formula 
\begin{align*}
P\left(D|p\right)P\left(p\right)= & \prod_{i}P\left(x_{i}|p\right)P\left(p\right)=\prod_{i}\prod_{k}p_{k}^{x_{ik}}\cdot\frac{\left(\sum c_{i}+K-1\right)!}{\prod c_{i}!}\prod_{k}p_{k}^{c_{k}}.
\end{align*}

\end_inset

 Thus, we want to minimize
\begin_inset Formula 
\[
f\left(p_{1},...,p_{K}\right)=\prod_{i,k}p_{k}^{x_{ik}}\prod_{k}p_{k}^{c_{k}},
\]

\end_inset

 or, equivalently, its log
\begin_inset Formula 
\[
g\left(p_{1},...,p_{K}\right)=\log f\left(p_{1},...,p_{K}\right)=\sum_{ik}x_{ik}\log\left(p_{k}\right)+\sum_{k}c_{k}\log\left(p_{k}\right).
\]

\end_inset

 We have
\begin_inset Formula 
\[
\begin{array}{cc}
\min & g\left(p_{1},...,p_{K}\right)\\
s.a. & \sum p_{i}=1\\
 & p_{i}\in\left[0,1\right],\forall i
\end{array}.
\]

\end_inset

 The lagrangian is
\begin_inset Formula 
\[
L=g-\lambda\left(\sum p_{i}-1\right),
\]

\end_inset

 so that
\begin_inset Formula 
\begin{align*}
\frac{\partial}{\partial p_{k}}L=\frac{\partial}{\partial p_{k}}g-\lambda= & \sum_{i}x_{ik}\frac{1}{p_{k}}+c_{k}\frac{1}{p_{k}}-\lambda=0\iff\lambda=\frac{\sum_{i}x_{ik}+c_{k}}{p_{k}}\iff p_{k}=\frac{\sum_{i}x_{ik}+c_{k}}{\lambda},
\end{align*}

\end_inset

 and then
\begin_inset Formula 
\[
1=\sum_{k}p_{k}=\sum_{k}\frac{\sum_{i}x_{ik}+c_{k}}{\lambda}=\frac{1}{\lambda}\left(n+\sum_{k}c_{k}\right)\iff\lambda=n+\sum_{k}c_{k}.
\]

\end_inset

 Finally, substituting back, we obtain
\begin_inset Formula 
\[
p_{k}=\frac{c_{k}+\sum_{i}x_{ik}}{n+\sum_{k}c_{k}},
\]

\end_inset

 as we wanted!
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "upc"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
