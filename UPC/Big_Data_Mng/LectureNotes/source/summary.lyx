#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{footmisc}
\usepackage{listings}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1,0.99,0.94}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,  
    frame=single,
    tabsize=2
}

\lstset{style=mystyle}
\end_preamble
\use_default_options true
\begin_modules
tcolorbox
customHeadersFooters
theorems-ams-bytype
theorems-sec-bytype
algorithm2e
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "linkcolor=blue, urlcolor=blue, citecolor=blue, pdfstartview={FitH}, unicode=true"
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\boxbgcolor #62a0ea
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 3cm
\rightmargin 2cm
\bottommargin 3cm
\headheight 2cm
\headsep 1cm
\footskip 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
BDM-MIRI - Big Data Management
\end_layout

\begin_layout Date
Spring 2023
\end_layout

\begin_layout Author
Jose Antonio Lorencio Abril
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename upc-logo.png
	scale 50

\end_inset


\end_layout

\begin_layout Standard
\align right
Professor: Alberto Abelló
\end_layout

\begin_layout Standard
\align right
Student e-mail: jose.antonio.lorencio@estudiantat..upc.edu
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Address
This is a summary of the course 
\emph on
Big Data Management
\emph default
 taught at the Universitat Politècnica de Catalunya by Professor Alberto
 Abelló in the academic year 22/23.
 Most of the content of this document is adapted from the course notes by
 Abelló and Nadal, 
\begin_inset CommandInset citation
LatexCommand cite
key "Abello2022"
literal "false"

\end_inset

, so I won't be citing it all the time.
 Other references will be provided when used.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList figure

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList table

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList algorithm

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Introduction to Big Data
\end_layout

\begin_layout Subsection
Recognise the relevance of data driven decision making
\end_layout

\begin_layout Standard

\series bold
Data driven decision making
\series default
 is the strategy of using data to make decisions, in order to improve the
 chances of obtaining a positive outcome.
 It has been gaining importance in the past years, mainly because the data
 generation rate is increasing rapidly, allowing greater analyses for those
 who are able to leverage all this data.
\end_layout

\begin_layout Standard
The ability to collect, store, combine and analyze relevant data enables
 companies to gain a competitive advantage over their competitors which
 are not able to take on these task.
\end_layout

\begin_layout Standard
In a nutshell, it is the confluence of three major socio-economic and technologi
cal trends that makes data driven innovation a new phenomenon:
\end_layout

\begin_layout Itemize
The exponential growth in data generated and collected.
\end_layout

\begin_layout Itemize
The widespread use of data analytitcs, including start-ups and small and
 medium entreprises.
\end_layout

\begin_layout Itemize
The emergence of a paradigm shift in knowledge.
\end_layout

\begin_layout Subsection
Identify the three high level categories of analytical tools
\end_layout

\begin_layout Standard

\series bold
Business Intelligence (BI)
\series default
 is the concept of using dashboard to represent the status and evolution
 of companies, using data from the different applications used by the production
 systems of the company, which needs to be processed with ETL (Extract,
 Transform, Load) pipelines into a Data Warehouse.
 This data is then modelled into data cubes, that are queried with OLAP
 (OnLine Analytic Processing) purposes.
 The analytical tools that this setup allows are three:
\end_layout

\begin_layout Enumerate
Static generation of reports.
\end_layout

\begin_layout Enumerate
Dynamic (dis)aggregation and navigation by means of OLAP operations.
\end_layout

\begin_layout Enumerate
Inference of hidden patterns or trends with data mining tools.
\end_layout

\begin_layout Subsection
Identify the two main sources of Big Data
\end_layout

\begin_layout Standard
The two main sources of Big Data are:
\end_layout

\begin_layout Itemize
The 
\series bold
Internet
\series default
, which shifted from a passive role, where static hand-crafted contents
 were provided by some gurus, to a dynamic role, where contents can be easily
 generated by anybody in the world, specially through social networks.
\end_layout

\begin_layout Itemize
The 
\series bold
improvement of automation and digitalization
\series default
 on the side of industries, which allows to monitor many relevant aspects
 of the company's scope, giving rise to the concept of Internet of Things
 (IoT), and generating a continuous flow of information.
\end_layout

\begin_layout Subsection
Give a definition of Big Data
\end_layout

\begin_layout Standard

\series bold
Big Data
\series default
 is a natural evolution of Business Intelligence, and inherits its ultimate
 goal of transforming raw data into valuable knowledge, and it can be characteri
zed in terms of the 
\series bold
five V's
\series default
:
\end_layout

\begin_layout Itemize

\series bold
Volume
\series default
: there are large amount of digital information produced and stored in new
 systems.
\end_layout

\begin_layout Itemize

\series bold
Velocity
\series default
: the pace at which data is generated, ingested and processed is very fast,
 giving rise to the concept of data stream (and two related challenges:
 
\emph on
data stream ingestion
\emph default
 and 
\emph on
data stream processing
\emph default
).
\end_layout

\begin_layout Itemize

\series bold
Variety
\series default
: there are multiple, heterogeneous data formats and schemas, which need
 to be dealt with.
 Special attention is needed for semi-structured and unstructured external
 data.
 The 
\emph on
data variety challenge
\emph default
 is considered as the most crucial challenge in data driven organizations.
\end_layout

\begin_layout Itemize

\series bold
Variability
\series default
: the incoming data can have an evolving nature, which the system needs
 to be able to cope with.
\end_layout

\begin_layout Itemize

\series bold
Veracity
\series default
: the veracity of the data is related to its quality, and it makes it compulsory
 to develop 
\emph on
Data Governance
\emph default
 practices, to effectively manage data assets.
\end_layout

\begin_layout Subsection
Compare traditional data warehousing against Big Data management
\end_layout

\begin_layout Standard
In traditional business intelligence, data from different sources inside
 the company is ETL-processed into the data warehouse, which can then be
 analyzed using the three types of analyses we've seen (Reports, OLAP, DM),
 in order to extract useful information that ultimately affects the strategy
 of the company.
 This is summarized in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Business-Intelligence-Cycle."
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 As highlighted in the figure, the data warehousing process encompasses
 the ETL processes and the Data Warehouse design and maintenance.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename BI_Cycle.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Business-Intelligence-Cycle."

\end_inset

Business Intelligence Cycle.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In the context of big data, the focus is shifted, from analyzing data from
 just inside sources, to data from all types of heterogeneous sources.
 In this setup, instead of doing an ETL process, the data is collected,
 through the process of ingestion, and stored into a Data Lake (from which
 analysts would extract data and perform all necessary transformations a
 posteriori) or a Polystore (which is a DBMS built on top of different other
 technologies, to be able to cope with heterogeneous data).
 Whatever the storing decision, Big Data Analytics are then done on this
 data, differenciating:
\end_layout

\begin_layout Itemize
Small analytics: querying and reporting the data and OLAP processing.
\end_layout

\begin_layout Itemize
Big Analytics: performing data mining on the data.
\end_layout

\begin_layout Standard
This process is depicted in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Big-Data-Cycle."
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 In this diagram, we see that the 
\series bold
Big Data Management
\series default
 consists of the task of ingestion, together with the design and maintenance
 of the Data Lake / Polystore.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename BigData_Cycle.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Big Data Cycle.
\begin_inset CommandInset label
LatexCommand label
name "fig:Big-Data-Cycle."

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Thus, the differences are:
\end_layout

\begin_layout Itemize
Data Warehousing does the ETL process over the data produced by the company,
 while Big Data Management does the process of ingestion, by which data
 from internal and external sources is collected.
\end_layout

\begin_layout Itemize
Data Warehousing uses a Data Warehouse to store the ETLed data and the analyses
 need to be designed with the structure of this stored data.
 In contrast, in Big Data Management, the storing facility can cope with
 the data as is, so the analyses have a wider scope, but they need to correctly
 treat the data for each analysis conducted.
\end_layout

\begin_layout Itemize
Thus, as can be inferred from the previous paragraphs, Big Data Management
 provides a more flexible setup than Data Warehousing, at the expense of
 needing to perform ad-hoc transformation for each analysis, which can lead
 to repetition and a decrease in performance.
 Nonetheless, this decrease is not really a drawback, because some big data
 analytics tasks cannot be undertaken without this added flexibility.
\end_layout

\begin_layout Subsection
Distinguish descriptive, predictive and prescriptive analysis
\end_layout

\begin_layout Itemize

\series bold
Descriptive analysis
\series default
: uses basic statistics to describe the data.
 In a DW environment, OLAP tools are used for this purpose, in an interactively
 manner, modifying the analysis point of vire to facilitate the understanding
 and gain knowledge about the stored data.
 Basically, understand past data (what happened, when happened, why it happened).
\end_layout

\begin_layout Itemize

\series bold
Predictive analysis
\series default
: uses a set of statistical techniques to analyze historical facts, with
 the aim of making predictions about future events.
 Basically, compare incoming data to our knowledge of past data, in order
 to make predictions about the future (what will happen).
\end_layout

\begin_layout Itemize

\series bold
Prescriptive analysis
\series default
: takes as input the predictions of previous analyses to suggest actions,
 decisions and describe the possible implications of each of them.
 Basically, use predictions obtained via predictive analysis to take action
 and make decisions, as well as to estimate the impact of these decisions
 in the future (how we should respond to this situation).
\end_layout

\begin_layout Subsection
Explain the novelty of Cloud Computing
\end_layout

\begin_layout Standard
The novelty of cloud computing is the same as when electricity shifted from
 being generated by each company to be centrally generated, benefiting from
 scale economies and improving the efficiency of the electricity generation.
 In the case of Cloud Computing, the shift is from companies having their
 own hardware and software, to an environment in which these resources are
 offered by a third company, which leverages again the economies of scale
 and the possibility to allocate resources when needed, increasing the overall
 efficiency of the tech industries and reducing the costs of each company,
 as they now don't need to buy expensive pieces of hardware and software,
 maintain them, etc.
\end_layout

\begin_layout Subsection
Justify the benefits of Cloud Computing
\end_layout

\begin_layout Itemize
It eliminates upfront investment, as it is not needed to buy hardware anymore.
\end_layout

\begin_layout Itemize
You pay for what you use, so costs are reduced because efficient allocation
 is a complex task to overcome.
\end_layout

\begin_layout Itemize
The main benefit comes from the aforementioned economy of scale, that allows
 to reduce costs and improve efficiency.
 A machine hosted in-house is most of the time underused, because companies
 don't usually require it being 100% operational all the time.
 However, when the machine is available for thousand or millions of customers,
 it will almost always be required to be working.
\end_layout

\begin_layout Itemize
Customers can adapt their costs to their needs at any time.
\end_layout

\begin_layout Itemize
There is no need to manage, maintain and upgrade hardware anymore.
\end_layout

\begin_layout Subsection
Explain the link between Big Data and Cloud Computing
\end_layout

\begin_layout Standard
Cloud computing and big data are closely related, and in many ways, cloud
 computing has enabled the growth and adoption of big data technologies.
\end_layout

\begin_layout Standard
One of the main advantages of cloud computing is its ability to provide
 flexible and scalable computing resources on demand.
 This is especially important for big data, which requires significant computing
 power to process and analyze large volumes of data.
 Cloud computing allows organizations to easily spin up large-scale computing
 clusters and storage systems to handle big data workloads, without the
 need to invest in expensive on-premises infrastructure.
\end_layout

\begin_layout Standard
In addition to providing scalable computing resources, cloud computing also
 offers a wide range of data storage and processing services that can be
 used for big data workloads.
 Cloud providers offer a variety of data storage services, such as object
 storage, file storage, and database services, that can be used to store
 and manage large volumes of data.
 Cloud providers also offer big data processing services, such as Apache
 Hadoop, Apache Spark, and machine learning tools, which can be used to
 analyze and extract insights from big data.
\end_layout

\begin_layout Standard
Cloud computing also provides the ability to easily integrate and share
 data between different systems and applications, both within an organization
 and with external partners.
 This is important for big data, which often requires data from multiple
 sources to be combined and analyzed to gain insights.
\end_layout

\begin_layout Standard
Overall, cloud computing has played a key role in enabling the growth and
 adoption of big data technologies, by providing flexible and scalable computing
 resources, a wide range of data storage and processing services, and the
 ability to easily integrate and share data between different systems and
 applications.
\end_layout

\begin_layout Subsection
Distinguish the main four service levels in Cloud Computing
\end_layout

\begin_layout Standard
The main four service levels are:
\end_layout

\begin_layout Itemize

\series bold
Infrastructure as a Service (IaaS)
\series default
: provides virtualized computing resources, such as virtual machines, storage,
 and networking, which can be provisioned and managed through an API or
 web console.
\end_layout

\begin_layout Itemize

\series bold
Platform as a Service (PaaS)
\series default
: provides a platform for building and deploying applications, including
 development tools, runtime environments, and middleware, which can be accessed
 through an API or web console.
\end_layout

\begin_layout Itemize

\series bold
Software as a Service (SaaS)
\series default
: provides access to software applications over the internet, which are
 hosted and managed by a third-party provider, and can be accessed through
 a web browser or API.
\end_layout

\begin_layout Itemize

\series bold
Business as a Service (BaaS)
\series default
: This is a type of cloud computing service that provides businesses with
 access to a range of software tools and services, such as customer relationship
 management (CRM) systems, enterprise resource planning (ERP) software,
 and human resources management tools.
 BaaS allows businesses to outsource the management and maintenance of these
 systems to a third-party provider, freeing up resources and allowing the
 business to focus on their core operations.
 BaaS can be a cost-effective way for businesses to access enterprise-level
 software tools without the need to invest in on-premises infrastructure
 and maintenance.
 This is, a whole business process is outsourced, for example using PayPal
 as a paying platform frees the company from this process.
\end_layout

\begin_layout Standard
But there are more services offered by Cloud Computing:
\end_layout

\begin_layout Itemize

\series bold
Database as a Service (DBaaS)
\series default
: specific platform services providing data management functionalities.
\end_layout

\begin_layout Itemize

\series bold
Container as a Service (CaaS)
\series default
: allows applications to be packaged into containers, which can be run consisten
tly across different environments, such as development, testing, and production.
\end_layout

\begin_layout Itemize

\series bold
Function as a Service (FaaS)
\series default
: creates small stand-alone pieces of software that can be easily combined
 to create business flows in interaction with other pieces from potentially
 other service providers.
\end_layout

\begin_layout Itemize

\series bold
Serverless computing
\series default
: allows developers to build and run applications without managing servers,
 by providing an event-driven computing model, in which code is executed
 in response to specific triggers.
\end_layout

\begin_layout Itemize

\series bold
Data analytics and storage
\series default
: provides tools for storing and analyzing large volumes of data, such as
 data warehouses, data lakes, and analytics tools, which can be accessed
 through APIs or web consoles.
\end_layout

\begin_layout Itemize

\series bold
Machine learning and artificial intelligence
\series default
: Provides tools and services for building, training, and deploying machine
 learning models, such as pre-trained models, APIs for image recognition
 and natural language processing, and tools for custom model development.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Big Data Design
\end_layout

\begin_layout Subsection
Define the impedance mismatch
\end_layout

\begin_layout Standard
Impedance mismatch often arises when data is passed between different layers
 of an application, such as between the front-end user interface and the
 back-end database, or between different applications that need to exchange
 data.
 The data structures used in each layer or system may be different, which
 can cause issues with data mapping, performance, and scalability.
\end_layout

\begin_layout Standard
For example, if a front-end application requires data that is stored in
 a relational database, the application may need to perform complex queries
 to retrieve and transform the data into a format that can be used by the
 user interface.
 This can lead to performance issues and increased complexity in the application
 code.
 Similarly, if different applications or services use different data formats
 or structures, it can be difficult to exchange data between them, which
 can lead to integration issues and increased development time.
\end_layout

\begin_layout Standard
To address impedance mismatch, software developers often use techniques
 such as object-relational mapping (ORM) to map data between different layers
 of an application, or use standard data formats such as JSON or XML to
 enable data exchange between different systems.
 These techniques can help to simplify data mapping, improve performance,
 and increase the scalability of the system.
\end_layout

\begin_layout Subsection
Identify applications handling different kinds of data
\end_layout

\begin_layout Itemize

\series bold
Relational data (OLTP)
\series default
: Relational databases are commonly used for online transaction processing
 (OLTP) applications, such as e-commerce websites, banking applications,
 and inventory management systems.
 Examples of applications that use relational databases include Oracle,
 MySQL, PostgreSQL, and Microsoft SQL Server.
\end_layout

\begin_layout Itemize

\series bold
Multidimensional data (OLAP)
\series default
: Multidimensional databases are commonly used for online analytical processing
 (OLAP) applications, such as data warehousing, business intelligence, and
 data mining.
 Examples of applications that use multidimensional databases include Microsoft
 Analysis Services, IBM Cognos, and Oracle Essbase.
\end_layout

\begin_layout Itemize

\series bold
Key-value data
\series default
: Key-value databases are commonly used for high-performance, highly scalable
 applications, such as caching, session storage, and user profiles.
 Examples of applications that use key-value databases include Redis, Amazon
 DynamoDB, and Apache Cassandra.
\end_layout

\begin_layout Itemize

\series bold
Column-family data
\series default
: Column-family databases are commonly used for applications that require
 fast reads and writes on a large-scale, such as content management systems,
 social networks, and recommendation engines.
 Examples of applications that use column-family databases include Apache
 HBase, Apache Cassandra, and ScyllaDB.
\end_layout

\begin_layout Itemize

\series bold
Graph data
\series default
: Graph databases are commonly used for applications that involve complex
 relationships between data, such as social networks, fraud detection, and
 recommendation engines.
 Examples of applications that use graph databases include Neo4j, OrientDB,
 and Amazon Neptune.
\end_layout

\begin_layout Itemize

\series bold
Document data
\series default
: Document databases are commonly used for applications that require flexible,
 dynamic data structures, such as content management systems, e-commerce
 platforms, and mobile applications.
 Examples of applications that use document databases include MongoDB, Couchbase
, and Amazon DocumentDB.
\end_layout

\begin_layout Standard
Note that many applications use multiple types of data models, depending
 on the nature of the data and the requirements of the application.
 For example, a social network might use a graph database to store social
 connections, a column-family database to store user data, and a key-value
 database to cache frequently accessed data.
\end_layout

\begin_layout Subsection
Name four different kinds of NOSQL systems
\end_layout

\begin_layout Itemize

\series bold
Key-Value
\series default
: stores data as a collection of key-value pairs.
 Each key is associated with a value, and values can be retrieved and updated
 by their corresponding keys.
 Key-value databases are simple and highly scalable, making them well-suited
 for applications that require high performance and low latency.
\end_layout

\begin_layout Itemize

\series bold
Wide-column (Column-family)
\series default
: stores data as a collection of columns grouped into column families.
 Each column family is a group of related columns, and each column consists
 of a name, a value, and a timestamp.
 Column-family databases are optimized for storing large amounts of data
 with fast writes and queries, making them well-suited for applications
 that require high write and query throughput.
 Such grouping of columns directly translates into a vertical partition
 of the table, and entails the consequent loss of schema flexibility.
\end_layout

\begin_layout Itemize

\series bold
Graph
\series default
: stores data as nodes and edges, representing complex relationships between
 data.
 Nodes represent entities, such as people, places, or things, and edges
 represent relationships between entities.
 Graph databases are optimized for querying and analyzing relationships
 between data, making them well-suited for applications that require complex
 querying.
\end_layout

\begin_layout Itemize

\series bold
Document
\series default
: stores data as documents, which can be thought of as semi-structured data
 with a flexible schema.
 Each document consists of key-value pairs, and documents can be grouped
 into collections.
 Document stores are optimized for storing and querying unstructured and
 semi-structured data, making them well-suited for applications that require
 flexibility in data modeling.
\end_layout

\begin_layout Subsection
Explain three consequences of schema variability
\end_layout

\begin_layout Standard
Schema variability refers to the dynamic and flexible nature of data models
 in NoSQL databases.
 Unlike relational databases, NoSQL databases allow for the schema to be
 flexible and adaptable, which means that the data structure can evolve
 over time without requiring changes to the database schema.
 This allows for greater agility in data modeling, as it makes it easier
 to add or remove fields or change the data structure as needed.
\end_layout

\begin_layout Standard
Its three main consequences are:
\end_layout

\begin_layout Itemize

\series bold
Gain in flexibility
\series default
: allowing schema variability makes the system more flexible to cope with
 changes in the data.
\end_layout

\begin_layout Itemize

\series bold
Reduced data semantics and consistency
\series default
: With a flexible schema, it is possible to store data that does not conform
 to a predefined structure.
 This can lead to inconsistencies in data quality and make it more difficult
 to enforce data constraints, such as data types or referential integrity.
\end_layout

\begin_layout Itemize

\series bold
Data independence principle is lost
\series default
: allowing schema variability can be seen as a departure from the traditional
 concept of data independence, which is a key principle of the relational
 data model.
 Data independence refers to the ability to change the physical storage
 or logical structure of the data without affecting the application programs
 that use the data.
 In a relational database, the data is organized into tables with fixed
 schema, which allows for greater data independence.
 
\end_layout

\begin_deeper
\begin_layout Standard
However, in NoSQL databases, schema variability is often seen as a necessary
 trade-off for achieving greater flexibility and scalability.
 By allowing for a more flexible data model, NoSQL databases can better
 accommodate changes to the data structure over time, without requiring
 changes to the database schema or application code.
 This can help improve agility and reduce development time.
 
\end_layout

\begin_layout Standard
That being said, NoSQL databases still adhere to the fundamental principles
 of data independence in many ways.
 For example, they still provide a layer of abstraction between the application
 and the physical storage of the data, which helps to insulate the application
 from changes to the underlying data storage.
 Additionally, many NoSQL databases provide APIs that allow for flexible
 querying and manipulation of data, which helps to maintain a level of data
 independence.
\end_layout

\end_deeper
\begin_layout Standard
Some more consequences:
\end_layout

\begin_layout Itemize
Increased data complexity: As the data model becomes more flexible, the
 data can become more complex and difficult to manage.
 This can lead to increased development and maintenance costs, as well as
 potential performance issues.
\end_layout

\begin_layout Itemize
Increased development and maintenance costs: As the schema becomes more
 flexible, the complexity of the data model can increase, which can result
 in higher development and maintenance costs.
\end_layout

\begin_layout Itemize
Reduced performance: With a more complex data model, queries can become
 more complex, which can result in slower query performance.
 Additionally, since the schema is not fixed, indexing and optimization
 become more difficult, which can further impact performance.
\end_layout

\begin_layout Subsection
Explain the consequences of physical independence
\end_layout

\begin_layout Standard
Physical independence is a key principle of the relational data model, which
 refers to the ability to change the physical storage of the data without
 affecting the logical structure of the data or the application programs
 that use the data.
 This means that the application should be able to access and manipulate
 the data without being aware of the underlying physical storage details,
 such as the storage medium or the location of the data.
\end_layout

\begin_layout Standard
The consequences of physical independence include:
\end_layout

\begin_layout Itemize
Reduced maintenance costs: With physical independence, it is easier to change
 the physical storage of the data without affecting the application.
 This can help reduce maintenance costs, as it allows for more flexibility
 in how the data is stored and accessed over time.
\end_layout

\begin_layout Itemize
Improved scalability: Physical independence can help improve scalability,
 as it allows for the data to be distributed across multiple physical storage
 locations or devices, which can help to improve performance and reduce
 the impact of failures.
\end_layout

\begin_layout Itemize
Greater portability: With physical independence, the application is not
 tied to a specific physical storage medium or location, which can help
 improve portability across different hardware or software platforms.
\end_layout

\begin_layout Itemize
Improved performance: Physical independence can help improve performance,
 as it allows for the data to be stored and accessed in the most efficient
 way possible, without being limited by the constraints of a specific physical
 storage medium or location.
\end_layout

\begin_layout Standard
Nonetheless, physical independence enhance the problem of the impedance
 mismatch, because if data is needed in a different form from how it is
 stored, it has to be transformed, introducing a computing overhead.
 If we store the data as needed for the application, this problem is reduced,
 but the physical independence can be lost.
\end_layout

\begin_layout Subsection
Explain the two dimensions to classify NOSQL systems according to how they
 manage the schema
\end_layout

\begin_layout Standard
The schema can be explicit/implicit:
\end_layout

\begin_layout Itemize

\series bold
Implicit schema
\series default
: schema that is not explicitly defined or documented.
 Instead, the schema is inferred or derived from the data itself, usually
 through analysis or observation.
 This can be useful in situations where the data is very dynamic or unstructured
, and where the structure of the data is not known in advance.
\end_layout

\begin_layout Itemize

\series bold
Explicit schema
\series default
: schema that is explicitly defined and documented, usually using a schema
 language or a data modeling tool.
 The schema specifies the types of data that can be stored, the relationships
 between different types of data, and any constraints or rules that govern
 the data.
\end_layout

\begin_layout Standard
And it can be fixed/variable:
\end_layout

\begin_layout Itemize

\series bold
Fixed schema
\series default
: schema that is static and unchanging, meaning that the structure and organizat
ion of the data is predefined and cannot be modified.
 This is common in relational databases, where the schema is usually defined
 in advance and remains fixed over time.
\end_layout

\begin_layout Itemize

\series bold
Variable schema
\series default
: schema that is dynamic and flexible, meaning that the structure and organizati
on of the data can change over time.
 This is common in NoSQL databases, where the schema may be more fluid and
 adaptable to changing data requirements.
\end_layout

\begin_layout Standard
Note that this is not a strict classification, but rather two dimension
 ranges in which a certain schema can lie.
\end_layout

\begin_layout Subsection
Explain the three elements of the RUM conjecture
\end_layout

\begin_layout Standard
The RUM conjecture suggests that in any database system, the overall performance
 can be characterized by a trade-off between the amount of memory used,
 the number of reads performed, and the number of updates performed.
 Specifically, the conjecture states that there is a fundamental asymmetry
 between reads and updates, and that the performance of the system is strongly
 influenced by the balance between these two operations.
 In general, the more reads a system performs, the more memory it requires,
 while the more updates it performs, the more it impacts the system's overall
 performance.
\end_layout

\begin_layout Standard
The RAM conjecture has three main elements:
\end_layout

\begin_layout Itemize
Reads: refer to the process of retrieving data from a database.
 In general, read-heavy workloads require more memory to achieve good performanc
e.
\end_layout

\begin_layout Itemize
Updates: refer to the process of modifying data in a database.
 In general, update-heavy workloads require more processing power and can
 negatively impact overall performance.
\end_layout

\begin_layout Itemize
Memory: refers to the amount of memory available to a system.
 In general, increasing memory can improve read-heavy workloads, but may
 not be as effective for update-heavy workloads.
\end_layout

\begin_layout Standard
The RUM conjecture is often used to guide the design and optimization of
 database systems, as it provides a useful framework for understanding the
 trade-offs between different system parameters and performance metrics.
 By understanding the RUM trade-offs, database designers can make informed
 decisions about how to allocate resources, optimize queries, and balance
 the workload of the system.
\end_layout

\begin_layout Subsection
Justify the need of polyglot persistence
\end_layout

\begin_layout Standard
A 
\series bold
polyglot system
\series default
 is a system that uses multiple technologies, languages, and tools to solve
 a problem.
 In the context of data management, a polyglot system is one that uses multiple
 data storage technologies to store and manage data.
 For example, a polyglot system might use a combination of relational databases,
 NoSQL databases, and search engines to store different types of data.
\end_layout

\begin_layout Standard

\series bold
Polyglot persistence
\series default
 is the practice of using multiple storage technologies to store different
 types of data within a single application.
 The idea is to choose the right tool for the job, and to use each technology
 to its fullest potential.
 For example, a polyglot system might use a NoSQL database to store unstructured
 data, a relational database to store structured data, and a search engine
 to provide full-text search capabilities.
\end_layout

\begin_layout Standard
There are several reasons why polyglot persistence is important:
\end_layout

\begin_layout Itemize

\series bold
Flexibility
\series default
: Polyglot systems are more flexible than monolithic systems that use a
 single technology to store all data.
 With a polyglot system, you can choose the right tool for the job, and
 you can adapt to changing requirements and data formats.
\end_layout

\begin_layout Itemize

\series bold
Performance
\series default
: Different data storage technologies are optimized for different types
 of data and workloads.
 By using the right tool for the job, you can improve performance and scalabilit
y.
\end_layout

\begin_layout Itemize

\series bold
Resilience
\series default
: Using multiple data storage technologies can improve the resilience of
 your system.
 If one database fails, the other databases can continue to operate, ensuring
 that your application remains available and responsive.
\end_layout

\begin_layout Itemize

\series bold
Future-proofing
\series default
: By using multiple data storage technologies, you can future-proof your
 system against changing data formats and requirements.
 As new data types and storage technologies emerge, you can add them to
 your system without having to completely overhaul your architecture.
\end_layout

\begin_layout Standard
In summary, polyglot persistence is a powerful approach to data management
 that allows you to use multiple storage technologies to store and manage
 different types of data within a single application.
 By adopting a polyglot approach, you can improve flexibility, performance,
 resilience, and future-proofing.
\end_layout

\begin_layout Subsection
Decide whether two NOSQL designs have more or less explicit/fix schema
\end_layout

\begin_layout Standard
There are several factors that can be used to assess the flexibility and
 explicitness of a schema:
\end_layout

\begin_layout Enumerate
Number of tables/collections: A schema with a large number of tables or
 collections is typically more explicit and less flexible than a schema
 with fewer tables or collections.
 This is because a large number of tables or collections often implies a
 more rigid structure, whereas a smaller number of tables or collections
 can allow for more flexibility.
\end_layout

\begin_layout Enumerate
Number of columns/fields: A schema with a large number of columns or fields
 is typically more explicit and less flexible than a schema with fewer columns
 or fields.
 This is because a large number of columns or fields often implies a more
 rigid structure, whereas a smaller number of columns or fields can allow
 for more flexibility.
\end_layout

\begin_layout Enumerate
Data types: A schema that uses a large number of data types is typically
 more explicit and less flexible than a schema that uses fewer data types.
 This is because a large number of data types often implies a more rigid
 structure, whereas a smaller number of data types can allow for more flexibilit
y.
\end_layout

\begin_layout Enumerate
Use of constraints: A schema that uses a large number of constraints (such
 as foreign keys or unique constraints) is typically more explicit and less
 flexible than a schema that uses fewer constraints.
 This is because constraints often imply a more rigid structure, whereas
 a schema with fewer constraints can allow for more flexibility.
\end_layout

\begin_layout Enumerate
Use of inheritance: A schema that uses inheritance (such as table or collection
 inheritance) is typically more flexible and less explicit than a schema
 that does not use inheritance.
 This is because inheritance allows for more flexibility in the structure
 of the data, whereas a schema that does not use inheritance is typically
 more explicit in its structure.
\end_layout

\begin_layout Standard
Overall, a more explicit schema is one that has a more rigid structure,
 with more tables, fields, data types, and constraints, whereas a more flexible
 schema is one that has fewer tables, fields, data types, and constraints,
 and may use inheritance to provide more flexibility.
\end_layout

\begin_layout Standard
Some examples can be:
\end_layout

\begin_layout Itemize
Fixed/Explicit: An example of a fixed/explicit schema in XML format might
 look like this:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset listings
lstparams "language=PHP"
inline false
status open

\begin_layout Plain Layout

<shopping_cart>
\end_layout

\begin_layout Plain Layout

  <customers>
\end_layout

\begin_layout Plain Layout

    <customer>
\end_layout

\begin_layout Plain Layout

      <name>John Smith</name>
\end_layout

\begin_layout Plain Layout

      <email>john@example.com</email>
\end_layout

\begin_layout Plain Layout

    </customer>
\end_layout

\begin_layout Plain Layout

  </customers>
\end_layout

\begin_layout Plain Layout

  <orders>
\end_layout

\begin_layout Plain Layout

    <order>
\end_layout

\begin_layout Plain Layout

      <order_date>2023-02-17</order_date>
\end_layout

\begin_layout Plain Layout

      <total_price>100.00</total_price>
\end_layout

\begin_layout Plain Layout

    </order>
\end_layout

\begin_layout Plain Layout

  </orders>
\end_layout

\begin_layout Plain Layout

  <products>
\end_layout

\begin_layout Plain Layout

    <product>
\end_layout

\begin_layout Plain Layout

      <name>Widget</name>
\end_layout

\begin_layout Plain Layout

      <description>A small, useful tool</description>
\end_layout

\begin_layout Plain Layout

      <price>10.00</price>
\end_layout

\begin_layout Plain Layout

    </product>
\end_layout

\begin_layout Plain Layout

  </products>
\end_layout

\begin_layout Plain Layout

</shopping_cart>
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this example, the schema is fixed because there are specific tables (customer
s, orders, and products) and specific fields for each table (such as name
 and email for customers, and order_date and total_price for orders).
 There is no room for variation in the structure of the schema.
\end_layout

\end_deeper
\begin_layout Itemize
Fixed/Implicit: An example of a fixed/implicit schema in XML format might
 look like this:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset listings
lstparams "language=PHP"
inline false
status open

\begin_layout Plain Layout

<blog>
\end_layout

\begin_layout Plain Layout

  <posts>
\end_layout

\begin_layout Plain Layout

    <post>
\end_layout

\begin_layout Plain Layout

      <title>My First Blog Post</title>
\end_layout

\begin_layout Plain Layout

      <content>This is my first blog post.</content>
\end_layout

\begin_layout Plain Layout

    </post>
\end_layout

\begin_layout Plain Layout

  </posts>
\end_layout

\begin_layout Plain Layout

</blog>
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this example, the schema is fixed because there is a specific table (posts)
 and specific fields for that table (such as title and content).
 However, there is no fixed field for metadata such as tags or categories.
\end_layout

\end_deeper
\begin_layout Itemize
Flexible/Explicit: An example of a flexible/explicit schema in XML format
 might look like this:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset listings
lstparams "language=PHP"
inline false
status open

\begin_layout Plain Layout

<scientific_data>
\end_layout

\begin_layout Plain Layout

  <experiments>
\end_layout

\begin_layout Plain Layout

    <experiment>
\end_layout

\begin_layout Plain Layout

      <date>2023-02-17</date>
\end_layout

\begin_layout Plain Layout

      <sample_size>100</sample_size>
\end_layout

\begin_layout Plain Layout

      <measurement_units>mg/L</measurement_units>
\end_layout

\begin_layout Plain Layout

    </experiment>
\end_layout

\begin_layout Plain Layout

    <experiment>
\end_layout

\begin_layout Plain Layout

      <date>2023-02-16</date>
\end_layout

\begin_layout Plain Layout

      <sample_size>50</sample_size>
\end_layout

\begin_layout Plain Layout

      <measurement_units>g/L</measurement_units>
\end_layout

\begin_layout Plain Layout

    </experiment>
\end_layout

\begin_layout Plain Layout

  </experiments>
\end_layout

\begin_layout Plain Layout

  <observations>
\end_layout

\begin_layout Plain Layout

    <observation>
\end_layout

\begin_layout Plain Layout

      <value>10.00</value>
\end_layout

\begin_layout Plain Layout

    </observation>
\end_layout

\begin_layout Plain Layout

    <observation>
\end_layout

\begin_layout Plain Layout

      <value>20.00</value>
\end_layout

\begin_layout Plain Layout

    </observation>
\end_layout

\begin_layout Plain Layout

  </observations>
\end_layout

\begin_layout Plain Layout

</scientific_data>
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this example, the schema is flexible because there can be any number
 of experiments and observations, and there are no fixed fields for metadata.
 However, each table (experiments and observations) and each field (such
 as date and sample_size) is explicitly defined in the schema.
\end_layout

\end_deeper
\begin_layout Itemize
Flexible/Implicit: An example of a flexible/implicit schema in XML format
 might look like this:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset listings
lstparams "language=PHP"
inline false
status open

\begin_layout Plain Layout

<social_media>
\end_layout

\begin_layout Plain Layout

  <posts>
\end_layout

\begin_layout Plain Layout

    <post>
\end_layout

\begin_layout Plain Layout

      <text>Hello world!</text>
\end_layout

\begin_layout Plain Layout

    </post>
\end_layout

\begin_layout Plain Layout

  </posts>
\end_layout

\begin_layout Plain Layout

  <comments>
\end_layout

\begin_layout Plain Layout

    <comment>
\end_layout

\begin_layout Plain Layout

      <text>Great post!</text>
\end_layout

\begin_layout Plain Layout

    </comment>
\end_layout

\begin_layout Plain Layout

    <comment>
\end_layout

\begin_layout Plain Layout

      <text>Thanks for sharing.</text>
\end_layout

\begin_layout Plain Layout

    </comment>
\end_layout

\begin_layout Plain Layout

  </comments>
\end_layout

\begin_layout Plain Layout

</social_media>
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this example, the schema is flexible because there can be any number
 of posts and comments, and there are no fixed fields for any table.
 The structure of the schema is also implicit because there is no fixed
 structure for the data.
\end_layout

\end_deeper
\begin_layout Subsection
Given a relatively small UML conceptual diagram, translate it into a logical
 representation of data considering flexible schema representation
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Distributed Data Management
\end_layout

\begin_layout Subsection
Give a definition of Distributed System
\end_layout

\begin_layout Standard
A distributed system is a system whose components, located at networked
 computers, communicate and coordinate their actions only by passing messages.
\end_layout

\begin_layout Subsection
Enumerate the six challenges of a Distributed System
\end_layout

\begin_layout Standard
The challenges of a distributed system are:
\end_layout

\begin_layout Itemize

\series bold
Scalability
\series default
: the system must be able to continuously evolve to support a grouwing amount
 of tasks.
 This can be achieve by:
\end_layout

\begin_deeper
\begin_layout Itemize
Scale up: upgrading or improving the components.
\end_layout

\begin_layout Itemize
Scale out: adding new components.
\end_layout

\begin_layout Standard
Scale out mitigates bottlenecks, but extra communication is needed between
 a growing number of components.
 This can be partially solved using direct communication between peers.
 Load-balancing is also crucial and, ideally, should happen automatically.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Performance/efficiency
\series default
: the system must guarantee an optimal performance and efficient processing.
 This is usually measured in terms of latency, response time and throughput.
 Parallelizing reduces response time, but uses more resources to do it,
 negatively affecting throughput unless resources are increased to compensate.
\end_layout

\begin_deeper
\begin_layout Standard
This effect can be mitigated optimizing network usage or using distributed
 indexes.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Reliability and availability
\series default
: the system must perform tasks consistently and without failure: it must
 be 
\series bold
reliable
\series default
.
 It also must keep performing tasks even if some of its components fail:
 it must be 
\series bold
available
\series default
.
 The availability is not always possible, and some functionalities might
 be affected, but at least a partial service could be provided when a component
 fails.
\end_layout

\begin_deeper
\begin_layout Standard
To increase failure tolerance, 
\series bold
heartbeat mechanisms
\series default
 can be used to monitor the status of the components, together with 
\series bold
automatic recovery mechanisms
\series default
.
\end_layout

\begin_layout Standard
It is also important to keep the consistency of data shared by different
 components, since this requires synchronization.
 This can be mitigated by 
\series bold
asynchronous synchronization mechanisms
\series default
 and 
\series bold
flexible routing of network messages
\series default
.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Concurrency
\series default
: the system should provide the required control mechanisms to avoid interferenc
es and deadlocks in the presence of concurrent requests.
 
\series bold
Consensus protocols
\series default
 can help solving conflicts and enabling the system to keep working without
 further consequences.
\end_layout

\begin_layout Itemize

\series bold
Transparency
\series default
: users of the system should not be aware of all the aforementioned complexities.
 Ideally, they should be able to work as if the system was not distributed.
\end_layout

\begin_layout Subsection
Give a definition of Distributed Database
\end_layout

\begin_layout Standard
A 
\series bold
Distributed Database (DDB)
\series default
 is an integrated collection of databases that is physically distributed
 across sites in a computer network and a 
\series bold
Distributed Database Management System (DDBMS)
\series default
 is the software system that manages a distributed database such that the
 distribution aspects are transparent to the users.
\end_layout

\begin_layout Standard
There are some terms worth detailing:
\end_layout

\begin_layout Itemize

\series bold
Integrated
\series default
: files in the database should be somehow structured, and an access interface
 common to all of them should be provided so that the physical location
 of data does not matter.
\end_layout

\begin_layout Itemize

\series bold
Physically distributed across sites in a computer network
\series default
: data may be distributed over large geographical areas but it could also
 be the case where distributed data is, indeed, in the very same room.
 The required characteristic is that the communication between nodes is
 done through a computer network instead of simply sharing memory or disk.
\end_layout

\begin_layout Itemize

\series bold
Distribution aspects are transparent to the users
\series default
: transparency refers to separation of the higher-level view of the system
 from lower-level implementation issues.
 Thus, the system must provide mechanisms to hide the implementation details.
\end_layout

\begin_layout Subsection
Explain the different transparency layers in DDBMS
\end_layout

\begin_layout Standard
In a DDBMS 
\series bold
distribution transparency 
\series default
must be ensured, i.e., the system must guarantee data, network, fragmentation
 and replication transparency:
\end_layout

\begin_layout Itemize

\series bold
Data independence
\series default
: data definition occurs at two different levels:
\end_layout

\begin_deeper
\begin_layout Itemize
Logical data independence: refers to indifference of user applications to
 changes in the logical structure of the database.
\end_layout

\begin_layout Itemize
Physical data independence: hides the storage details to the user.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Network transparency
\series default
: the user should be protected from the operation details of the network,
 even hiding its existence whenever possible.
 There are two subclasses:
\end_layout

\begin_deeper
\begin_layout Itemize
Location transparency: any task performed should be independent of both
 the location and system where the operation must be performed.
\end_layout

\begin_layout Itemize
Naming transparency: each object must have a unique name in the database,
 irrespectively of its storage site.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Replication transparency
\series default
: refers to whether synchronizing replicas is left to the user or automatically
 performed by the system.
 Ideally, all these issues should be transparent to users, and they should
 act as if a single copy of data were available.
\end_layout

\begin_layout Itemize

\series bold
Fragmentation transparency
\series default
: when data is fragmented, queries need to be translated from the global
 query into fragmented queries, handling each fragment.
 This translation should be performed by the DDBMS, transparently to the
 user.
\end_layout

\begin_layout Standard
Note that all these transparency levels are incremental.
\end_layout

\begin_layout Standard
Note also that full transparency makes the management of distributed data
 very difficult, so it is widely accepted that data independence and network
 transparency are a must, but replication and/or fragmentation transparency
 might be relaxed to boost performance.
\end_layout

\begin_layout Subsection
Identify the requirements that distribution imposes on the ANSI/SPARC architectu
re
\end_layout

\begin_layout Standard
The 
\series bold
Extended ANSI/SPARC
\series default
 
\series bold
architecture
\series default
was designed to provide a comprehensive framework for organizing and managing
 complex database systems.
 The extended architecture includes the same three levels as the original
 ANSI/SPARC architecture, but it adds a fourth level, called the user level.
\end_layout

\begin_layout Standard
The four levels of the extended ANSI/SPARC architecture are:
\end_layout

\begin_layout Itemize
User Level: The user level is the highest level and includes the end users
 or applications that access the database system.
 The user level provides a simplified view of the data that is available
 in the system, and it defines the interactions between the user and the
 system.
\end_layout

\begin_layout Itemize
External Level: The external level is the next level down and includes the
 external schemas that define the view of the data that is presented to
 the end users or applications.
 Each external schema is specific to a particular user or group of users
 and provides a simplified view of the data that is relevant to their needs.
\end_layout

\begin_layout Itemize
Conceptual Level: The conceptual level is the third level and includes the
 global conceptual schema that describes the overall logical structure of
 the database system.
 The global conceptual schema provides a unified view of the data in the
 system and defines the relationships between different data elements.
\end_layout

\begin_layout Itemize
Internal Level: The internal level is the lowest level and includes the
 physical schema that defines the storage structures and access methods
 used to store and retrieve the data.
 The internal schema is specific to the particular database management system
 and hardware platform that is being used.
\end_layout

\begin_layout Standard
This is summarized in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Extended-ANSI/SPARC-architecture"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ansisparc.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Extended-ANSI/SPARC-architecture"

\end_inset

Extended ANSI/SPARC architecture.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
This architecture does not consider distributed, so it need to be consequently
 adapted to provide distribution trasnparency.
 To this end, a global conceptual schema is needed to define a single logical
 database.
 But the database is composed of several nodes, each of which must now define
 a local conceptual schema and an internal schema.
 These adaptations are depicted in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Extended-ANSI/SPARC-architecture-1"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ansisparc2.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Extended ANSI/SPARC architecture with distribution.
\begin_inset CommandInset label
LatexCommand label
name "fig:Extended-ANSI/SPARC-architecture-1"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In both architectures, mappings between each layer are stored in the global
 catalog, but in the distributed architecture there two mappings which are
 particularly important, namely the 
\series bold
fragmentation schema 
\series default
and the 
\series bold
allocation schema
\series default
.
\end_layout

\begin_layout Subsection
Draw a classical reference functional architecture for DDBMS
\end_layout

\begin_layout Standard
The functional architecture of a centralized DBMS is depicted in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Functional-architecture-of"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The 
\series bold
query manager
\series default
 is a component of a database management system (DBMS) that is responsible
 for handling user queries and managing the overall query processing.
 It is composed of several sub-components:
\end_layout

\begin_layout Itemize
The 
\series bold
view manager
\series default
 is responsible for managing the views defined in the system.
 Views are virtual tables that are derived from the base tables in the database
 and are used to simplify the user's interaction with the database.
 The view manager translates user queries that reference views into queries
 that reference the base tables, allowing the user to interact with the
 database at a higher level of abstraction.
\end_layout

\begin_layout Itemize
The 
\series bold
security manager
\series default
 is responsible for enforcing security policies and access controls in the
 system.
 It ensures that only authorized users are allowed to access the database
 and that they only have access to the data that they are authorized to
 see.
 The security manager also enforces constraints and ensures that the data
 in the database is consistent and valid.
\end_layout

\begin_layout Itemize
The 
\series bold
constraint checker
\series default
 is responsible for verifying that the data in the database conforms to
 the integrity constraints defined in the schema.
 It checks for violations of primary key, foreign key, and other constraints,
 and ensures that the data in the database is consistent and valid.
\end_layout

\begin_layout Itemize
The 
\series bold
query optimizer
\series default
 is responsible for optimizing user queries to improve performance.
 It analyzes the query and determines the most efficient way to execute
 it, taking into account factors such as the available indexes, the size
 of the tables involved, and the cost of different query execution plans.
 The query optimizer generates an optimal query execution plan that minimizes
 the time required to process the query.
\end_layout

\begin_layout Standard
Once these steps are done, the 
\series bold
execution manager
\series default
 launches the different operators in the access plan in order, building
 up the results appropriately.
 
\end_layout

\begin_layout Standard
The 
\series bold
scheduler
\series default
 deals with the problem of keeping the databases in a consistent state,
 even when concurrent accesses occur, preserving isolation (I from ACID).
\end_layout

\begin_layout Standard
The 
\series bold
recovery manager
\series default
 is responsible for preserving the consistency (C), atomicity (A) and durability
 (D) properties.
 
\end_layout

\begin_layout Standard
The 
\series bold
buffer manager
\series default
 is responsible for bringing data to main memory from disk, and vice-versa,
 communicating with the operating system.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado1.png
	scale 70

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Functional architecture of a centralized DBMS.
\begin_inset CommandInset label
LatexCommand label
name "fig:Functional-architecture-of"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
This architecture is not sufficient to deal with distributed data.
 The functional architecture of a distributed DBMS (DDBMS) is depicted in
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Functional-architecture-of-1"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 As we can see, there are now two stages:
\end_layout

\begin_layout Enumerate
Modules cooperate at the global level, transforming the data flow and mapping
 it to the lower layers, dealing with a single view of the database and
 the distribution transparency:
\end_layout

\begin_deeper
\begin_layout Enumerate
The 
\series bold
global query manager
\series default
 contains the view manager, security manager, constraint checker and query
 ooptimizer, which behave as in the centralized case, except for the optimizer,
 which now considers data location and consults the global schema to determine
 which node does what.
 
\end_layout

\begin_layout Enumerate
The 
\series bold
global execution manager
\series default
 inserts communication primitives in the execution plan and coordinates
 the execution of the pieces of the query in the different components to
 build up the final results from all the query pieces executed distributedly.
\end_layout

\begin_layout Enumerate
The 
\series bold
global scheduler
\series default
 receives the global execution plan and distributes trasks between the available
 sites, guaranteeing isolation between different users.
\end_layout

\end_deeper
\begin_layout Enumerate
Modules cooperate at the local level, with a very similar behavior to that
 of the centralized DBMS.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado2.png
	scale 70

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Functional architecture of a DDBMS.
\begin_inset CommandInset label
LatexCommand label
name "fig:Functional-architecture-of-1"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Enumerate the eight main features of Cloud Databases
\end_layout

\begin_layout Itemize
Ability to scale horizontally.
\end_layout

\begin_layout Itemize
Efficient fragmentation techniques.
\end_layout

\begin_layout Itemize
Use as efficiently as possible distributed memory and indexing mechanisms
 to parallelize execution.
 Spped up relies on massive replication and parallelism (which in turn improve
 reliability and availability).
\end_layout

\begin_layout Itemize
Cloud Databases relax the strong consistency asked by ACID transactions
 and define the weaker concept of eventual consistency.
\end_layout

\begin_layout Itemize
A simplistic call level interface or protocol is provided to manage data,
 which is easy to learn and use, but puts the optimization burden on the
 side of the developers.
 This also compromises some transparency.
 The schemaless nature of these systems complicates even more the creation
 of a declarative query language like SQL.
\end_layout

\begin_layout Itemize
The setting up of hardware and software must be quick and cheap.
\end_layout

\begin_layout Itemize
The concept of 
\series bold
multi-tenancy
\series default
 appears: the same hardware/software is shared by many tenants.
 This requires mechanisms to manage the sharing and actually benefitting
 from it.
\end_layout

\begin_layout Itemize
Rigid pre-defined schemas are not appropriate for these databases.
 Instead, there is a need towards gaining flexibility.
\end_layout

\begin_layout Subsection
Explain the difficulties of Cloud Database providers to have multiple tenants
\end_layout

\begin_layout Standard
The difficulty can be summarized as the need to deal with the potential
 high number of tenants, and the unpredictability of their workloads' characteri
stics.
 Popularity of tenants can change very rapidly, fact that impacts the Cloud
 services hosting their products.
 Also, the activities that they perform can change.
\end_layout

\begin_layout Standard
Thus, the provider has to implement mechanisms to be able to deal with this
 variety and variability in the workloads.
\end_layout

\begin_layout Standard
Also, the system should tolerate failures and offer self-healing mechanisms,
 if possible.
\end_layout

\begin_layout Standard
Finally, the software should easily allow to scale out to guarantee the
 required latencies.
 Adding or upgrading new machines should happen progressively, so that service
 suspension is not necessary at all.
\end_layout

\begin_layout Subsection
Enumerate the four main problems tenants/users need to tackle in Cloud Databases
\end_layout

\begin_layout Itemize

\series bold
Data design
\series default
: provides the means to decide on how to fragment the data, where to place
 each fragment, and how many times they will be stored (replication).
\end_layout

\begin_layout Itemize

\series bold
Catalog management
\series default
: requires the same considerations as the design of the database regarding
 fragmentation, locality and replication, but with regard to metadata instead
 of data.
 The difference in this case is that some of the decisions are already made
 on designing the tool and few degrees of freedom are left for administrators
 and developers.
\end_layout

\begin_layout Itemize

\series bold
Transaction management
\series default
: it is specially hard and expensive in distributed environments.
 Distributed recovery and concurrency control mechanisms exist, but there
 is a need to find a trade-off between the security they guarantee and the
 performance impact they have.
\end_layout

\begin_deeper
\begin_layout Standard
Specially relevant in this case is the management of replicas, which are
 expensive to update, but reduce query latency and improve availability.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Query processing
\series default
: it must be as efficient as possible.
 Parallelism should benefit from data distribution, without incurring in
 much communication overhead, which can be reduced by replicating data.
\end_layout

\begin_layout Subsection
Distinguish the cost of sequential and random access
\end_layout

\begin_layout Subsection
Explain the difference between the cost of sequential and random access
\end_layout

\begin_layout Subsection
Distinguish vertical and horizontal fragmentation
\end_layout

\begin_layout Standard

\series bold
Data fragmentation
\series default
 deals with the problem of breaking datasets into smaller pieces, decreasing
 the working unit in the distributed system.
 It has been useful to reflect the fact that applications and users might
 be interested in accessing different subsets of the data.
 Different subsets are naturally needed at different nodes and it makes
 sense to allocate fragments where they are more likely to be needed for
 use.
 This is 
\series bold
data locality
\series default
.
\end_layout

\begin_layout Standard
There are two main fragmentation approaches:
\end_layout

\begin_layout Itemize

\series bold
Horizontal fragmentation
\series default
: a selection predicate is used to create different fragments and, according
 to an attribute value, place each row in the corresponding fragment.
\end_layout

\begin_deeper
\begin_layout Standard
A distributed system benefits from horizontal fragmentation when it needs
 to mirror geographically distributed data to facilitate recovery and parallelis
m, to reduce the depth of indexes and to reduce contention.
\end_layout

\begin_layout Standard
Fragmentation can go from one extreme (no fragmentation) to the other (placing
 each row in a different fragment).
 We need to know which predicates are of interest in our database.
 As a general rule: 
\emph on
the 20% most active users produce 80% of the total accesses
\emph default
.
 We should focus on these users to determine which predicates to consider
 in our analysis.
\end_layout

\begin_layout Standard
Finally, we need to guarantee the correctness:
\end_layout

\begin_layout Itemize
Completeness: the fragmentation predicates must guarantee every row is assigned
 to, at least, one fragment.
\end_layout

\begin_layout Itemize
Disjointness: the fragmentation preficates must be mutually exclusive (
\series bold
minimality property
\series default
).
\end_layout

\begin_layout Itemize
Reconstruction: the union of all the fragments must constitute the original
 dataset.
\end_layout

\begin_layout Standard
We have only considered single relations for this analysis, but it is also
 possible to consider related datasets and fragment them together, this
 is called 
\series bold
derived horizontal fragmentation
\series default
.
 Let 
\begin_inset Formula $R,S$
\end_inset

 be two relations such that 
\begin_inset Formula $R$
\end_inset

 possess a foreign key to 
\begin_inset Formula $S$
\end_inset

 and are related by means of a relationship 
\begin_inset Formula $r$
\end_inset

.
 In this case, 
\begin_inset Formula $S$
\end_inset

 is the 
\series bold
owner
\series default
 and 
\begin_inset Formula $R$
\end_inset

 is the 
\series bold
member
\series default
.
 Suppose also that 
\begin_inset Formula $S$
\end_inset

 is fragmented in 
\begin_inset Formula $n$
\end_inset

 fragments 
\begin_inset Formula $S_{i},i=1,...,n$
\end_inset

, and we want to fragment 
\begin_inset Formula $R$
\end_inset

 regarding 
\begin_inset Formula $S$
\end_inset

 using the relationship 
\begin_inset Formula $r$
\end_inset

.
 The derived horizontal fragmentation is defined as 
\begin_inset Formula 
\[
R_{i}=R\ltimes S_{i},\ i=1,...,n,
\]

\end_inset

 where 
\begin_inset Formula $\ltimes$
\end_inset

 is the left-semijoin
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Formula $R\ltimes S$
\end_inset

 pairs those tuples in 
\begin_inset Formula $R$
\end_inset

 for which there is at least one tuple in 
\begin_inset Formula $S$
\end_inset

 with matching joining key.
\end_layout

\end_inset

 and the joining attributes are those in 
\begin_inset Formula $r$
\end_inset

.
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $R$
\end_inset

 and 
\begin_inset Formula $S$
\end_inset

 are related by more than one relationship, we should apply the following
 criteria to decide which one to use:
\end_layout

\begin_layout Itemize
The fragmentation more used by users/applications.
\end_layout

\begin_layout Itemize
The fragmentation that maximizes the parallel execution of the queries.
\end_layout

\begin_layout Standard
In order to consider a derived horizontal fragmentaiton to be complete and
 disjoint, two additional constraints must hold on top of those stated before:
\end_layout

\begin_layout Itemize
Completeness: the relationship used to semijoin both datasets must enforce
 the referential integrity constraint.
\end_layout

\begin_layout Itemize
Disjointness: the join attribute must be the owner's key.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Vertical fragmentation
\series default
: partitions the datasets in smaller subsets by projecting some attributes
 in each fragment.
 
\end_layout

\begin_deeper
\begin_layout Standard
Vertical fragmentation has been traditionally overlooked in practice, because
 it worsened insertions and update timems of transactional systems in many
 times.
 However, with the arrival of read-only workloads, this kind of fragmentation
 arose as a powerful alternative to decrease the number of attributes to
 be read from a dataset.
 
\end_layout

\begin_layout Standard
In general, it improves the ratio of useful data read and it also reduces
 contention and facilitates recovery and parallelism.
\end_layout

\begin_layout Standard
As disadvantages, note that it increases the number of indexes, worsens
 update and insertion time and increases the space used by data, because
 the primery key is replicated at each fragment.
\end_layout

\begin_layout Standard
Deciding how to group attributes is not obvious at all.
 The information required is:
\end_layout

\begin_layout Itemize
Data characteristics: set of attributes and value distribution for all attribute
s.
\end_layout

\begin_layout Itemize
Workload: frequency of each query, access plan and estimated cost of each
 query and selectivity of each predicate.
\end_layout

\begin_layout Standard
A good heuristic is the following:
\end_layout

\begin_layout Enumerate
Determine primary partitions (subsets of attributes always accessed together).
\end_layout

\begin_layout Enumerate
Generate a disjoint and covering combination of primary partitions, which
 would potentially be stored together.
\end_layout

\begin_layout Enumerate
Evaluate the cost of all combinations generated in the previous phase.
\end_layout

\end_deeper
\begin_layout Subsection
Recognize the complexity and benefits of data allocation
\end_layout

\begin_layout Standard
Once the data is fragmented, we must decide where to place each segment,
 trying to optimize some criteria:
\end_layout

\begin_layout Itemize

\series bold
Minimal cost
\series default
: function resulting of computing the cost of storing each fragment 
\begin_inset Formula $F_{i}$
\end_inset

 at a certain node 
\begin_inset Formula $N_{i}$
\end_inset

, the cost of querying 
\begin_inset Formula $F_{i}$
\end_inset

 at 
\begin_inset Formula $N_{i}$
\end_inset

 and the cost of updating each fragment 
\begin_inset Formula $F_{i}$
\end_inset

 at all places where it is replicated, and the cost od communication.
\end_layout

\begin_layout Itemize

\series bold
Maximal performance
\series default
: the aim is to minimize the response time or maximize the overall throughput.
\end_layout

\begin_layout Standard
This problem is NP-hard and the optimal solution depends on many factors.
 
\end_layout

\begin_layout Standard
In a dynamic environment the workload and access patterns may change and
 all these statistics should always be available in order to find the optimal
 solution.
 Thus, the problem is simplified with certain assumptions and simplified
 cost models are built so that any optimization algorithm can be adopted
 to approximate the optimal solution.
\end_layout

\begin_layout Standard
There are several benefits to data allocation, including:
\end_layout

\begin_layout Itemize
Improved performance: By distributing the data across multiple nodes, the
 workload can be distributed among the nodes, reducing the load on any single
 node and improving overall performance.
\end_layout

\begin_layout Itemize
Increased availability: With data replicated across multiple nodes, the
 failure of any single node does not result in a loss of data or loss of
 access to the data.
\end_layout

\begin_layout Itemize
Scalability: Distributed data allocation allows for scaling the system by
 adding more nodes to the system, as needed.
\end_layout

\begin_layout Itemize
Reduced network traffic: By keeping data local to the nodes where it is
 most frequently accessed, data allocation can reduce the amount of network
 traffic needed to access the data.
\end_layout

\begin_layout Itemize
Better resource utilization: Data allocation can help to balance the use
 of resources across the nodes in the system, avoiding overloading some
 nodes while underutilizing others.
\end_layout

\begin_layout Subsection
Explain the benefits of replication
\end_layout

\begin_layout Standard
Data replication refers to the process of making and maintaining multiple
 copies of data across multiple nodes in a distributed database system.
 There are several benefits to data replication, including:
\end_layout

\begin_layout Itemize
Improved availability: By replicating data across multiple nodes, the system
 can continue to function even if one or more nodes fail or become unavailable,
 ensuring the availability of the data.
\end_layout

\begin_layout Itemize
Increased fault tolerance: Data replication can help to ensure that data
 remains available even in the event of a hardware or software failure,
 improving the overall fault tolerance of the system.
\end_layout

\begin_layout Itemize
Faster data access: With multiple copies of data available across multiple
 nodes, data can be accessed more quickly by users and applications, improving
 overall system performance.
\end_layout

\begin_layout Itemize
Improved load balancing: Replicating data across multiple nodes can help
 to balance the workload on each node, improving overall system performance
 and efficiency.
\end_layout

\begin_layout Itemize
Enhanced data locality: Replication can also help to improve data locality
 by ensuring that frequently accessed data is available on the same node,
 reducing the need to access data over the network.
\end_layout

\begin_layout Subsection
Discuss the alternatives of a distributed catalog
\end_layout

\begin_layout Standard
The same design problems and criteria can be applied to the catalog, but
 now we are storing metadata.
 This requires two important considerations:
\end_layout

\begin_layout Enumerate
Metadata is much smalles than data, which makes it easier to manage.
\end_layout

\begin_layout Enumerate
Optimizing performance is much more critical, since accessing this metadata
 is a requirement for any operation in the system.
\end_layout

\begin_layout Standard
Many decisions are already made by the architects of the system, and only
 few options can be parameterized on instantiaitng it.
\end_layout

\begin_layout Itemize

\series bold
Global metadata
\series default
: are allocated in the coordinator node.
\end_layout

\begin_layout Itemize

\series bold
Local metadata
\series default
: are distributed in the different nodes.
\end_layout

\begin_layout Standard
A typical choice we can make in many NOSQL systems is having a secondary
 copy of the coordinator (
\series bold
mirroring
\series default
) that takes control in case of failure.
 Of course, this redundancy consume some resources.
\end_layout

\begin_layout Subsection
Decide when a fragmentation strategy is correct
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "upc"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
