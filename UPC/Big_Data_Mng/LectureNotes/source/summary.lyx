#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{footmisc}
\usepackage{listings}
\usepackage{colortbl}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1,0.99,0.94}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,  
    frame=single,
    tabsize=2
}

\lstset{style=mystyle}
\end_preamble
\use_default_options true
\begin_modules
tcolorbox
customHeadersFooters
theorems-ams-bytype
theorems-sec-bytype
algorithm2e
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "linkcolor=blue, urlcolor=blue, citecolor=blue, pdfstartview={FitH}, unicode=true"
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\boxbgcolor #62a0ea
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 3cm
\rightmargin 2cm
\bottommargin 3cm
\headheight 2cm
\headsep 1cm
\footskip 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
BDM-MIRI - Big Data Management
\end_layout

\begin_layout Date
Spring 2023
\end_layout

\begin_layout Author
Jose Antonio Lorencio Abril
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename upc-logo.png
	scale 50

\end_inset


\end_layout

\begin_layout Standard
\align right
Professor: Alberto Abelló
\end_layout

\begin_layout Standard
\align right
Student e-mail: jose.antonio.lorencio@estudiantat..upc.edu
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Address
This is a summary of the course 
\emph on
Big Data Management
\emph default
 taught at the Universitat Politècnica de Catalunya by Professor Alberto
 Abelló in the academic year 22/23.
 Most of the content of this document is adapted from the course notes by
 Abelló and Nadal, 
\begin_inset CommandInset citation
LatexCommand cite
key "Abello2022"
literal "false"

\end_inset

, so I won't be citing it all the time.
 Other references will be provided when used.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList figure

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList table

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList algorithm

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Introduction to Big Data
\end_layout

\begin_layout Subsection
Recognise the relevance of data driven decision making
\end_layout

\begin_layout Standard

\series bold
Data driven decision making
\series default
 is the strategy of using data to make decisions, in order to improve the
 chances of obtaining a positive outcome.
 It has been gaining importance in the past years, mainly because the data
 generation rate is increasing rapidly, allowing greater analyses for those
 who are able to leverage all this data.
\end_layout

\begin_layout Standard
The ability to collect, store, combine and analyze relevant data enables
 companies to gain a competitive advantage over their competitors which
 are not able to take on these task.
\end_layout

\begin_layout Standard
In a nutshell, it is the confluence of three major socio-economic and technologi
cal trends that makes data driven innovation a new phenomenon:
\end_layout

\begin_layout Itemize
The exponential growth in data generated and collected.
\end_layout

\begin_layout Itemize
The widespread use of data analytitcs, including start-ups and small and
 medium entreprises.
\end_layout

\begin_layout Itemize
The emergence of a paradigm shift in knowledge.
\end_layout

\begin_layout Subsection
Identify the three high level categories of analytical tools
\end_layout

\begin_layout Standard

\series bold
Business Intelligence (BI)
\series default
 is the concept of using dashboard to represent the status and evolution
 of companies, using data from the different applications used by the production
 systems of the company, which needs to be processed with ETL (Extract,
 Transform, Load) pipelines into a Data Warehouse.
 This data is then modelled into data cubes, that are queried with OLAP
 (OnLine Analytic Processing) purposes.
 The analytical tools that this setup allows are three:
\end_layout

\begin_layout Enumerate
Static generation of reports.
\end_layout

\begin_layout Enumerate
Dynamic (dis)aggregation and navigation by means of OLAP operations.
\end_layout

\begin_layout Enumerate
Inference of hidden patterns or trends with data mining tools.
\end_layout

\begin_layout Subsection
Identify the two main sources of Big Data
\end_layout

\begin_layout Standard
The two main sources of Big Data are:
\end_layout

\begin_layout Itemize
The 
\series bold
Internet
\series default
, which shifted from a passive role, where static hand-crafted contents
 were provided by some gurus, to a dynamic role, where contents can be easily
 generated by anybody in the world, specially through social networks.
\end_layout

\begin_layout Itemize
The 
\series bold
improvement of automation and digitalization
\series default
 on the side of industries, which allows to monitor many relevant aspects
 of the company's scope, giving rise to the concept of Internet of Things
 (IoT), and generating a continuous flow of information.
\end_layout

\begin_layout Subsection
Give a definition of Big Data
\end_layout

\begin_layout Standard

\series bold
Big Data
\series default
 is a natural evolution of Business Intelligence, and inherits its ultimate
 goal of transforming raw data into valuable knowledge, and it can be characteri
zed in terms of the 
\series bold
five V's
\series default
:
\end_layout

\begin_layout Itemize

\series bold
Volume
\series default
: there are large amount of digital information produced and stored in new
 systems.
\end_layout

\begin_layout Itemize

\series bold
Velocity
\series default
: the pace at which data is generated, ingested and processed is very fast,
 giving rise to the concept of data stream (and two related challenges:
 
\emph on
data stream ingestion
\emph default
 and 
\emph on
data stream processing
\emph default
).
\end_layout

\begin_layout Itemize

\series bold
Variety
\series default
: there are multiple, heterogeneous data formats and schemas, which need
 to be dealt with.
 Special attention is needed for semi-structured and unstructured external
 data.
 The 
\emph on
data variety challenge
\emph default
 is considered as the most crucial challenge in data driven organizations.
\end_layout

\begin_layout Itemize

\series bold
Variability
\series default
: the incoming data can have an evolving nature, which the system needs
 to be able to cope with.
\end_layout

\begin_layout Itemize

\series bold
Veracity
\series default
: the veracity of the data is related to its quality, and it makes it compulsory
 to develop 
\emph on
Data Governance
\emph default
 practices, to effectively manage data assets.
\end_layout

\begin_layout Subsection
Compare traditional data warehousing against Big Data management
\end_layout

\begin_layout Standard
In traditional business intelligence, data from different sources inside
 the company is ETL-processed into the data warehouse, which can then be
 analyzed using the three types of analyses we've seen (Reports, OLAP, DM),
 in order to extract useful information that ultimately affects the strategy
 of the company.
 This is summarized in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Business-Intelligence-Cycle."
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 As highlighted in the figure, the data warehousing process encompasses
 the ETL processes and the Data Warehouse design and maintenance.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename BI_Cycle.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Business-Intelligence-Cycle."

\end_inset

Business Intelligence Cycle.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In the context of big data, the focus is shifted, from analyzing data from
 just inside sources, to data from all types of heterogeneous sources.
 In this setup, instead of doing an ETL process, the data is collected,
 through the process of ingestion, and stored into a Data Lake (from which
 analysts would extract data and perform all necessary transformations a
 posteriori) or a Polystore (which is a DBMS built on top of different other
 technologies, to be able to cope with heterogeneous data).
 Whatever the storing decision, Big Data Analytics are then done on this
 data, differenciating:
\end_layout

\begin_layout Itemize
Small analytics: querying and reporting the data and OLAP processing.
\end_layout

\begin_layout Itemize
Big Analytics: performing data mining on the data.
\end_layout

\begin_layout Standard
This process is depicted in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Big-Data-Cycle."
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 In this diagram, we see that the 
\series bold
Big Data Management
\series default
 consists of the task of ingestion, together with the design and maintenance
 of the Data Lake / Polystore.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename BigData_Cycle.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Big Data Cycle.
\begin_inset CommandInset label
LatexCommand label
name "fig:Big-Data-Cycle."

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Thus, the differences are:
\end_layout

\begin_layout Itemize
Data Warehousing does the ETL process over the data produced by the company,
 while Big Data Management does the process of ingestion, by which data
 from internal and external sources is collected.
\end_layout

\begin_layout Itemize
Data Warehousing uses a Data Warehouse to store the ETLed data and the analyses
 need to be designed with the structure of this stored data.
 In contrast, in Big Data Management, the storing facility can cope with
 the data as is, so the analyses have a wider scope, but they need to correctly
 treat the data for each analysis conducted.
\end_layout

\begin_layout Itemize
Thus, as can be inferred from the previous paragraphs, Big Data Management
 provides a more flexible setup than Data Warehousing, at the expense of
 needing to perform ad-hoc transformation for each analysis, which can lead
 to repetition and a decrease in performance.
 Nonetheless, this decrease is not really a drawback, because some big data
 analytics tasks cannot be undertaken without this added flexibility.
\end_layout

\begin_layout Subsection
Distinguish descriptive, predictive and prescriptive analysis
\end_layout

\begin_layout Itemize

\series bold
Descriptive analysis
\series default
: uses basic statistics to describe the data.
 In a DW environment, OLAP tools are used for this purpose, in an interactively
 manner, modifying the analysis point of vire to facilitate the understanding
 and gain knowledge about the stored data.
 Basically, understand past data (what happened, when happened, why it happened).
\end_layout

\begin_layout Itemize

\series bold
Predictive analysis
\series default
: uses a set of statistical techniques to analyze historical facts, with
 the aim of making predictions about future events.
 Basically, compare incoming data to our knowledge of past data, in order
 to make predictions about the future (what will happen).
\end_layout

\begin_layout Itemize

\series bold
Prescriptive analysis
\series default
: takes as input the predictions of previous analyses to suggest actions,
 decisions and describe the possible implications of each of them.
 Basically, use predictions obtained via predictive analysis to take action
 and make decisions, as well as to estimate the impact of these decisions
 in the future (how we should respond to this situation).
\end_layout

\begin_layout Subsection
Explain the novelty of Cloud Computing
\end_layout

\begin_layout Standard
The novelty of cloud computing is the same as when electricity shifted from
 being generated by each company to be centrally generated, benefiting from
 scale economies and improving the efficiency of the electricity generation.
 In the case of Cloud Computing, the shift is from companies having their
 own hardware and software, to an environment in which these resources are
 offered by a third company, which leverages again the economies of scale
 and the possibility to allocate resources when needed, increasing the overall
 efficiency of the tech industries and reducing the costs of each company,
 as they now don't need to buy expensive pieces of hardware and software,
 maintain them, etc.
\end_layout

\begin_layout Subsection
Justify the benefits of Cloud Computing
\end_layout

\begin_layout Itemize
It eliminates upfront investment, as it is not needed to buy hardware anymore.
\end_layout

\begin_layout Itemize
You pay for what you use, so costs are reduced because efficient allocation
 is a complex task to overcome.
\end_layout

\begin_layout Itemize
The main benefit comes from the aforementioned economy of scale, that allows
 to reduce costs and improve efficiency.
 A machine hosted in-house is most of the time underused, because companies
 don't usually require it being 100% operational all the time.
 However, when the machine is available for thousand or millions of customers,
 it will almost always be required to be working.
\end_layout

\begin_layout Itemize
Customers can adapt their costs to their needs at any time.
\end_layout

\begin_layout Itemize
There is no need to manage, maintain and upgrade hardware anymore.
\end_layout

\begin_layout Subsection
Explain the link between Big Data and Cloud Computing
\end_layout

\begin_layout Standard
Cloud computing and big data are closely related, and in many ways, cloud
 computing has enabled the growth and adoption of big data technologies.
\end_layout

\begin_layout Standard
One of the main advantages of cloud computing is its ability to provide
 flexible and scalable computing resources on demand.
 This is especially important for big data, which requires significant computing
 power to process and analyze large volumes of data.
 Cloud computing allows organizations to easily spin up large-scale computing
 clusters and storage systems to handle big data workloads, without the
 need to invest in expensive on-premises infrastructure.
\end_layout

\begin_layout Standard
In addition to providing scalable computing resources, cloud computing also
 offers a wide range of data storage and processing services that can be
 used for big data workloads.
 Cloud providers offer a variety of data storage services, such as object
 storage, file storage, and database services, that can be used to store
 and manage large volumes of data.
 Cloud providers also offer big data processing services, such as Apache
 Hadoop, Apache Spark, and machine learning tools, which can be used to
 analyze and extract insights from big data.
\end_layout

\begin_layout Standard
Cloud computing also provides the ability to easily integrate and share
 data between different systems and applications, both within an organization
 and with external partners.
 This is important for big data, which often requires data from multiple
 sources to be combined and analyzed to gain insights.
\end_layout

\begin_layout Standard
Overall, cloud computing has played a key role in enabling the growth and
 adoption of big data technologies, by providing flexible and scalable computing
 resources, a wide range of data storage and processing services, and the
 ability to easily integrate and share data between different systems and
 applications.
\end_layout

\begin_layout Subsection
Distinguish the main four service levels in Cloud Computing
\end_layout

\begin_layout Standard
The main four service levels are:
\end_layout

\begin_layout Itemize

\series bold
Infrastructure as a Service (IaaS)
\series default
: provides virtualized computing resources, such as virtual machines, storage,
 and networking, which can be provisioned and managed through an API or
 web console.
\end_layout

\begin_layout Itemize

\series bold
Platform as a Service (PaaS)
\series default
: provides a platform for building and deploying applications, including
 development tools, runtime environments, and middleware, which can be accessed
 through an API or web console.
\end_layout

\begin_layout Itemize

\series bold
Software as a Service (SaaS)
\series default
: provides access to software applications over the internet, which are
 hosted and managed by a third-party provider, and can be accessed through
 a web browser or API.
\end_layout

\begin_layout Itemize

\series bold
Business as a Service (BaaS)
\series default
: This is a type of cloud computing service that provides businesses with
 access to a range of software tools and services, such as customer relationship
 management (CRM) systems, enterprise resource planning (ERP) software,
 and human resources management tools.
 BaaS allows businesses to outsource the management and maintenance of these
 systems to a third-party provider, freeing up resources and allowing the
 business to focus on their core operations.
 BaaS can be a cost-effective way for businesses to access enterprise-level
 software tools without the need to invest in on-premises infrastructure
 and maintenance.
 This is, a whole business process is outsourced, for example using PayPal
 as a paying platform frees the company from this process.
\end_layout

\begin_layout Standard
But there are more services offered by Cloud Computing:
\end_layout

\begin_layout Itemize

\series bold
Database as a Service (DBaaS)
\series default
: specific platform services providing data management functionalities.
\end_layout

\begin_layout Itemize

\series bold
Container as a Service (CaaS)
\series default
: allows applications to be packaged into containers, which can be run consisten
tly across different environments, such as development, testing, and production.
\end_layout

\begin_layout Itemize

\series bold
Function as a Service (FaaS)
\series default
: creates small stand-alone pieces of software that can be easily combined
 to create business flows in interaction with other pieces from potentially
 other service providers.
\end_layout

\begin_layout Itemize

\series bold
Serverless computing
\series default
: allows developers to build and run applications without managing servers,
 by providing an event-driven computing model, in which code is executed
 in response to specific triggers.
\end_layout

\begin_layout Itemize

\series bold
Data analytics and storage
\series default
: provides tools for storing and analyzing large volumes of data, such as
 data warehouses, data lakes, and analytics tools, which can be accessed
 through APIs or web consoles.
\end_layout

\begin_layout Itemize

\series bold
Machine learning and artificial intelligence
\series default
: Provides tools and services for building, training, and deploying machine
 learning models, such as pre-trained models, APIs for image recognition
 and natural language processing, and tools for custom model development.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Big Data Design
\end_layout

\begin_layout Subsection
Define the impedance mismatch
\end_layout

\begin_layout Standard
Impedance mismatch often arises when data is passed between different layers
 of an application, such as between the front-end user interface and the
 back-end database, or between different applications that need to exchange
 data.
 The data structures used in each layer or system may be different, which
 can cause issues with data mapping, performance, and scalability.
\end_layout

\begin_layout Standard
For example, if a front-end application requires data that is stored in
 a relational database, the application may need to perform complex queries
 to retrieve and transform the data into a format that can be used by the
 user interface.
 This can lead to performance issues and increased complexity in the application
 code.
 Similarly, if different applications or services use different data formats
 or structures, it can be difficult to exchange data between them, which
 can lead to integration issues and increased development time.
\end_layout

\begin_layout Standard
To address impedance mismatch, software developers often use techniques
 such as object-relational mapping (ORM) to map data between different layers
 of an application, or use standard data formats such as JSON or XML to
 enable data exchange between different systems.
 These techniques can help to simplify data mapping, improve performance,
 and increase the scalability of the system.
\end_layout

\begin_layout Subsection
Identify applications handling different kinds of data
\end_layout

\begin_layout Itemize

\series bold
Relational data (OLTP)
\series default
: Relational databases are commonly used for online transaction processing
 (OLTP) applications, such as e-commerce websites, banking applications,
 and inventory management systems.
 Examples of applications that use relational databases include Oracle,
 MySQL, PostgreSQL, and Microsoft SQL Server.
\end_layout

\begin_layout Itemize

\series bold
Multidimensional data (OLAP)
\series default
: Multidimensional databases are commonly used for online analytical processing
 (OLAP) applications, such as data warehousing, business intelligence, and
 data mining.
 Examples of applications that use multidimensional databases include Microsoft
 Analysis Services, IBM Cognos, and Oracle Essbase.
\end_layout

\begin_layout Itemize

\series bold
Key-value data
\series default
: Key-value databases are commonly used for high-performance, highly scalable
 applications, such as caching, session storage, and user profiles.
 Examples of applications that use key-value databases include Redis, Amazon
 DynamoDB, and Apache Cassandra.
\end_layout

\begin_layout Itemize

\series bold
Column-family data
\series default
: Column-family databases are commonly used for applications that require
 fast reads and writes on a large-scale, such as content management systems,
 social networks, and recommendation engines.
 Examples of applications that use column-family databases include Apache
 HBase, Apache Cassandra, and ScyllaDB.
\end_layout

\begin_layout Itemize

\series bold
Graph data
\series default
: Graph databases are commonly used for applications that involve complex
 relationships between data, such as social networks, fraud detection, and
 recommendation engines.
 Examples of applications that use graph databases include Neo4j, OrientDB,
 and Amazon Neptune.
\end_layout

\begin_layout Itemize

\series bold
Document data
\series default
: Document databases are commonly used for applications that require flexible,
 dynamic data structures, such as content management systems, e-commerce
 platforms, and mobile applications.
 Examples of applications that use document databases include MongoDB, Couchbase
, and Amazon DocumentDB.
\end_layout

\begin_layout Standard
Note that many applications use multiple types of data models, depending
 on the nature of the data and the requirements of the application.
 For example, a social network might use a graph database to store social
 connections, a column-family database to store user data, and a key-value
 database to cache frequently accessed data.
\end_layout

\begin_layout Subsection
Name four different kinds of NOSQL systems
\end_layout

\begin_layout Itemize

\series bold
Key-Value
\series default
: stores data as a collection of key-value pairs.
 Each key is associated with a value, and values can be retrieved and updated
 by their corresponding keys.
 Key-value databases are simple and highly scalable, making them well-suited
 for applications that require high performance and low latency.
\end_layout

\begin_layout Itemize

\series bold
Wide-column (Column-family)
\series default
: stores data as a collection of columns grouped into column families.
 Each column family is a group of related columns, and each column consists
 of a name, a value, and a timestamp.
 Column-family databases are optimized for storing large amounts of data
 with fast writes and queries, making them well-suited for applications
 that require high write and query throughput.
 Such grouping of columns directly translates into a vertical partition
 of the table, and entails the consequent loss of schema flexibility.
\end_layout

\begin_layout Itemize

\series bold
Graph
\series default
: stores data as nodes and edges, representing complex relationships between
 data.
 Nodes represent entities, such as people, places, or things, and edges
 represent relationships between entities.
 Graph databases are optimized for querying and analyzing relationships
 between data, making them well-suited for applications that require complex
 querying.
\end_layout

\begin_layout Itemize

\series bold
Document
\series default
: stores data as documents, which can be thought of as semi-structured data
 with a flexible schema.
 Each document consists of key-value pairs, and documents can be grouped
 into collections.
 Document stores are optimized for storing and querying unstructured and
 semi-structured data, making them well-suited for applications that require
 flexibility in data modeling.
\end_layout

\begin_layout Subsection
Explain three consequences of schema variability
\end_layout

\begin_layout Standard
Schema variability refers to the dynamic and flexible nature of data models
 in NoSQL databases.
 Unlike relational databases, NoSQL databases allow for the schema to be
 flexible and adaptable, which means that the data structure can evolve
 over time without requiring changes to the database schema.
 This allows for greater agility in data modeling, as it makes it easier
 to add or remove fields or change the data structure as needed.
\end_layout

\begin_layout Standard
Its three main consequences are:
\end_layout

\begin_layout Itemize

\series bold
Gain in flexibility
\series default
: allowing schema variability makes the system more flexible to cope with
 changes in the data.
\end_layout

\begin_layout Itemize

\series bold
Reduced data semantics and consistency
\series default
: With a flexible schema, it is possible to store data that does not conform
 to a predefined structure.
 This can lead to inconsistencies in data quality and make it more difficult
 to enforce data constraints, such as data types or referential integrity.
\end_layout

\begin_layout Itemize

\series bold
Data independence principle is lost
\series default
: allowing schema variability can be seen as a departure from the traditional
 concept of data independence, which is a key principle of the relational
 data model.
 Data independence refers to the ability to change the physical storage
 or logical structure of the data without affecting the application programs
 that use the data.
 In a relational database, the data is organized into tables with fixed
 schema, which allows for greater data independence.
 
\end_layout

\begin_deeper
\begin_layout Standard
However, in NoSQL databases, schema variability is often seen as a necessary
 trade-off for achieving greater flexibility and scalability.
 By allowing for a more flexible data model, NoSQL databases can better
 accommodate changes to the data structure over time, without requiring
 changes to the database schema or application code.
 This can help improve agility and reduce development time.
 
\end_layout

\begin_layout Standard
That being said, NoSQL databases still adhere to the fundamental principles
 of data independence in many ways.
 For example, they still provide a layer of abstraction between the application
 and the physical storage of the data, which helps to insulate the application
 from changes to the underlying data storage.
 Additionally, many NoSQL databases provide APIs that allow for flexible
 querying and manipulation of data, which helps to maintain a level of data
 independence.
\end_layout

\end_deeper
\begin_layout Standard
Some more consequences:
\end_layout

\begin_layout Itemize
Increased data complexity: As the data model becomes more flexible, the
 data can become more complex and difficult to manage.
 This can lead to increased development and maintenance costs, as well as
 potential performance issues.
\end_layout

\begin_layout Itemize
Increased development and maintenance costs: As the schema becomes more
 flexible, the complexity of the data model can increase, which can result
 in higher development and maintenance costs.
\end_layout

\begin_layout Itemize
Reduced performance: With a more complex data model, queries can become
 more complex, which can result in slower query performance.
 Additionally, since the schema is not fixed, indexing and optimization
 become more difficult, which can further impact performance.
\end_layout

\begin_layout Subsection
Explain the consequences of physical independence
\end_layout

\begin_layout Standard
Physical independence is a key principle of the relational data model, which
 refers to the ability to change the physical storage of the data without
 affecting the logical structure of the data or the application programs
 that use the data.
 This means that the application should be able to access and manipulate
 the data without being aware of the underlying physical storage details,
 such as the storage medium or the location of the data.
\end_layout

\begin_layout Standard
The consequences of physical independence include:
\end_layout

\begin_layout Itemize
Reduced maintenance costs: With physical independence, it is easier to change
 the physical storage of the data without affecting the application.
 This can help reduce maintenance costs, as it allows for more flexibility
 in how the data is stored and accessed over time.
\end_layout

\begin_layout Itemize
Improved scalability: Physical independence can help improve scalability,
 as it allows for the data to be distributed across multiple physical storage
 locations or devices, which can help to improve performance and reduce
 the impact of failures.
\end_layout

\begin_layout Itemize
Greater portability: With physical independence, the application is not
 tied to a specific physical storage medium or location, which can help
 improve portability across different hardware or software platforms.
\end_layout

\begin_layout Itemize
Improved performance: Physical independence can help improve performance,
 as it allows for the data to be stored and accessed in the most efficient
 way possible, without being limited by the constraints of a specific physical
 storage medium or location.
\end_layout

\begin_layout Standard
Nonetheless, physical independence enhance the problem of the impedance
 mismatch, because if data is needed in a different form from how it is
 stored, it has to be transformed, introducing a computing overhead.
 If we store the data as needed for the application, this problem is reduced,
 but the physical independence can be lost.
\end_layout

\begin_layout Subsection
Explain the two dimensions to classify NOSQL systems according to how they
 manage the schema
\end_layout

\begin_layout Standard
The schema can be explicit/implicit:
\end_layout

\begin_layout Itemize

\series bold
Implicit schema
\series default
: schema that is not explicitly defined or documented.
 Instead, the schema is inferred or derived from the data itself, usually
 through analysis or observation.
 This can be useful in situations where the data is very dynamic or unstructured
, and where the structure of the data is not known in advance.
\end_layout

\begin_layout Itemize

\series bold
Explicit schema
\series default
: schema that is explicitly defined and documented, usually using a schema
 language or a data modeling tool.
 The schema specifies the types of data that can be stored, the relationships
 between different types of data, and any constraints or rules that govern
 the data.
\end_layout

\begin_layout Standard
And it can be fixed/variable:
\end_layout

\begin_layout Itemize

\series bold
Fixed schema
\series default
: schema that is static and unchanging, meaning that the structure and organizat
ion of the data is predefined and cannot be modified.
 This is common in relational databases, where the schema is usually defined
 in advance and remains fixed over time.
\end_layout

\begin_layout Itemize

\series bold
Variable schema
\series default
: schema that is dynamic and flexible, meaning that the structure and organizati
on of the data can change over time.
 This is common in NoSQL databases, where the schema may be more fluid and
 adaptable to changing data requirements.
\end_layout

\begin_layout Standard
Note that this is not a strict classification, but rather two dimension
 ranges in which a certain schema can lie.
\end_layout

\begin_layout Subsection
Explain the three elements of the RUM conjecture
\end_layout

\begin_layout Standard
The RUM conjecture suggests that in any database system, the overall performance
 can be characterized by a trade-off between the amount of memory used,
 the number of reads performed, and the number of updates performed.
 Specifically, the conjecture states that there is a fundamental asymmetry
 between reads and updates, and that the performance of the system is strongly
 influenced by the balance between these two operations.
 In general, the more reads a system performs, the more memory it requires,
 while the more updates it performs, the more it impacts the system's overall
 performance.
\end_layout

\begin_layout Standard
The RAM conjecture has three main elements:
\end_layout

\begin_layout Itemize
Reads: refer to the process of retrieving data from a database.
 In general, read-heavy workloads require more memory to achieve good performanc
e.
\end_layout

\begin_layout Itemize
Updates: refer to the process of modifying data in a database.
 In general, update-heavy workloads require more processing power and can
 negatively impact overall performance.
\end_layout

\begin_layout Itemize
Memory: refers to the amount of memory available to a system.
 In general, increasing memory can improve read-heavy workloads, but may
 not be as effective for update-heavy workloads.
\end_layout

\begin_layout Standard
The RUM conjecture is often used to guide the design and optimization of
 database systems, as it provides a useful framework for understanding the
 trade-offs between different system parameters and performance metrics.
 By understanding the RUM trade-offs, database designers can make informed
 decisions about how to allocate resources, optimize queries, and balance
 the workload of the system.
\end_layout

\begin_layout Subsection
Justify the need of polyglot persistence
\end_layout

\begin_layout Standard
A 
\series bold
polyglot system
\series default
 is a system that uses multiple technologies, languages, and tools to solve
 a problem.
 In the context of data management, a polyglot system is one that uses multiple
 data storage technologies to store and manage data.
 For example, a polyglot system might use a combination of relational databases,
 NoSQL databases, and search engines to store different types of data.
\end_layout

\begin_layout Standard

\series bold
Polyglot persistence
\series default
 is the practice of using multiple storage technologies to store different
 types of data within a single application.
 The idea is to choose the right tool for the job, and to use each technology
 to its fullest potential.
 For example, a polyglot system might use a NoSQL database to store unstructured
 data, a relational database to store structured data, and a search engine
 to provide full-text search capabilities.
\end_layout

\begin_layout Standard
There are several reasons why polyglot persistence is important:
\end_layout

\begin_layout Itemize

\series bold
Flexibility
\series default
: Polyglot systems are more flexible than monolithic systems that use a
 single technology to store all data.
 With a polyglot system, you can choose the right tool for the job, and
 you can adapt to changing requirements and data formats.
\end_layout

\begin_layout Itemize

\series bold
Performance
\series default
: Different data storage technologies are optimized for different types
 of data and workloads.
 By using the right tool for the job, you can improve performance and scalabilit
y.
\end_layout

\begin_layout Itemize

\series bold
Resilience
\series default
: Using multiple data storage technologies can improve the resilience of
 your system.
 If one database fails, the other databases can continue to operate, ensuring
 that your application remains available and responsive.
\end_layout

\begin_layout Itemize

\series bold
Future-proofing
\series default
: By using multiple data storage technologies, you can future-proof your
 system against changing data formats and requirements.
 As new data types and storage technologies emerge, you can add them to
 your system without having to completely overhaul your architecture.
\end_layout

\begin_layout Standard
In summary, polyglot persistence is a powerful approach to data management
 that allows you to use multiple storage technologies to store and manage
 different types of data within a single application.
 By adopting a polyglot approach, you can improve flexibility, performance,
 resilience, and future-proofing.
\end_layout

\begin_layout Subsection
Decide whether two NOSQL designs have more or less explicit/fix schema
\end_layout

\begin_layout Standard
There are several factors that can be used to assess the flexibility and
 explicitness of a schema:
\end_layout

\begin_layout Enumerate
Number of tables/collections: A schema with a large number of tables or
 collections is typically more explicit and less flexible than a schema
 with fewer tables or collections.
 This is because a large number of tables or collections often implies a
 more rigid structure, whereas a smaller number of tables or collections
 can allow for more flexibility.
\end_layout

\begin_layout Enumerate
Number of columns/fields: A schema with a large number of columns or fields
 is typically more explicit and less flexible than a schema with fewer columns
 or fields.
 This is because a large number of columns or fields often implies a more
 rigid structure, whereas a smaller number of columns or fields can allow
 for more flexibility.
\end_layout

\begin_layout Enumerate
Data types: A schema that uses a large number of data types is typically
 more explicit and less flexible than a schema that uses fewer data types.
 This is because a large number of data types often implies a more rigid
 structure, whereas a smaller number of data types can allow for more flexibilit
y.
\end_layout

\begin_layout Enumerate
Use of constraints: A schema that uses a large number of constraints (such
 as foreign keys or unique constraints) is typically more explicit and less
 flexible than a schema that uses fewer constraints.
 This is because constraints often imply a more rigid structure, whereas
 a schema with fewer constraints can allow for more flexibility.
\end_layout

\begin_layout Enumerate
Use of inheritance: A schema that uses inheritance (such as table or collection
 inheritance) is typically more flexible and less explicit than a schema
 that does not use inheritance.
 This is because inheritance allows for more flexibility in the structure
 of the data, whereas a schema that does not use inheritance is typically
 more explicit in its structure.
\end_layout

\begin_layout Standard
Overall, a more explicit schema is one that has a more rigid structure,
 with more tables, fields, data types, and constraints, whereas a more flexible
 schema is one that has fewer tables, fields, data types, and constraints,
 and may use inheritance to provide more flexibility.
\end_layout

\begin_layout Standard
Some examples can be:
\end_layout

\begin_layout Itemize
Fixed/Explicit: An example of a fixed/explicit schema in XML format might
 look like this:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset listings
lstparams "language=PHP"
inline false
status open

\begin_layout Plain Layout

<shopping_cart>
\end_layout

\begin_layout Plain Layout

  <customers>
\end_layout

\begin_layout Plain Layout

    <customer>
\end_layout

\begin_layout Plain Layout

      <name>John Smith</name>
\end_layout

\begin_layout Plain Layout

      <email>john@example.com</email>
\end_layout

\begin_layout Plain Layout

    </customer>
\end_layout

\begin_layout Plain Layout

  </customers>
\end_layout

\begin_layout Plain Layout

  <orders>
\end_layout

\begin_layout Plain Layout

    <order>
\end_layout

\begin_layout Plain Layout

      <order_date>2023-02-17</order_date>
\end_layout

\begin_layout Plain Layout

      <total_price>100.00</total_price>
\end_layout

\begin_layout Plain Layout

    </order>
\end_layout

\begin_layout Plain Layout

  </orders>
\end_layout

\begin_layout Plain Layout

  <products>
\end_layout

\begin_layout Plain Layout

    <product>
\end_layout

\begin_layout Plain Layout

      <name>Widget</name>
\end_layout

\begin_layout Plain Layout

      <description>A small, useful tool</description>
\end_layout

\begin_layout Plain Layout

      <price>10.00</price>
\end_layout

\begin_layout Plain Layout

    </product>
\end_layout

\begin_layout Plain Layout

  </products>
\end_layout

\begin_layout Plain Layout

</shopping_cart>
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this example, the schema is fixed because there are specific tables (customer
s, orders, and products) and specific fields for each table (such as name
 and email for customers, and order_date and total_price for orders).
 There is no room for variation in the structure of the schema.
\end_layout

\end_deeper
\begin_layout Itemize
Fixed/Implicit: An example of a fixed/implicit schema in XML format might
 look like this:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset listings
lstparams "language=PHP"
inline false
status open

\begin_layout Plain Layout

<blog>
\end_layout

\begin_layout Plain Layout

  <posts>
\end_layout

\begin_layout Plain Layout

    <post>
\end_layout

\begin_layout Plain Layout

      <title>My First Blog Post</title>
\end_layout

\begin_layout Plain Layout

      <content>This is my first blog post.</content>
\end_layout

\begin_layout Plain Layout

    </post>
\end_layout

\begin_layout Plain Layout

  </posts>
\end_layout

\begin_layout Plain Layout

</blog>
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this example, the schema is fixed because there is a specific table (posts)
 and specific fields for that table (such as title and content).
 However, there is no fixed field for metadata such as tags or categories.
\end_layout

\end_deeper
\begin_layout Itemize
Flexible/Explicit: An example of a flexible/explicit schema in XML format
 might look like this:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset listings
lstparams "language=PHP"
inline false
status open

\begin_layout Plain Layout

<scientific_data>
\end_layout

\begin_layout Plain Layout

  <experiments>
\end_layout

\begin_layout Plain Layout

    <experiment>
\end_layout

\begin_layout Plain Layout

      <date>2023-02-17</date>
\end_layout

\begin_layout Plain Layout

      <sample_size>100</sample_size>
\end_layout

\begin_layout Plain Layout

      <measurement_units>mg/L</measurement_units>
\end_layout

\begin_layout Plain Layout

    </experiment>
\end_layout

\begin_layout Plain Layout

    <experiment>
\end_layout

\begin_layout Plain Layout

      <date>2023-02-16</date>
\end_layout

\begin_layout Plain Layout

      <sample_size>50</sample_size>
\end_layout

\begin_layout Plain Layout

      <measurement_units>g/L</measurement_units>
\end_layout

\begin_layout Plain Layout

    </experiment>
\end_layout

\begin_layout Plain Layout

  </experiments>
\end_layout

\begin_layout Plain Layout

  <observations>
\end_layout

\begin_layout Plain Layout

    <observation>
\end_layout

\begin_layout Plain Layout

      <value>10.00</value>
\end_layout

\begin_layout Plain Layout

    </observation>
\end_layout

\begin_layout Plain Layout

    <observation>
\end_layout

\begin_layout Plain Layout

      <value>20.00</value>
\end_layout

\begin_layout Plain Layout

    </observation>
\end_layout

\begin_layout Plain Layout

  </observations>
\end_layout

\begin_layout Plain Layout

</scientific_data>
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this example, the schema is flexible because there can be any number
 of experiments and observations, and there are no fixed fields for metadata.
 However, each table (experiments and observations) and each field (such
 as date and sample_size) is explicitly defined in the schema.
\end_layout

\end_deeper
\begin_layout Itemize
Flexible/Implicit: An example of a flexible/implicit schema in XML format
 might look like this:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset listings
lstparams "language=PHP"
inline false
status open

\begin_layout Plain Layout

<social_media>
\end_layout

\begin_layout Plain Layout

  <posts>
\end_layout

\begin_layout Plain Layout

    <post>
\end_layout

\begin_layout Plain Layout

      <text>Hello world!</text>
\end_layout

\begin_layout Plain Layout

    </post>
\end_layout

\begin_layout Plain Layout

  </posts>
\end_layout

\begin_layout Plain Layout

  <comments>
\end_layout

\begin_layout Plain Layout

    <comment>
\end_layout

\begin_layout Plain Layout

      <text>Great post!</text>
\end_layout

\begin_layout Plain Layout

    </comment>
\end_layout

\begin_layout Plain Layout

    <comment>
\end_layout

\begin_layout Plain Layout

      <text>Thanks for sharing.</text>
\end_layout

\begin_layout Plain Layout

    </comment>
\end_layout

\begin_layout Plain Layout

  </comments>
\end_layout

\begin_layout Plain Layout

</social_media>
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this example, the schema is flexible because there can be any number
 of posts and comments, and there are no fixed fields for any table.
 The structure of the schema is also implicit because there is no fixed
 structure for the data.
\end_layout

\end_deeper
\begin_layout Subsection
Given a relatively small UML conceptual diagram, translate it into a logical
 representation of data considering flexible schema representation
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Distributed Data Management
\end_layout

\begin_layout Subsection
Give a definition of Distributed System
\end_layout

\begin_layout Standard
A distributed system is a system whose components, located at networked
 computers, communicate and coordinate their actions only by passing messages.
\end_layout

\begin_layout Subsection
Enumerate the six challenges of a Distributed System
\end_layout

\begin_layout Standard
The challenges of a distributed system are:
\end_layout

\begin_layout Itemize

\series bold
Scalability
\series default
: the system must be able to continuously evolve to support a grouwing amount
 of tasks.
 This can be achieve by:
\end_layout

\begin_deeper
\begin_layout Itemize
Scale up: upgrading or improving the components.
\end_layout

\begin_layout Itemize
Scale out: adding new components.
\end_layout

\begin_layout Standard
Scale out mitigates bottlenecks, but extra communication is needed between
 a growing number of components.
 This can be partially solved using direct communication between peers.
 Load-balancing is also crucial and, ideally, should happen automatically.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Performance/efficiency
\series default
: the system must guarantee an optimal performance and efficient processing.
 This is usually measured in terms of latency, response time and throughput.
 Parallelizing reduces response time, but uses more resources to do it,
 negatively affecting throughput unless resources are increased to compensate.
\end_layout

\begin_deeper
\begin_layout Standard
This effect can be mitigated optimizing network usage or using distributed
 indexes.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Reliability and availability
\series default
: the system must perform tasks consistently and without failure: it must
 be 
\series bold
reliable
\series default
.
 It also must keep performing tasks even if some of its components fail:
 it must be 
\series bold
available
\series default
.
 The availability is not always possible, and some functionalities might
 be affected, but at least a partial service could be provided when a component
 fails.
\end_layout

\begin_deeper
\begin_layout Standard
To increase failure tolerance, 
\series bold
heartbeat mechanisms
\series default
 can be used to monitor the status of the components, together with 
\series bold
automatic recovery mechanisms
\series default
.
\end_layout

\begin_layout Standard
It is also important to keep the consistency of data shared by different
 components, since this requires synchronization.
 This can be mitigated by 
\series bold
asynchronous synchronization mechanisms
\series default
 and 
\series bold
flexible routing of network messages
\series default
.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Concurrency
\series default
: the system should provide the required control mechanisms to avoid interferenc
es and deadlocks in the presence of concurrent requests.
 
\series bold
Consensus protocols
\series default
 can help solving conflicts and enabling the system to keep working without
 further consequences.
\end_layout

\begin_layout Itemize

\series bold
Transparency
\series default
: users of the system should not be aware of all the aforementioned complexities.
 Ideally, they should be able to work as if the system was not distributed.
\end_layout

\begin_layout Subsection
Give a definition of Distributed Database
\end_layout

\begin_layout Standard
A 
\series bold
Distributed Database (DDB)
\series default
 is an integrated collection of databases that is physically distributed
 across sites in a computer network and a 
\series bold
Distributed Database Management System (DDBMS)
\series default
 is the software system that manages a distributed database such that the
 distribution aspects are transparent to the users.
\end_layout

\begin_layout Standard
There are some terms worth detailing:
\end_layout

\begin_layout Itemize

\series bold
Integrated
\series default
: files in the database should be somehow structured, and an access interface
 common to all of them should be provided so that the physical location
 of data does not matter.
\end_layout

\begin_layout Itemize

\series bold
Physically distributed across sites in a computer network
\series default
: data may be distributed over large geographical areas but it could also
 be the case where distributed data is, indeed, in the very same room.
 The required characteristic is that the communication between nodes is
 done through a computer network instead of simply sharing memory or disk.
\end_layout

\begin_layout Itemize

\series bold
Distribution aspects are transparent to the users
\series default
: transparency refers to separation of the higher-level view of the system
 from lower-level implementation issues.
 Thus, the system must provide mechanisms to hide the implementation details.
\end_layout

\begin_layout Subsection
Explain the different transparency layers in DDBMS
\end_layout

\begin_layout Standard
In a DDBMS 
\series bold
distribution transparency 
\series default
must be ensured, i.e., the system must guarantee data, network, fragmentation
 and replication transparency:
\end_layout

\begin_layout Itemize

\series bold
Data independence
\series default
: data definition occurs at two different levels:
\end_layout

\begin_deeper
\begin_layout Itemize
Logical data independence: refers to indifference of user applications to
 changes in the logical structure of the database.
\end_layout

\begin_layout Itemize
Physical data independence: hides the storage details to the user.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Network transparency
\series default
: the user should be protected from the operation details of the network,
 even hiding its existence whenever possible.
 There are two subclasses:
\end_layout

\begin_deeper
\begin_layout Itemize
Location transparency: any task performed should be independent of both
 the location and system where the operation must be performed.
\end_layout

\begin_layout Itemize
Naming transparency: each object must have a unique name in the database,
 irrespectively of its storage site.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Replication transparency
\series default
: refers to whether synchronizing replicas is left to the user or automatically
 performed by the system.
 Ideally, all these issues should be transparent to users, and they should
 act as if a single copy of data were available.
\end_layout

\begin_layout Itemize

\series bold
Fragmentation transparency
\series default
: when data is fragmented, queries need to be translated from the global
 query into fragmented queries, handling each fragment.
 This translation should be performed by the DDBMS, transparently to the
 user.
\end_layout

\begin_layout Standard
Note that all these transparency levels are incremental.
\end_layout

\begin_layout Standard
Note also that full transparency makes the management of distributed data
 very difficult, so it is widely accepted that data independence and network
 transparency are a must, but replication and/or fragmentation transparency
 might be relaxed to boost performance.
\end_layout

\begin_layout Subsection
Identify the requirements that distribution imposes on the ANSI/SPARC architectu
re
\end_layout

\begin_layout Standard
The 
\series bold
Extended ANSI/SPARC
\series default
 
\series bold
architecture
\series default
was designed to provide a comprehensive framework for organizing and managing
 complex database systems.
 The extended architecture includes the same three levels as the original
 ANSI/SPARC architecture, but it adds a fourth level, called the user level.
\end_layout

\begin_layout Standard
The four levels of the extended ANSI/SPARC architecture are:
\end_layout

\begin_layout Itemize
User Level: The user level is the highest level and includes the end users
 or applications that access the database system.
 The user level provides a simplified view of the data that is available
 in the system, and it defines the interactions between the user and the
 system.
\end_layout

\begin_layout Itemize
External Level: The external level is the next level down and includes the
 external schemas that define the view of the data that is presented to
 the end users or applications.
 Each external schema is specific to a particular user or group of users
 and provides a simplified view of the data that is relevant to their needs.
\end_layout

\begin_layout Itemize
Conceptual Level: The conceptual level is the third level and includes the
 global conceptual schema that describes the overall logical structure of
 the database system.
 The global conceptual schema provides a unified view of the data in the
 system and defines the relationships between different data elements.
\end_layout

\begin_layout Itemize
Internal Level: The internal level is the lowest level and includes the
 physical schema that defines the storage structures and access methods
 used to store and retrieve the data.
 The internal schema is specific to the particular database management system
 and hardware platform that is being used.
\end_layout

\begin_layout Standard
This is summarized in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Extended-ANSI/SPARC-architecture"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ansisparc.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Extended-ANSI/SPARC-architecture"

\end_inset

Extended ANSI/SPARC architecture.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
This architecture does not consider distributed, so it need to be consequently
 adapted to provide distribution trasnparency.
 To this end, a global conceptual schema is needed to define a single logical
 database.
 But the database is composed of several nodes, each of which must now define
 a local conceptual schema and an internal schema.
 These adaptations are depicted in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Extended-ANSI/SPARC-architecture-1"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ansisparc2.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Extended ANSI/SPARC architecture with distribution.
\begin_inset CommandInset label
LatexCommand label
name "fig:Extended-ANSI/SPARC-architecture-1"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In both architectures, mappings between each layer are stored in the global
 catalog, but in the distributed architecture there two mappings which are
 particularly important, namely the 
\series bold
fragmentation schema 
\series default
and the 
\series bold
allocation schema
\series default
.
\end_layout

\begin_layout Subsection
Draw a classical reference functional architecture for DDBMS
\end_layout

\begin_layout Standard
The functional architecture of a centralized DBMS is depicted in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Functional-architecture-of"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The 
\series bold
query manager
\series default
 is a component of a database management system (DBMS) that is responsible
 for handling user queries and managing the overall query processing.
 It is composed of several sub-components:
\end_layout

\begin_layout Itemize
The 
\series bold
view manager
\series default
 is responsible for managing the views defined in the system.
 Views are virtual tables that are derived from the base tables in the database
 and are used to simplify the user's interaction with the database.
 The view manager translates user queries that reference views into queries
 that reference the base tables, allowing the user to interact with the
 database at a higher level of abstraction.
\end_layout

\begin_layout Itemize
The 
\series bold
security manager
\series default
 is responsible for enforcing security policies and access controls in the
 system.
 It ensures that only authorized users are allowed to access the database
 and that they only have access to the data that they are authorized to
 see.
 The security manager also enforces constraints and ensures that the data
 in the database is consistent and valid.
\end_layout

\begin_layout Itemize
The 
\series bold
constraint checker
\series default
 is responsible for verifying that the data in the database conforms to
 the integrity constraints defined in the schema.
 It checks for violations of primary key, foreign key, and other constraints,
 and ensures that the data in the database is consistent and valid.
\end_layout

\begin_layout Itemize
The 
\series bold
query optimizer
\series default
 is responsible for optimizing user queries to improve performance.
 It analyzes the query and determines the most efficient way to execute
 it, taking into account factors such as the available indexes, the size
 of the tables involved, and the cost of different query execution plans.
 The query optimizer generates an optimal query execution plan that minimizes
 the time required to process the query.
\end_layout

\begin_layout Standard
Once these steps are done, the 
\series bold
execution manager
\series default
 launches the different operators in the access plan in order, building
 up the results appropriately.
 
\end_layout

\begin_layout Standard
The 
\series bold
scheduler
\series default
 deals with the problem of keeping the databases in a consistent state,
 even when concurrent accesses occur, preserving isolation (I from ACID).
\end_layout

\begin_layout Standard
The 
\series bold
recovery manager
\series default
 is responsible for preserving the consistency (C), atomicity (A) and durability
 (D) properties.
 
\end_layout

\begin_layout Standard
The 
\series bold
buffer manager
\series default
 is responsible for bringing data to main memory from disk, and vice-versa,
 communicating with the operating system.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado1.png
	scale 70

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Functional architecture of a centralized DBMS.
\begin_inset CommandInset label
LatexCommand label
name "fig:Functional-architecture-of"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
This architecture is not sufficient to deal with distributed data.
 The functional architecture of a distributed DBMS (DDBMS) is depicted in
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Functional-architecture-of-1"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 As we can see, there are now two stages:
\end_layout

\begin_layout Enumerate
Modules cooperate at the global level, transforming the data flow and mapping
 it to the lower layers, dealing with a single view of the database and
 the distribution transparency:
\end_layout

\begin_deeper
\begin_layout Enumerate
The 
\series bold
global query manager
\series default
 contains the view manager, security manager, constraint checker and query
 ooptimizer, which behave as in the centralized case, except for the optimizer,
 which now considers data location and consults the global schema to determine
 which node does what.
 
\end_layout

\begin_layout Enumerate
The 
\series bold
global execution manager
\series default
 inserts communication primitives in the execution plan and coordinates
 the execution of the pieces of the query in the different components to
 build up the final results from all the query pieces executed distributedly.
\end_layout

\begin_layout Enumerate
The 
\series bold
global scheduler
\series default
 receives the global execution plan and distributes trasks between the available
 sites, guaranteeing isolation between different users.
\end_layout

\end_deeper
\begin_layout Enumerate
Modules cooperate at the local level, with a very similar behavior to that
 of the centralized DBMS.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado2.png
	scale 70

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Functional architecture of a DDBMS.
\begin_inset CommandInset label
LatexCommand label
name "fig:Functional-architecture-of-1"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Enumerate the eight main features of Cloud Databases
\end_layout

\begin_layout Itemize
Ability to scale horizontally.
\end_layout

\begin_layout Itemize
Efficient fragmentation techniques.
\end_layout

\begin_layout Itemize
Use as efficiently as possible distributed memory and indexing mechanisms
 to parallelize execution.
 Spped up relies on massive replication and parallelism (which in turn improve
 reliability and availability).
\end_layout

\begin_layout Itemize
Cloud Databases relax the strong consistency asked by ACID transactions
 and define the weaker concept of eventual consistency.
\end_layout

\begin_layout Itemize
A simplistic call level interface or protocol is provided to manage data,
 which is easy to learn and use, but puts the optimization burden on the
 side of the developers.
 This also compromises some transparency.
 The schemaless nature of these systems complicates even more the creation
 of a declarative query language like SQL.
\end_layout

\begin_layout Itemize
The setting up of hardware and software must be quick and cheap.
\end_layout

\begin_layout Itemize
The concept of 
\series bold
multi-tenancy
\series default
 appears: the same hardware/software is shared by many tenants.
 This requires mechanisms to manage the sharing and actually benefitting
 from it.
\end_layout

\begin_layout Itemize
Rigid pre-defined schemas are not appropriate for these databases.
 Instead, there is a need towards gaining flexibility.
\end_layout

\begin_layout Subsection
Explain the difficulties of Cloud Database providers to have multiple tenants
\end_layout

\begin_layout Standard
The difficulty can be summarized as the need to deal with the potential
 high number of tenants, and the unpredictability of their workloads' characteri
stics.
 Popularity of tenants can change very rapidly, fact that impacts the Cloud
 services hosting their products.
 Also, the activities that they perform can change.
\end_layout

\begin_layout Standard
Thus, the provider has to implement mechanisms to be able to deal with this
 variety and variability in the workloads.
\end_layout

\begin_layout Standard
Also, the system should tolerate failures and offer self-healing mechanisms,
 if possible.
\end_layout

\begin_layout Standard
Finally, the software should easily allow to scale out to guarantee the
 required latencies.
 Adding or upgrading new machines should happen progressively, so that service
 suspension is not necessary at all.
\end_layout

\begin_layout Subsection
Enumerate the four main problems tenants/users need to tackle in Cloud Databases
\end_layout

\begin_layout Itemize

\series bold
Data design
\series default
: provides the means to decide on how to fragment the data, where to place
 each fragment, and how many times they will be stored (replication).
\end_layout

\begin_layout Itemize

\series bold
Catalog management
\series default
: requires the same considerations as the design of the database regarding
 fragmentation, locality and replication, but with regard to metadata instead
 of data.
 The difference in this case is that some of the decisions are already made
 on designing the tool and few degrees of freedom are left for administrators
 and developers.
\end_layout

\begin_layout Itemize

\series bold
Transaction management
\series default
: it is specially hard and expensive in distributed environments.
 Distributed recovery and concurrency control mechanisms exist, but there
 is a need to find a trade-off between the security they guarantee and the
 performance impact they have.
\end_layout

\begin_deeper
\begin_layout Standard
Specially relevant in this case is the management of replicas, which are
 expensive to update, but reduce query latency and improve availability.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Query processing
\series default
: it must be as efficient as possible.
 Parallelism should benefit from data distribution, without incurring in
 much communication overhead, which can be reduced by replicating data.
\end_layout

\begin_layout Subsection
Distinguish the cost of sequential and random access
\end_layout

\begin_layout Subsection
Explain the difference between the cost of sequential and random access
\end_layout

\begin_layout Subsection
Distinguish vertical and horizontal fragmentation
\end_layout

\begin_layout Standard

\series bold
Data fragmentation
\series default
 deals with the problem of breaking datasets into smaller pieces, decreasing
 the working unit in the distributed system.
 It has been useful to reflect the fact that applications and users might
 be interested in accessing different subsets of the data.
 Different subsets are naturally needed at different nodes and it makes
 sense to allocate fragments where they are more likely to be needed for
 use.
 This is 
\series bold
data locality
\series default
.
\end_layout

\begin_layout Standard
There are two main fragmentation approaches:
\end_layout

\begin_layout Itemize

\series bold
Horizontal fragmentation
\series default
: a selection predicate is used to create different fragments and, according
 to an attribute value, place each row in the corresponding fragment.
\end_layout

\begin_deeper
\begin_layout Standard
A distributed system benefits from horizontal fragmentation when it needs
 to mirror geographically distributed data to facilitate recovery and parallelis
m, to reduce the depth of indexes and to reduce contention.
\end_layout

\begin_layout Standard
Fragmentation can go from one extreme (no fragmentation) to the other (placing
 each row in a different fragment).
 We need to know which predicates are of interest in our database.
 As a general rule: 
\emph on
the 20% most active users produce 80% of the total accesses
\emph default
.
 We should focus on these users to determine which predicates to consider
 in our analysis.
\end_layout

\begin_layout Standard
Finally, we need to guarantee the correctness:
\end_layout

\begin_layout Itemize
Completeness: the fragmentation predicates must guarantee every row is assigned
 to, at least, one fragment.
\end_layout

\begin_layout Itemize
Disjointness: the fragmentation preficates must be mutually exclusive (
\series bold
minimality property
\series default
).
\end_layout

\begin_layout Itemize
Reconstruction: the union of all the fragments must constitute the original
 dataset.
\end_layout

\begin_layout Standard
We have only considered single relations for this analysis, but it is also
 possible to consider related datasets and fragment them together, this
 is called 
\series bold
derived horizontal fragmentation
\series default
.
 Let 
\begin_inset Formula $R,S$
\end_inset

 be two relations such that 
\begin_inset Formula $R$
\end_inset

 possess a foreign key to 
\begin_inset Formula $S$
\end_inset

 and are related by means of a relationship 
\begin_inset Formula $r$
\end_inset

.
 In this case, 
\begin_inset Formula $S$
\end_inset

 is the 
\series bold
owner
\series default
 and 
\begin_inset Formula $R$
\end_inset

 is the 
\series bold
member
\series default
.
 Suppose also that 
\begin_inset Formula $S$
\end_inset

 is fragmented in 
\begin_inset Formula $n$
\end_inset

 fragments 
\begin_inset Formula $S_{i},i=1,...,n$
\end_inset

, and we want to fragment 
\begin_inset Formula $R$
\end_inset

 regarding 
\begin_inset Formula $S$
\end_inset

 using the relationship 
\begin_inset Formula $r$
\end_inset

.
 The derived horizontal fragmentation is defined as 
\begin_inset Formula 
\[
R_{i}=R\ltimes S_{i},\ i=1,...,n,
\]

\end_inset

 where 
\begin_inset Formula $\ltimes$
\end_inset

 is the left-semijoin
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Formula $R\ltimes S$
\end_inset

 pairs those tuples in 
\begin_inset Formula $R$
\end_inset

 for which there is at least one tuple in 
\begin_inset Formula $S$
\end_inset

 with matching joining key.
\end_layout

\end_inset

 and the joining attributes are those in 
\begin_inset Formula $r$
\end_inset

.
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $R$
\end_inset

 and 
\begin_inset Formula $S$
\end_inset

 are related by more than one relationship, we should apply the following
 criteria to decide which one to use:
\end_layout

\begin_layout Itemize
The fragmentation more used by users/applications.
\end_layout

\begin_layout Itemize
The fragmentation that maximizes the parallel execution of the queries.
\end_layout

\begin_layout Standard
In order to consider a derived horizontal fragmentaiton to be complete and
 disjoint, two additional constraints must hold on top of those stated before:
\end_layout

\begin_layout Itemize
Completeness: the relationship used to semijoin both datasets must enforce
 the referential integrity constraint.
\end_layout

\begin_layout Itemize
Disjointness: the join attribute must be the owner's key.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Vertical fragmentation
\series default
: partitions the datasets in smaller subsets by projecting some attributes
 in each fragment.
 
\end_layout

\begin_deeper
\begin_layout Standard
Vertical fragmentation has been traditionally overlooked in practice, because
 it worsened insertions and update timems of transactional systems in many
 times.
 However, with the arrival of read-only workloads, this kind of fragmentation
 arose as a powerful alternative to decrease the number of attributes to
 be read from a dataset.
 
\end_layout

\begin_layout Standard
In general, it improves the ratio of useful data read and it also reduces
 contention and facilitates recovery and parallelism.
\end_layout

\begin_layout Standard
As disadvantages, note that it increases the number of indexes, worsens
 update and insertion time and increases the space used by data, because
 the primery key is replicated at each fragment.
\end_layout

\begin_layout Standard
Deciding how to group attributes is not obvious at all.
 The information required is:
\end_layout

\begin_layout Itemize
Data characteristics: set of attributes and value distribution for all attribute
s.
\end_layout

\begin_layout Itemize
Workload: frequency of each query, access plan and estimated cost of each
 query and selectivity of each predicate.
\end_layout

\begin_layout Standard
A good heuristic is the following:
\end_layout

\begin_layout Enumerate
Determine primary partitions (subsets of attributes always accessed together).
\end_layout

\begin_layout Enumerate
Generate a disjoint and covering combination of primary partitions, which
 would potentially be stored together.
\end_layout

\begin_layout Enumerate
Evaluate the cost of all combinations generated in the previous phase.
\end_layout

\end_deeper
\begin_layout Subsection
Recognize the complexity and benefits of data allocation
\end_layout

\begin_layout Standard
Once the data is fragmented, we must decide where to place each segment,
 trying to optimize some criteria:
\end_layout

\begin_layout Itemize

\series bold
Minimal cost
\series default
: function resulting of computing the cost of storing each fragment 
\begin_inset Formula $F_{i}$
\end_inset

 at a certain node 
\begin_inset Formula $N_{i}$
\end_inset

, the cost of querying 
\begin_inset Formula $F_{i}$
\end_inset

 at 
\begin_inset Formula $N_{i}$
\end_inset

 and the cost of updating each fragment 
\begin_inset Formula $F_{i}$
\end_inset

 at all places where it is replicated, and the cost od communication.
\end_layout

\begin_layout Itemize

\series bold
Maximal performance
\series default
: the aim is to minimize the response time or maximize the overall throughput.
\end_layout

\begin_layout Standard
This problem is NP-hard and the optimal solution depends on many factors.
 
\end_layout

\begin_layout Standard
In a dynamic environment the workload and access patterns may change and
 all these statistics should always be available in order to find the optimal
 solution.
 Thus, the problem is simplified with certain assumptions and simplified
 cost models are built so that any optimization algorithm can be adopted
 to approximate the optimal solution.
\end_layout

\begin_layout Standard
There are several benefits to data allocation, including:
\end_layout

\begin_layout Itemize
Improved performance: By distributing the data across multiple nodes, the
 workload can be distributed among the nodes, reducing the load on any single
 node and improving overall performance.
\end_layout

\begin_layout Itemize
Increased availability: With data replicated across multiple nodes, the
 failure of any single node does not result in a loss of data or loss of
 access to the data.
\end_layout

\begin_layout Itemize
Scalability: Distributed data allocation allows for scaling the system by
 adding more nodes to the system, as needed.
\end_layout

\begin_layout Itemize
Reduced network traffic: By keeping data local to the nodes where it is
 most frequently accessed, data allocation can reduce the amount of network
 traffic needed to access the data.
\end_layout

\begin_layout Itemize
Better resource utilization: Data allocation can help to balance the use
 of resources across the nodes in the system, avoiding overloading some
 nodes while underutilizing others.
\end_layout

\begin_layout Subsection
Explain the benefits of replication
\end_layout

\begin_layout Standard
Data replication refers to the process of making and maintaining multiple
 copies of data across multiple nodes in a distributed database system.
 There are several benefits to data replication, including:
\end_layout

\begin_layout Itemize
Improved availability: By replicating data across multiple nodes, the system
 can continue to function even if one or more nodes fail or become unavailable,
 ensuring the availability of the data.
\end_layout

\begin_layout Itemize
Increased fault tolerance: Data replication can help to ensure that data
 remains available even in the event of a hardware or software failure,
 improving the overall fault tolerance of the system.
\end_layout

\begin_layout Itemize
Faster data access: With multiple copies of data available across multiple
 nodes, data can be accessed more quickly by users and applications, improving
 overall system performance.
\end_layout

\begin_layout Itemize
Improved load balancing: Replicating data across multiple nodes can help
 to balance the workload on each node, improving overall system performance
 and efficiency.
\end_layout

\begin_layout Itemize
Enhanced data locality: Replication can also help to improve data locality
 by ensuring that frequently accessed data is available on the same node,
 reducing the need to access data over the network.
\end_layout

\begin_layout Subsection
Discuss the alternatives of a distributed catalog
\end_layout

\begin_layout Standard
The same design problems and criteria can be applied to the catalog, but
 now we are storing metadata.
 This requires two important considerations:
\end_layout

\begin_layout Enumerate
Metadata is much smalles than data, which makes it easier to manage.
\end_layout

\begin_layout Enumerate
Optimizing performance is much more critical, since accessing this metadata
 is a requirement for any operation in the system.
\end_layout

\begin_layout Standard
Many decisions are already made by the architects of the system, and only
 few options can be parameterized on instantiaitng it.
\end_layout

\begin_layout Itemize

\series bold
Global metadata
\series default
: are allocated in the coordinator node.
\end_layout

\begin_layout Itemize

\series bold
Local metadata
\series default
: are distributed in the different nodes.
\end_layout

\begin_layout Standard
A typical choice we can make in many NOSQL systems is having a secondary
 copy of the coordinator (
\series bold
mirroring
\series default
) that takes control in case of failure.
 Of course, this redundancy consume some resources.
\end_layout

\begin_layout Subsection
Decide when a fragmentation strategy is correct
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Distributed Data Processing
\end_layout

\begin_layout Subsection
Explain the CAP theorem
\end_layout

\begin_layout Standard
The CAP theorem, also known as Brewer's theorem
\begin_inset Foot
status open

\begin_layout Plain Layout
See 
\begin_inset CommandInset citation
LatexCommand cite
key "brewer2000towards"
literal "false"

\end_inset

.
\end_layout

\end_inset

, is a principle that states that in a distributed system, it is impossible
 to simultaneously provide all three of the following guarantees:
\end_layout

\begin_layout Itemize

\series bold
Consistency
\series default
: Every read operation will return the most recent write or an error.
 All nodes see the same data at the same time.
\end_layout

\begin_layout Itemize

\series bold
Availability
\series default
: Every non-failing node returns a response for every request in a reasonable
 amount of time, without guaranteeing that it contains the most recent write.
\end_layout

\begin_layout Itemize

\series bold
Partition tolerance
\series default
: The system continues to operate despite arbitrary message loss or network
 failure between nodes.
\end_layout

\begin_layout Standard
According to the CAP theorem, a distributed system can only provide two
 out of these three guarantees at a time.
 In other words, a distributed system can either prioritize consistency
 and partition tolerance, consistency and availability, or availability
 and partition tolerance, but it cannot achieve all three simultaneously.
\end_layout

\begin_layout Standard
This theorem has important implications for the design and operation of
 distributed systems, as designers must carefully consider which trade-offs
 to make when choosing between consistency, availability, and partition
 tolerance.
 In larger distributed-scale systems, network partitions are given for granted.
 Thus, we must choose between consistency and availability: Either we have
 an always-consistent system that becomes temporally unavailable, or an
 always-available system that temporally shows some inconsistencies.
\end_layout

\begin_layout Subsection
Identify the 3 configuration alternatives given by the CAP theorem
\end_layout

\begin_layout Itemize

\series bold
Strong consistency
\series default
: replicas are synchonously modified and guarantee consistent query answering
 and the whole system will be declared not to be available in case of network
 partition.
\end_layout

\begin_layout Itemize

\series bold
Eventual consistency
\series default
: changes are asynchronously propagated to replicas, so answer to the same
 query depends on the replica being used.
 In case of network partition, changes will be simply delayed.
\end_layout

\begin_layout Itemize

\series bold
Non-distributed data
\series default
: connectivity cannot be lost and we can have strong consistency without
 affecting availability.
\end_layout

\begin_layout Subsection
Explain the 4 synchronization protocols we can have
\end_layout

\begin_layout Standard
There are two choices that generate four alternative configurations for
 replica synchronization management:
\end_layout

\begin_layout Itemize

\series bold
Primary/secondary versioning
\series default
:
\end_layout

\begin_deeper
\begin_layout Itemize
Primary versioning refers to a scheme where one copy of the data is designated
 as the primary copy, and all updates are made to this copy first.
 Once the primary copy is updated, the changes are propagated to secondary
 copies.
 This approach ensures that all nodes eventually receive the same data,
 but it may introduce a delay in propagation.
\end_layout

\begin_layout Itemize
In contrast, secondary versioning involves making updates to multiple copies
 simultaneously, with all nodes being able to receive updates independently.
 This approach can reduce the propagation delay, but it may increase the
 complexity of the replication process.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Eager/lazy replication
\series default
:
\end_layout

\begin_deeper
\begin_layout Itemize
Eager replication refers to a scheme where updates are propagated to all
 replicas immediately upon completion, ensuring that all nodes have the
 most recent version of the data at all times.
 This approach can be resource-intensive, as it requires significant network
 bandwidth and processing power.
\end_layout

\begin_layout Itemize
On the other hand, lazy replication involves delaying the propagation of
 updates until necessary, such as when a read request is received for a
 particular node.
 This approach can reduce the network and processing costs associated with
 replication but may lead to inconsistencies between replicas in the short
 term.
\end_layout

\end_deeper
\begin_layout Standard
These two choices give rise to four possible alternatives, depicted in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Replica-synchronization-alternat"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado3.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Replica-synchronization-alternat"

\end_inset

Replica synchronization alternatives.
 Source: 
\begin_inset CommandInset citation
LatexCommand cite
key "abiteboul_manolescu_rigaux_rousset_senellart_2011"
literal "false"

\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
a)
\end_layout

\end_inset

 A user can only modify the primary copy, and his changes are immediately
 propagated to any other existing copy (which can always be read by any
 user).
 Only after being properly propagated and changes acknowledged by all servers,
 the user receives confirmation.
 
\end_layout

\begin_layout Enumerate
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
b)
\end_layout

\end_inset

 A user can only modify the primary copy, and receives confirmation of this
 change immediately.
 His changes are eventually propagated to any other existing copy (which
 can always be read by any user).
 
\end_layout

\begin_layout Enumerate
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
c)
\end_layout

\end_inset

 A user can modify any replica, and her changes are immediately propagated
 to any other existing copy (which can always be read by any user).
 Only after being properly propagated and changes acknowledged by all servers,
 the user receives confirmation.
 
\end_layout

\begin_layout Enumerate
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
d)
\end_layout

\end_inset

 A user can modify any replica, and receives confirmation of this change
 immediately.
 His changes are eventually propagated to any other existing copy (which
 can always be read by any user).
\end_layout

\begin_layout Standard
a) and c) correspond to the traditional concept of consistency, while b)
 and d) correspond to the concept of eventual consistency.
\end_layout

\begin_layout Subsection
Explain what eventual consistency means
\end_layout

\begin_layout Standard
Eventual consistency is a concept in distributed databases that refers to
 a property of the system where all updates to a data item will eventually
 propagate to all nodes in the system and converge to a consistent state,
 given a sufficiently long period of time without updates.
\end_layout

\begin_layout Standard
In a distributed system, data is replicated across multiple nodes, and each
 node maintains a copy of the data.
 Due to network latency, nodes may have different versions of the data at
 any given time, leading to inconsistencies between replicas.
 Eventual consistency allows for these inconsistencies to exist temporarily
 until all nodes have received the updated data.
\end_layout

\begin_layout Standard
Eventual consistency does not guarantee immediate consistency between replicas,
 but it does ensure that all replicas will eventually converge to a consistent
 state.
 This property is particularly useful for distributed systems that prioritize
 availability and partition tolerance over consistency 
\begin_inset Foot
status open

\begin_layout Plain Layout
Remember the CAP theorem.
\end_layout

\end_inset

, such as in large-scale web applications or data-intensive systems.
\end_layout

\begin_layout Subsubsection
Replication management configurations
\end_layout

\begin_layout Standard
Let:
\end_layout

\begin_layout Itemize
\begin_inset Formula $N$
\end_inset

: number of replicas
\end_layout

\begin_layout Itemize
\begin_inset Formula $W$
\end_inset

: number of uncommited written replicas
\end_layout

\begin_layout Itemize
\begin_inset Formula $R$
\end_inset

: number of replicas with the same information to be read before giving
 a response
\end_layout

\begin_layout Standard
The 
\series bold
inconsistency window
\series default
 is the time during which 
\begin_inset Formula $W<N$
\end_inset

.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $W+R>N$
\end_inset

, then it is assured that some read replica would have been modified, and
 strong consistency is achieved because the user will always receive the
 updated value.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $W+R\leq N$
\end_inset

, then we cannot ensure this, and the consistency is eventual.
\end_layout

\begin_layout Standard
Some usual configurations are:
\end_layout

\begin_layout Itemize

\series bold
Fault tolerant system
\series default
: 
\begin_inset Formula $N=3,R=2,W=2$
\end_inset

.
\end_layout

\begin_layout Itemize

\series bold
Massive replication for read scaling
\series default
: 
\begin_inset Formula $N$
\end_inset

 is big and 
\begin_inset Formula $R=1$
\end_inset

.
\end_layout

\begin_layout Itemize

\series bold
Read One-Write All (ROWA)
\series default
: 
\begin_inset Formula $R=1,W=N$
\end_inset

.
 
\end_layout

\begin_layout Subsection
Enumerate the phases of distributed query processing
\end_layout

\begin_layout Enumerate
The 
\series bold
global query optimizer
\series default
 performs:
\end_layout

\begin_deeper
\begin_layout Enumerate
Semantic optimization
\end_layout

\begin_layout Enumerate
Syntactic optimization:
\end_layout

\begin_deeper
\begin_layout Enumerate
Generation of syntactic trees
\end_layout

\begin_layout Enumerate
Data localization
\end_layout

\begin_layout Enumerate
Reduction
\end_layout

\end_deeper
\begin_layout Enumerate
Global physical optimization
\end_layout

\end_deeper
\begin_layout Enumerate
Then, the 
\series bold
local query optimizer
\series default
 performs local physical optimization.
\end_layout

\begin_layout Subsection
Explain the difference between data shipping and query shipping
\end_layout

\begin_layout Standard
Data shipping and query shipping are both techniques used in distributed
 databases to improve performance and reduce network traffic.
 However, they differ in the way they handle data movement.
\end_layout

\begin_layout Standard

\series bold
Data shipping
\series default
 involves moving the data itself from one node to another node in the network
 to execute the query.
 In other words, the data is shipped to the node where the query is executed.
 This approach works well when the amount of data being moved is small,
 and the network has low latency and high bandwidth.
\end_layout

\begin_layout Standard
On the other hand, 
\series bold
query shipping
\series default
 involves shipping the query to the nodes where the data resides and executing
 the query on those nodes.
 In this approach, the network traffic is reduced because only the query
 is sent over the network, and the data remains in its original location.
 This approach works well when the data is large, and the network has high
 latency and low bandwidth.
\end_layout

\begin_layout Standard
It is possible to design 
\series bold
hybrid strategies
\series default
, in which it is dynamically decided what kind of shipping to perform.
\end_layout

\begin_layout Subsection
Explain the meaning of '
\emph on
reconstruction
\emph default
' and '
\emph on
reduction
\emph default
' in syntactic optimization
\end_layout

\begin_layout Standard

\series bold
Reconstruction
\series default
 refers to how the datasets are obtained from their fragments.
 For example, a dataset which is horizontally fragmented is reconstructed
 by means of unions.
\end_layout

\begin_layout Standard
On the other hand, 
\series bold
reduction
\series default
 refers to the process of removing redundant or unnecessary operations from
 a query without changing its semantics.
 This is achieved by applying various optimization techniques, such as eliminati
on of common sub-expressions, dead-code elimination, and constant folding,
 which can simplify the query execution plan and reduce the number of operations
 required to produce the result.
\end_layout

\begin_layout Subsection
Explain the purpose of the '
\emph on
exchange
\emph default
' operator in physical optimization
\end_layout

\begin_layout Standard
The exchange operator is used to redistribute data between nodes when it
 is needed to complete a query.
 For example, when a query involves joining two tables that are partitioned
 across multiple nodes, the exchange operator is used to redistribute the
 data so that the join can be performed locally on each node, instead of
 sending all the data to a single node for processing.
\end_layout

\begin_layout Standard
The exchange operator can be used for both horizontal and vertical partitioning.
 In horizontal partitioning, the exchange operator is used to redistribute
 the rows of a table between nodes.
 In vertical partitioning, the exchange operator is used to redistribute
 the columns of a table between nodes.
\end_layout

\begin_layout Subsection
Enumerate the 4 different cost factors in distributed query processing
\end_layout

\begin_layout Standard
The cost is the sum of the local cost and the communication cost:
\end_layout

\begin_layout Itemize
The 
\series bold
local cost
\series default
 is cost of the processing at each node, divided in:
\end_layout

\begin_deeper
\begin_layout Itemize
Cost of central unit processing, 
\begin_inset Formula $\#cycles$
\end_inset

.
\end_layout

\begin_layout Itemize
Unit cost of I/O operations, 
\begin_inset Formula $\#IOs$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
The 
\series bold
commnication cost
\series default
 is the cost due to the exchange of information between nodes for synchronizatio
n, divided in:
\end_layout

\begin_deeper
\begin_layout Itemize
Cost of initiating a message and sending a message, 
\begin_inset Formula $\#messages$
\end_inset

.
\end_layout

\begin_layout Itemize
Cost of transmitting a byte, 
\begin_inset Formula $\#bytes$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Subsection
Distinguish between response time and query time
\end_layout

\begin_layout Standard

\series bold
Query time
\series default
 (or execution time) is the time that it takes for the system to process
 a query, since it starts it execution until the results start being returned
 to the user (or are completely returned, if desirable).
\end_layout

\begin_layout Standard

\series bold
Response time 
\series default
is a wider term, that refers to the time it takes for the system since the
 user issues a query until she receives the response.
\end_layout

\begin_layout Subsection
Explain the different kinds of parallelism
\end_layout

\begin_layout Itemize

\series bold
Inter-query parallelism
\series default
 refers to the execution of multiple queries in parallel.
 This means that the queries are executed independently of each other and
 can run simultaneously on different processors or nodes in a distributed
 system.
\end_layout

\begin_layout Itemize

\series bold
Intra-query parallelism
\series default
, on the other hand, involves breaking down a single query into smaller
 parts or sub-queries that can be executed in parallel.
 This can improve query performance by allowing multiple parts of a query
 to be executed simultaneously.
\end_layout

\begin_deeper
\begin_layout Standard
Within intra-query parallelism, there are two types of parallelism:
\end_layout

\begin_layout Itemize

\series bold
Intra-operator parallelism 
\series default
refers to the parallel execution of operations within a single query operator.
 For example, if a query involves a selection operation, the selection can
 be parallelized by partitioning the data and having multiple processors
 or nodes evaluate the selection condition on different partitions in parallel.
 In the context of the process tree, it corresponds to several parts of
 the same node executing in parallel.
\end_layout

\begin_layout Itemize

\series bold
Inter-operator parallelism
\series default
 refers to the parallel execution of different query operators.
 For example, if a query involves both a selection and a join operation,
 the selection and join can be executed in parallel by having different
 processors or nodes evaluate different parts of the query plan simultaneously.
 In the context of the process tree, it corresponds to several nodes executing
 in parallel.
\end_layout

\end_deeper
\begin_layout Subsection
Identify the impact of fragmentation in intra-operator parallelism
\end_layout

\begin_layout Standard
Intra-operator parallelism is based on fragmenting data, so that the same
 operator can be executed parallely by issuing it to different fragments
 of the data.
\end_layout

\begin_layout Standard
If there is a preexistant (a priori) fragmentation, it can be used for this.
 But even if the dataset has not been previously fragmented, the DDBMS can
 fragment it on the fly to benefit from this approach.
\end_layout

\begin_layout Standard
The input of an operation can be dynamically fragmented and parallelized,
 with different strategies:
\end_layout

\begin_layout Itemize
Round Robin: This method involves distributing the data uniformly across
 multiple nodes in a circular fashion.
 Each new record is assigned to the next node in the circle, and when the
 last node is reached, the next record is assigned to the first node again.
 This type of fragmentation works well when the workload is evenly distributed
 across all nodes.
\end_layout

\begin_layout Itemize
Range: With this method, the data is partitioned based on a specific range
 of values in a column.
 For example, a column with dates could be used to partition the data into
 different time periods.
 Each node would be responsible for storing data within a certain range
 of dates.
 This method is useful when there are specific patterns in the data that
 can be used to group it.
 This approach facilitates directed searches, but needs accurate quartile
 information.
\end_layout

\begin_layout Itemize
Hash: This method involves taking a hash value of a column and using it
 to determine which node the data should be stored on.
 The hash function should distribute data evenly across all nodes, and it
 should be consistent so that the same value always hashes to the same node.
 This type of fragmentation works well when the workload is unpredictable,
 and there is no specific pattern to the data.
 This approach allows directed searches, but performance depends on the
 hash function chosen.
\end_layout

\begin_layout Standard
If dynamic fragmentation is used, a new property containing information
 about the fragmentation strategy being used, the fragmentation predicates
 and the number of fragments produced must be added to the process tree.
\end_layout

\begin_layout Subsection
Explain the impact of tree topologies (i.e.
 linear and bushy) in inter-operator parallelism
\end_layout

\begin_layout Standard
A
\series bold
 linear query plan
\series default
, also known as a pipeline, consists of a series of operators that are executed
 in a linear sequence.
 In this topology, inter-operator parallelism is limited because the output
 of one operator must be fully consumed by the next operator before it can
 begin processing its input.
 This means that the degree of parallelism is limited by the slowest operator
 in the pipeline.
\end_layout

\begin_layout Standard
On the other hand, a 
\series bold
bushy query plan
\series default
 consists of multiple subtrees that can be executed in parallel.
 In this topology, operators are arranged in a more complex structure that
 allows for more inter-operator parallelism.
 For example, two independent subtrees can be executed in parallel, with
 the results of each subtree combined in a later operation.
\end_layout

\begin_layout Standard
In general, a bushy query plan is more amenable to inter-operator parallelism
 than a linear query plan.
 However, the degree of parallelism that can be achieved depends on many
 factors, including the number of available processing resources, the characteri
stics of the data being processed, and the specifics of the query being
 executed.
\end_layout

\begin_layout Standard
Linear trees can be exploited with parallelism by pipelining, which consists
 in the creation of a chain of nested iterators, having one of them per
 operator in the process tree.
 The system pulls from the root iterator, which transitively propagates
 the call through all other iterators in the pipeline.
 This does not allow parallelism per se, but it can if we add a buffer to
 each iterator, so they can generate next rows without waiting for a parent
 call.
 Thus, the producer leaves its result in an intermediate bugger and the
 consumer takes its content asynchronously.
 This buffers imply that stalls can happen when an operator becomes ready
 and no new input is available in its input buffer, propagating the stall
 to the rest of the chain.
\end_layout

\begin_layout Subsection
Explain the limits of scalability
\end_layout

\begin_layout Standard

\series bold
Amdahl's law
\series default
 states that
\begin_inset Formula 
\[
S\left(p,N\right)=\frac{1}{\left(1-p\right)+\frac{p}{N}},
\]

\end_inset

 where 
\begin_inset Formula $S$
\end_inset

 is the maximum improvement reachable by parallelizing the system, 
\begin_inset Formula $N$
\end_inset

 is the number of subsystems, and 
\begin_inset Formula $p$
\end_inset

 is the fraction of parallelizable work of the system.
\end_layout

\begin_layout Standard
This is generalized by the 
\series bold
universal scalability law
\series default
, which states that
\begin_inset Formula 
\[
C\left(\sigma,\kappa,N\right)=\frac{N}{1+\sigma\cdot\left(N-1\right)+\kappa\cdot N\left(N-1\right)},
\]

\end_inset

 where 
\begin_inset Formula $C$
\end_inset

 is the maximum improvement reachable by parallelizing the system, 
\begin_inset Formula $N$
\end_inset

 is the number of subsystems, 
\begin_inset Formula $\sigma$
\end_inset

 is the system's contention or the non-parallelizable fraction work of the
 system, and 
\begin_inset Formula $\kappa$
\end_inset

 is the system's consistency delay, which models how much the different
 parallel units require communication.
\end_layout

\begin_layout Standard
Thus, scalability is limited by:
\end_layout

\begin_layout Itemize
The number of useful subsystem.
\end_layout

\begin_layout Itemize
The fraction of parallelizable work.
\end_layout

\begin_layout Itemize
The need for communication between subsystems.
\end_layout

\begin_layout Subsection
Given the overall number of machines in the cluster, identify the consistency
 problems that arise depending on the configuration of the number of required
 replicas read and written to confirm the corresponding operations
\end_layout

\begin_layout Subsection
Given a parallel system and a workload, find the number of machines maximizing
 throughput
\end_layout

\begin_layout Subsection
Estimate the cost of a distributed query
\end_layout

\begin_layout Subsection
Given a query and a database design, recognize the difficulties and opportunitie
s behind distributed query processing
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Hadoop Distributed File System (HDFS)
\end_layout

\begin_layout Subsection
Recognize the need of persistent storage
\end_layout

\begin_layout Standard
Persistent storage is important because it allows data to be stored and
 accessed even after a system or application has been shut down or restarted.
 Without persistent storage, data would be lost each time the system or
 application is shut down, which is clearly not desirable in most cases.
\end_layout

\begin_layout Subsection
Enumerate the design goals of GFS
\end_layout

\begin_layout Itemize

\series bold
Efficient file management
\series default
: optimized for large files.
\end_layout

\begin_layout Itemize

\series bold
Efficient append
\series default
: because the main access pattern is assumed to be 
\series bold
Write Once, Read Many (WORM)
\series default
, and updates are usually done by appending new data, rather than overwriting
 existing ones.
\end_layout

\begin_layout Itemize

\series bold
Multi-client
\series default
.
\end_layout

\begin_layout Itemize

\series bold
Optimal sequential scans
\series default
: useful to quickly read large files.
\end_layout

\begin_layout Itemize

\series bold
Resilience failure
\series default
: failures must be monitored and detected, and when they happen there must
 be recovery mechanisms in place to revert potential inconsistencies.
\end_layout

\begin_layout Subsection
Explain the structural components of HDFS
\end_layout

\begin_layout Standard
The architecture is 
\series bold
Coordinator-Worker
\series default
:
\end_layout

\begin_layout Itemize

\series bold
Coordinator node
\series default
: responsible for tracking the available state of the cluster and managing
 the worker nodes.
 In HDFS it is called 
\series bold
namenode
\series default
 (in Google File System (GFS) it is called master).
\end_layout

\begin_layout Itemize

\series bold
Worker nodes
\series default
: those doing the actual work.
 In HDFS they are called 
\series bold
datanodes
\series default
 (in GFS they are called chunknodes).
\end_layout

\begin_layout Standard
They have the following characteristics:
\end_layout

\begin_layout Itemize

\series bold
Files are splitted into chunks
\series default
: a 
\series bold
chunk
\series default
 is the minimal unit of distribution.
 The chunk size can be customized per file.
\end_layout

\begin_layout Itemize

\series bold
Chunks can be replicated
\series default
: to guarantee 
\series bold
robustness
\series default
, each replica must be stored in a different datanotes, and the amount of
 replicas to store is also customizable.
 If the cluster cannot replicate a chunk for the required value, the chunk
 is said to be 
\series bold
underreplicated
\series default
.
\end_layout

\begin_layout Itemize

\series bold
In-memory namespace
\series default
: thousands of client applications should be served with minimal overhead.
 Thus, to reduce lookup costs, the data structure containing the file hierarchy
 and references to their chunks resides in memory in the coordinator node.
\end_layout

\begin_layout Itemize

\series bold
Bi-directional communication
\series default
: the namenode and datanotes have mechanisms to send both data and control
 messages between them.
 Datanodes can also send control messages to each other, but in most cases
 they will be exchanging chunks of data.
\end_layout

\begin_layout Itemize

\series bold
Single point of failure
\series default
: as the coordination is done by a single done, the situation is that of
 a 
\series bold
single point of failure (SPOF)
\series default
: if the namenode fails, then the whole cluster becomes unavailable.
 To overcome this, it is commong to maintain a 
\series bold
failover replica
\series default
 (or mirror).
\end_layout

\begin_layout Standard
A depiction of the architecture is in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:HDFS-Architecture.-Source:"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pegado4.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:HDFS-Architecture.-Source:"

\end_inset

HDFS Architecture.
 Source: 
\begin_inset CommandInset href
LatexCommand href
name "HDFS-Architecture"
target "https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html"
literal "false"

\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Name three file formats in HDFS and explain their differences
\end_layout

\begin_layout Itemize

\series bold
Horizontal Layout
\series default
: data is stored in separate files based on some logical grouping or partitionin
g criteria, such as by date or by user.
 Each file contains records that are self-contained and do not span multiple
 files.
 This layout is best suited for situations where data is constantly being
 appended or modified, as it allows for efficient data access and manipulation
 without the need to scan the entire dataset.
\end_layout

\begin_layout Itemize

\series bold
Vertical Layout
\series default
: In the vertical layout, data is stored in different files, dividing it
 into its columns.
 This layout is optimized for situations where data is read and processed
 in a column-wise manner.
 It allows for efficient compression and encoding of data, and enables quick
 access to specific fields without the need to scan the entire dataset.
\end_layout

\begin_layout Itemize

\series bold
Hybrid Layout
\series default
: The hybrid layout combines aspects of both the horizontal and vertical
 layouts.
 Data is partitioned into separate files based on some logical grouping,
 and they are also separated into their columns.
 This layout provides the benefits of both the horizontal and vertical layouts,
 allowing for efficient data access and manipulation while also enabling
 column-wise processing.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="5">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top" width="3cm">
<column alignment="center" valignment="top" width="4cm">
<column alignment="center" valignment="top" width="3cm">
<column alignment="center" valignment="top" width="4cm">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Format
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Description
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Pros
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Cons
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Use Cases
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
SequenceFile
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
Simple binary file format consisting of key-value pairs.
\end_layout

\begin_layout Plain Layout
(
\color blue
Horizontal layout
\color inherit
)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Compact and efficient for large files
\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Good for streaming
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Not as flexible as other formats
\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Lacks some compression options
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Log files
\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Sensor data
\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Web server logs
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Avro
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
Data serialization system with a compact binary format and support for schema
 evolution
\end_layout

\begin_layout Plain Layout
(
\color blue
Horizontal layout
\color inherit
)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Supports rich data types and schema evolution
\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Can be used with many programming languages
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- May not be as efficient as some other formats
\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Requires a schema
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Data exchange between Hadoop and other systems
\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Machine learning applications
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Zebra
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
Table-based format for structured data with support for indexing and filtering
\end_layout

\begin_layout Plain Layout
(
\color red
Vertical layout
\color inherit
)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Provides indexing and filtering capabilities
\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Efficient for join operations
\end_layout

\begin_layout Plain Layout
- Good for projection-based workloads
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Limited support for compression
\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- May not be as flexible as other formats
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Data warehousing
\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- OLAP
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
ORC
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
Optimized Row Columnar format for storing Hive tables with support for compressi
on and predicate pushdown
\end_layout

\begin_layout Plain Layout
(
\color green
Hybrid layout
\color inherit
)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Efficient for analytical workloads
\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Supports predicate pushdown for filtering
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Requires schema
\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Not as widely supported as some other formats
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Data warehousing
\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Hive tables
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Parquet
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
Columnar file format with support for nested data and compression, optimized
 for query performance
\end_layout

\begin_layout Plain Layout
(
\color green
Hybrid layout
\color inherit
)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Supports nested data types and compression
\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Efficient for analytical queries
\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Supports predicate pushdown for filtering
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Not as efficient for write-heavy workloads
\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- May require more memory
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Analytics
\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Data warehousing
\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- Machine learning
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Pros and cons of each data format of HDFS.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="9" columns="6">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell multirow="3" alignment="center" valignment="middle" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Feature
\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Horizontal
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Vertical
\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Hybrid
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
SequenceFile
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Avro
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Zebra
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
ORC
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Parquet
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Schema
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
No
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Column Pruning
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
No
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
No
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Predicate Pushdown
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
No
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
No
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
No
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Metadata
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
No
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
No
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
No
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Nested Recrods
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
No
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
No
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Compression
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Encoding
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
No
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
No
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Yes
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Comparison of data formats of HDFS.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Column pruning 
\series default
involves eliminating unnecessary columns from the result set of a query
 before executing the query.
 This is done by analyzing the query and determining which columns are required
 to satisfy the query.
 The optimizer then generates a plan that only includes the necessary columns,
 reducing the amount of I/O and CPU processing required to execute the query.
 Column pruning is particularly useful in queries that involve large tables
 with many columns, as it can significantly reduce the amount of data that
 needs to be scanned.
\end_layout

\begin_layout Standard

\series bold
Predicate pushdown
\series default
 involves pushing down filtering conditions into the storage layer, rather
 than applying the filters after reading the data into memory.
 This is done by analyzing the query and determining which predicates can
 be evaluated at the storage layer before the data is read into memory.
 The storage layer then applies the predicates before returning the data
 to the query engine.
 This can significantly reduce the amount of data that needs to be read
 into memory, reducing the I/O and CPU processing required to execute the
 query.
 Predicate pushdown is particularly useful in queries that involve large
 tables with many rows, as it can significantly reduce the amount of data
 that needs to be read from disk.
\end_layout

\begin_layout Subsection
Recognize the importance of choosing the file format depending on the workload
\end_layout

\begin_layout Standard
As we have seen, each format provides a different set of features, which
 will affect the overall performance when retrieving the data from disk.
 There are heuristic rules to decide the most suitable file format depending
 on the kind of query to be executed:
\end_layout

\begin_layout Itemize
SequenceFile: if the dataset has exactly two columns.
\end_layout

\begin_layout Itemize
Parquet: if the first operation of the data flow aggregates data or projects
 some of the columns.
\end_layout

\begin_layout Itemize
Avro: if the first operation of the data flow scans all columns, or performs
 other kinds of operations such as joins, distinct, or sort.
\end_layout

\begin_layout Subsection
Explain the actions of the coordinator node in front of chunkserver failure
\end_layout

\begin_layout Standard
The coordinator is in charge of the detection of failures and tolerance.
 Periodically, the namenode receives 
\series bold
heartbeat messages
\series default
 from the datanodes.
 If a namenode systematically fails to send heartbeats, then the namenode
 assumes that the datanote is unavailable and corrective actions must be
 taken.
 The namenode:
\end_layout

\begin_layout Enumerate
Looks up the file namespace to find out what replicas were maintained in
 the lost chunkserver.
\end_layout

\begin_layout Enumerate
This missing replicas are fetched from the other datanodes maintaining them.
\end_layout

\begin_layout Enumerate
They are copied to a new datanode to get the system back to a robust state.
\end_layout

\begin_layout Subsection
Explain a mechanism to avoid overloading the master node in HDFS
\end_layout

\begin_layout Standard
The strategy is based on caching metadata in the client, and works as follows:
\end_layout

\begin_layout Enumerate
The first time a file is requested, the client applications must request
 the information from the namenode.
\end_layout

\begin_layout Enumerate
The namenode instructs the corresponding datanodes to send the appropriate
 chunks.
 They are chosen according to the closeness in the network to the client,
 optimizing bandwidth.
\end_layout

\begin_layout Enumerate
The datanodes send the chunks composing the file to the client application.
\end_layout

\begin_layout Enumerate
The client is now able to read the file.
 But it also keeps the locations of all the chunks in a cache.
\end_layout

\begin_layout Enumerate
If the client needs the same file, it does not need to ask the namenode,
 but can request directly to the datanodes whose information was cached
 before.
\end_layout

\begin_layout Standard
The set of caches in the clients can be seen a strategy for fragmentation
 and replication of the catalog.
\end_layout

\begin_layout Subsection
Explain how data is partitioned and replicated in HDFS
\end_layout

\begin_layout Itemize
Balancing allows HDFS to have a great performance working with latge datasets,
 since any read/write operation exploits the parallelism of the cloud.
 This balancing is donde by 
\series bold
randomly distributing
\series default
 the data chunks into different servers.
\end_layout

\begin_deeper
\begin_layout Standard
Having more blocks per node in the cluster increases the probability that
 the blocks will be better balanced over the nodes, but even using 40 blocks
 per node, the coefficient of variation is still big (more than 10%).
 To correct skewed distributions, Hadoop offers the 
\series bold
Balancer
\series default
, which examines the current cluster load distribution and based on a threshold
 parameter, it redistributes blocks across cluster nodes to achieve better
 balancing.
 Exceeding the threshold in either way would mean that the node is rebalanced.
 In addition to periodically fixing the distribution of an HDFS cluster
 in use, the Balancer is also useful when changing the cluster's topology.
\end_layout

\end_deeper
\begin_layout Itemize
The 
\series bold
replication factor
\series default
 is a value that indicates how many replicas of each file must be maintained
 and it can be customized globally or at the level of file.
 If it is not possible to maintain the replication factor, the system informs
 the user about the specific chunks that are 
\series bold
underreplicated
\series default
.
\end_layout

\begin_deeper
\begin_layout Standard

\series bold
Stale replicas
\series default
 can appear either because a worker was down and its blocks were not updated,
 or due to a failure of a write operation.
 Detecting this replicas is crucial to maintain consistency.
 For this, the namenode maintains block version numbers to distinguish up-to-dat
e and stale replicas.
 When the namenode aims to append new data to a file, it updates the block
 version numbers associated to the file's blocks.
\end_layout

\end_deeper
\begin_layout Subsection
Transaction management
\end_layout

\begin_layout Standard
HDFS applies a 
\series bold
eager/primary-copy strategy
\series default
 for replica synchronization:
\end_layout

\begin_layout Itemize
Writing can only happen on the primary-copy and the replicas are blocked
 until they are synchronized:
\end_layout

\begin_deeper
\begin_layout Enumerate
The client communicates to the namenode that it wants to write data into
 a file.
 It communicates the target path and how many chunks it wants to write.
\end_layout

\begin_layout Enumerate
The coordinator annotates how many replicas are to be stored, and where
 they will be stored.
 This information is sent back to the client.
\end_layout

\begin_layout Enumerate
The client writes the 
\series bold
primary replica
\series default
 into a datanode (the 
\series bold
primary datanode
\series default
 or primary chunkserver).
 Datanodes talk to each other, sending replicas of the chunk until a consistent
 state is reached.
\end_layout

\begin_layout Enumerate
When all chunks have been written, the client sends a commit message to
 the primary replica.
\end_layout

\begin_layout Enumerate
The primary replica commits the changes to all other replicas.
\end_layout

\begin_layout Enumerate
The rest of the replicas confirm the commit.
\end_layout

\begin_layout Enumerate
The primary replica acknowledge the changes to the client.
\end_layout

\end_deeper
\begin_layout Subsection
Recognize the relevance of sequential read
\end_layout

\begin_layout Standard
Sequential reads heavily benefit from 
\series bold
data locality
\series default
, which is mostly ignored by random access.
\end_layout

\begin_layout Standard
If we use a rotating disk, then the cost basically depdens on three components:
\end_layout

\begin_layout Itemize
Seek time: time to position the arm.
\end_layout

\begin_layout Itemize
Rotation time: average time waiting for the disk to spin until the head
 reaches the right sector.
\end_layout

\begin_layout Itemize
Data transfer: which depends on the bandwidth.
\end_layout

\begin_layout Standard
The different between sequential and random acces is that random access
 does not find the data together, so its cost is:
\begin_inset Formula 
\[
C_{RA}=n\cdot\left(seek+rotation+transfer\right),
\]

\end_inset

 while for sequential access, seek and rotation only accounts once:
\begin_inset Formula 
\[
C_{SA}=seek+rotation+n\cdot transfer.
\]

\end_inset

 Moreover, sequential acces.s pattern makes the next read absolutely predictable
 with 
\series bold
pre-fetching
\series default
, maximizing the effective read ratio by benefitting from the multiples
 layers of caching.
 In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Memory-caching-diagram."
plural "false"
caps "false"
noprefix "false"

\end_inset

, there is a diagram of how memory caching works.
 The closer to the disk, the more capacity the memory has, but with higher
 latency.
 The closer to the CPU, the faster and smaller the memory is.
 Thus, finding data in the top levels of cache is hard, but crucial to gain
 performance.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename diagram-MemoryCache-400x400-1.png
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Memory-caching-diagram."

\end_inset

Memory caching diagram.
 Source: 
\begin_inset CommandInset href
LatexCommand href
name "Hazelcast Glossary of Terms"
target "https://hazelcast.com/glossary/memory-caching/"
literal "false"

\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Choose the format for an HDFS file based on heuristics
\end_layout

\begin_layout Subsection
Estimate the data retrieved by scan, projection and selection operations
 in SequenceFile, Avro and Parquet
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
HBase
\end_layout

\begin_layout Subsection
Give the definition of the BigTable data model
\end_layout

\begin_layout Standard
A 
\series bold
BigTable
\series default
 is a sparse, distributed, persistent, multi-dimensional, sorted map:
\end_layout

\begin_layout Itemize
Sparse: few keys have an associated value.
\end_layout

\begin_layout Itemize
Distributed: enabling cluster parallelism.
\end_layout

\begin_layout Itemize
Persistent: data is stored in disk (using HDFS as underlying technology).
\end_layout

\begin_layout Itemize
Multi-dimensional: the values have columns.
\end_layout

\begin_layout Itemize
Sorted: lexicographically by the primary key.
\end_layout

\begin_layout Itemize
Map: a map is a data structure that associates a unique key to each value
 (record).
\end_layout

\begin_layout Subsection
Explain what a map structure is
\end_layout

\begin_layout Standard
A 
\series bold
map data structure
\series default
 is a collection of key-value pairs that allows fast and flexible access
 to its elements based on keys.
 Maps can store different types of keys and values, such as numbers, strings,
 objects, etc.
 Maps are useful for storing associations between two objects or values.
\end_layout

\begin_layout Subsection
Explain the difference between a Key-Value and a Wide-Column store
\end_layout

\begin_layout Standard
A 
\series bold
key-value store
\series default
 is a simple model that stores data as pairs of keys and values.
 A 
\series bold
wide-column store
\series default
 is a more complex model that stores data as rows and columns, where each
 row can have different columns.
 Some differences between them are:
\end_layout

\begin_layout Itemize
A key-value store can only query data by key, while a wide-column store
 can query data by row or column.
\end_layout

\begin_layout Itemize
A key-value store has no schema, every key has one value, that can be of
 any form.
\end_layout

\begin_layout Itemize
A wide-column store has a flexible schema, where every row can have different
 columns.
 Each of the columns is schema-less.
\end_layout

\begin_layout Itemize
A key-value store is suitable for simple lookups, while a wide-column store
 is suitable for analytical queries.
 
\end_layout

\begin_layout Standard
Some examples of key-value stores are Redis, etcd and Memcached.
 Some examples of wide-column stores are Cassandra, HBase and ScyllaDB.
\end_layout

\begin_layout Subsection
Enumerate the main schema elements of HBase
\end_layout

\begin_layout Itemize

\series bold
Table
\series default
: a table is a collection of one or more column families.
\end_layout

\begin_layout Itemize

\series bold
Column family
\series default
: a column family is a group of columns that share a common prefix and storage
 options.
 A 
\series bold
qualifier
\series default
 is an internal field of the columns.
 It is set at the row-level, so it is not fixed and can vary from column
 to column and from row to row.
\end_layout

\begin_layout Itemize

\series bold
Row
\series default
: a row is a collection of column values that are identified by a unique
 row key.
\end_layout

\begin_layout Itemize

\series bold
Cell
\series default
: a cell is the value of a column in a row.
 Each cell can contain multiple versions of the same data:
\end_layout

\begin_deeper
\begin_layout Itemize
Each version is identified by a timestamp, which can be assigned explicitly
 or automatically.
\end_layout

\end_deeper
\begin_layout Example
An example
\end_layout

\begin_layout Example
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Users
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Row key
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ColumnFamily1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ColumnFamily2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Personal
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Address
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Alice
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
name: Alice Smith, email: alice@upc.es
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
street: Avinguda Diagonal, city: Barcelona
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Bob
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
name: Bob Jones
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
street: Gran Vía, city: Murcia
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Charlie
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
name: Charlie Brown, email: charlie@ch.ch
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Example
If we wanted to recreate this example in HBase:
\end_layout

\begin_layout Example
\begin_inset listings
lstparams "language=SQL"
inline false
status open

\begin_layout Plain Layout

--0 Open the HBase shell
\end_layout

\begin_layout Plain Layout

--1 Create the table Users with column families Personal and Address
\end_layout

\begin_layout Plain Layout

CREATE TABLE 'Users', 'Personal', 'Address'
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

--2 Insert records
\end_layout

\begin_layout Plain Layout

PUT 'Users', 'Alice', 'Personal:name', 'Alice Smith', 'Personal:email',
 'alice@upc.es', 'Address:street', 'Avinguda Diagonal'
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

--3 We can add more values (we forgot the city!)
\end_layout

\begin_layout Plain Layout

PUT 'Users', 'Alice', 'Address:city', 'Barcelona'
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

--4 Same for the rest
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

--5 To read a value
\end_layout

\begin_layout Plain Layout

GET 'Users', 'Alice'
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Explain the main operations available of HBase
\end_layout

\begin_layout Itemize
CREATE TABLE <tablename>, <colf1>,...,<colfn>: Creates a table with defined
 column families.
\end_layout

\begin_layout Itemize
DESCRIBE <tablename>: Describes the schema of the specified table, including
 its column families and any compression or encoding settings.
\end_layout

\begin_layout Itemize
ALTER <tablename>, <params>: Used to modify the schema of an existing table,
 such as adding or removing column families, changing compression or encoding
 settings, or modifying table-level properties.
\end_layout

\begin_layout Itemize
COUNT <tablename>: Counts the number of rows in the specified table.
\end_layout

\begin_layout Itemize
EXISTS <tablename> [, <params>]: Used to check whether a table or column
 family exists in HBase or not.
\end_layout

\begin_layout Itemize
DISABLE <tablename>: Disables the specified table, i.e., the table is taken
 offline and no further reads or writes are allowed.
\end_layout

\begin_layout Itemize
ENABLE <tablename>: Enables a disabled table.
\end_layout

\begin_layout Itemize
DROP <tablename>: Permanently deletes the specified table.
\end_layout

\begin_layout Itemize
PUT <tablename>, <rowkey> [, <columns>]: Puts a new record with the specified
 key and columns in the specified table.
\end_layout

\begin_layout Itemize
GET <tablename>, <rowkey> [, <columns>]: Retrieves the data for the specified
 row and columns from the specified table.
\end_layout

\begin_layout Itemize
DELETE <tablename>, <rowkey> [,<columns>]: Deletes the specified row or
 columns from the specified table.
\end_layout

\begin_layout Itemize
SCAN <tablename> [, <columns>]: Scans the specified table and retrieves
 all rows that match the specified criteria.
\end_layout

\begin_layout Itemize
LIST: Lists all the tables in the HBase cluster.
 
\end_layout

\begin_layout Itemize
EXIT: Used to exit the HBase shell.
 When you execute this command, the shell will close and you will be returned
 to the command prompt.
\end_layout

\begin_layout Itemize
STATUS [{summary | simple | detailed}]: Used to display the status of the
 HBase cluster, including the number of servers and regions, the average
 load, and the cluster ID.
\end_layout

\begin_layout Itemize
SHUTDOWN: Used to shut down the HBase cluster.
 This command will stop all HBase daemons and bring down the HBase cluster.
 You should use this command with caution, as it will result in the loss
 of any data that has not been flushed to disk.
\end_layout

\begin_layout Subsection
Enumerate the main functional components of HBase
\end_layout

\begin_layout Subsection
Explain the role of the different functional components in HBase
\end_layout

\begin_layout Itemize

\series bold
Region Servers
\series default
 
\series bold
(HRegionServer)
\series default
: HBase stores data in regions, which are subsets of a table.
 Each region is served by a region server, which is responsible for serving
 read and write requests for that region.
\end_layout

\begin_layout Itemize

\series bold
HMaster
\series default
: The HMaster is the coordinator node for the cluster.
 It manages the metadata for the HBase tables, including region assignment
 and load balancing.
\end_layout

\begin_layout Itemize

\series bold
ZooKeeper
\series default
: HBase relies on ZooKeeper for coordination and synchronization of distributed
 processes.
 ZooKeeper is used to elect the HMaster and to store metadata for HBase.
\end_layout

\begin_layout Itemize

\series bold
HDFS
\series default
: HBase stores its data in Hadoop Distributed File System (HDFS).
 HDFS provides scalable and fault-tolerant storage for HBase data.
 HBase uses two types of HDFS files:
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
HFile
\series default
: regular data files containing column data.
\end_layout

\begin_layout Itemize

\series bold
HLog
\series default
: region's log files, that allow flush/fsync for small append-style writes.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
HBase Client
\series default
: The HBase client is used to interact with the HBase cluster.
 It provides APIs for creating, reading, updating, and deleting data in
 HBase.
 
\end_layout

\begin_layout Subsection
Explain the tree structure of data in HBase
\end_layout

\begin_layout Standard
In HBase, data is stored in a tree structure that is composed of regions,
 stores, and memstores.
\end_layout

\begin_layout Itemize

\series bold
Regions
\series default
: A region is a subset of a table that contains a range of contiguous rows.
 HBase automatically splits regions as they grow in size and merges regions
 as they shrink, in order to maintain a balanced distribution of data across
 the cluster.
\end_layout

\begin_layout Itemize

\series bold
Stores
\series default
 
\series bold
(StoreFile)
\series default
: A store is a physical storage unit that is associated with a region and
 contains a set of column families.
 Each store is responsible for storing the data for one or more column families.
 StoreFiles are divided by HDFS in chunks.
 
\end_layout

\begin_layout Itemize

\series bold
MemStores
\series default
: A MemStore is an in-memory data structure that is associated with a store
 and contains a sorted map of key-value pairs.
 When a client writes data to HBase, it is initially written to the memstore.
 Once the MemStore reaches a certain threshold, it is flushed to disk as
 a new store file.
 Usually, the size of a MemStore is 128 MB.
\end_layout

\begin_layout Standard
Within each store, data is stored in a column-oriented fashion, with all
 values for a given column stored together.
 This allows for efficient access to columns and column families, and also
 enables compression and other optimization techniques.
\end_layout

\begin_layout Standard
Thus, a StoreFile is a file of HFile format, consisting on several HDFS
 chunks of size 128 MB, which are structured into HBase blocks of size 64
 KB.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
StoreFile 
\series default
(HFile format)
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="3" alignment="center" valignment="middle" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
128 MB
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64 KB
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64 KB
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64 KB
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64 KB
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64 KB
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64 KB
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64 KB
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64 KB
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
...
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="3" alignment="center" valignment="middle" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
128 MB
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64 KB
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64 KB
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64 KB
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64 KB
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64 KB
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64 KB
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64 KB
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64 KB
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
...
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
A StoreFile.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Explain the 3 basic algorithms of HBase
\end_layout

\begin_layout Standard
HBase employs three basic algorithms to manage the storage and retrieval
 of data: flush, minor compaction, and major compaction.
\end_layout

\begin_layout Itemize

\series bold
Flush
\series default
: When a client writes data to HBase, it is initially written to an in-memory
 data structure called the memstore.
 Once the memstore reaches a certain threshold, HBase will flush it to disk
 as a new store file, a SSTable.
 This process is known as a flush.
 Flushing is important to ensure that data is written to disk in a timely
 manner and to free up memory for new writes.
 Flushing generates different disk versions of the same record.
\end_layout

\begin_layout Itemize

\series bold
Minor compaction
\series default
: Over time, as data is written and deleted from HBase, SSTables can become
 fragmented and contain empty or deleted cells.
 To address this, HBase periodically performs a minor compaction, which
 merges smaller SSTables together and removes any empty or deleted cells.
 This helps to optimize data storage and improve query performance.
 This process runs regularly in the background.
 Note that this operation does not remove all record versions (only some).
\end_layout

\begin_layout Itemize

\series bold
Major compaction
\series default
: In addition to minor compaction, HBase also performs periodic or, most
 often, manually triggered major compactions, which merge all SSTables for
 a given region into a single SSTable and remove any deleted cells.
 Major compactions are more resource-intensive than minor compactions, but
 they help to further optimize data storage and improve query performance.
 After a major compaction occurs, all versions of the records are merged
 into one (so they can be seen as a consistency checkpoint of the region).
\end_layout

\begin_layout Standard
Both minor and major compactions are configurable, and the frequency and
 timing of these operations can be adjusted based on the workload and resource
 availability of the HBase cluster.
\end_layout

\begin_layout Subsection
Explain the main components and behavior of an LSM-tree
\end_layout

\begin_layout Standard
The LSM-tree (Log-Structured Merge Tree) is a data structure that is used
 in many modern distributed databases, including HBase, Cassandra, and LevelDB.
 The LSM-tree is designed to provide efficient and scalable read and write
 performance for large datasets.
 The main components and behavior of an LSM-tree are:
\end_layout

\begin_layout Itemize

\series bold
Log-structured storage
\series default
: In an LSM-tree, new data is initially written to a write-ahead log, which
 is an append-only file that is optimized for sequential writes.
 The log provides durability for the data, ensuring that it is safely stored
 on disk in the event of a crash.
\end_layout

\begin_layout Itemize

\series bold
MemTable
\series default
: In addition to the write-ahead log, an LSM-tree also includes an in-memory
 data structure called the MemTable.
 As new data is written to the database, it is first written to the MemTable.
 Once the memtable becomes full or reaches a certain size threshold, it
 is flushed to disk as a new SSTable.
 This MemStore holds the most recent updates sorted by key, enabling for
 fast lookups.
\end_layout

\begin_layout Itemize

\series bold
Sorted string table (SSTable)
\series default
: An SSTable is a persistent, sorted data structure that contains a subset
 of the data in the database.
 SSTables are created through the flushing of the MemTable and are organized
 as a series of key-value pairs, sorted by key.
 These, as mentioned previously, may contain different versions of the same
 row.
\end_layout

\begin_layout Itemize

\series bold
Merge and compaction
\series default
: Over time, the database accumulates many SSTables, which can become fragmented
 and inefficient to query.
 To address this, LSM-trees periodically merge and compact SSTables together,
 creating a new, more efficient SSTable that contains a subset of the data
 in the original SSTables.
 This process is typically performed as a background process, with the merged
 data being written to a new, compacted SSTable.
\end_layout

\begin_layout Itemize

\series bold
Bloom filters
\series default
: LSM-trees often use Bloom filters to improve read performance.
 A Bloom filter is a probabilistic data structure that is used to test whether
 a key is present in an SSTable without actually reading the data.
 Bloom filters can be used to avoid costly disk seeks for keys that are
 not present in the database.
\end_layout

\begin_layout Standard
As can has been outlined, there are two main maintenance operations:
\end_layout

\begin_layout Itemize
The in-memory structure reaches the threshold:
\end_layout

\begin_deeper
\begin_layout Enumerate
Take next in memory leafs.
\end_layout

\begin_layout Enumerate
Flush them to an SSTable.
\end_layout

\end_deeper
\begin_layout Itemize
On triggering a compaction:
\end_layout

\begin_deeper
\begin_layout Enumerate
Take 
\begin_inset Formula $n$
\end_inset

 SSTables and merge them.
\end_layout

\begin_layout Enumerate
Put the merge in an in-memory buffer.
\end_layout

\begin_layout Enumerate
If buffer size exceeds chunk size:
\end_layout

\begin_deeper
\begin_layout Enumerate
Write one chunk to disk
\end_layout

\begin_layout Enumerate
Purge buffer
\end_layout

\begin_layout Enumerate
Keep exceeds in the buffer
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection
Compare a distributed tree against a hash structure of data
\end_layout

\begin_layout Standard
A distributed tree and a hash structure are two different approaches to
 organizing data in a distributed system, and each has its own strengths
 and weaknesses.
\end_layout

\begin_layout Standard
A 
\series bold
distributed tree
\series default
, such as HBase's region-based storage model, uses a hierarchical structure
 to organize data.
 The tree is partitioned into regions, with each region containing a range
 of contiguous row keys.
 Each region is stored on a separate server, allowing the system to scale
 horizontally by adding more servers as needed.
 This approach is well-suited for read-heavy workloads where data is frequently
 accessed based on its key, as it allows for efficient range scans and lookups
 of individual keys.
 However, it can be less efficient for write-heavy workloads, as writes
 may require updating multiple nodes in the tree.
\end_layout

\begin_layout Standard
A 
\series bold
hash structure
\series default
, on the other hand, uses a non-hierarchical approach to organizing data,
 with each item in the structure being assigned a unique key based on a
 hash function.
 This allows for efficient storage and retrieval of data based on its key,
 as the hash function can be used to quickly locate the relevant data without
 requiring a hierarchical lookup.
 Hash structures are well-suited for write-heavy workloads, as they can
 be designed to minimize the number of nodes that need to be updated for
 each write.
 However, they may be less efficient for range scans or other types of queries
 that require traversing large amounts of data.
\end_layout

\begin_layout Standard
In general, the choice between a distributed tree and a hash structure will
 depend on the specific requirements of the application and the workload
 it needs to support.
 Both approaches have their strengths and weaknesses, and the best choice
 will depend on factors such as the size and structure of the data, the
 expected read and write patterns, and the performance and scalability requireme
nts of the system.
\end_layout

\begin_layout Subsection
Justify the need of dynamic hashing
\end_layout

\begin_layout Standard

\series bold
Dynamic hashing
\series default
 is a technique used in database management systems to handle collisions
 that can occur when multiple keys are hashed to the same index in a hash
 table.
 This technique involves adjusting the size of the hash table dynamically
 as the number of keys increases, to maintain a low collision rate and ensure
 efficient access to the data.
\end_layout

\begin_layout Standard
There are several reasons why dynamic hashing is necessary:
\end_layout

\begin_layout Itemize

\series bold
Efficient use of memory
\series default
: If the hash table is too small, collisions will occur frequently, leading
 to degraded performance.
 On the other hand, if the hash table is too large, it may waste memory.
 Dynamic hashing allows the system to adjust the size of the hash table
 dynamically, ensuring that it is large enough to handle the data, but not
 so large that it wastes memory.
\end_layout

\begin_layout Itemize

\series bold
Scalability
\series default
: Dynamic hashing is essential for systems that need to handle large and
 growing datasets.
 As the size of the data increases, the hash table needs to be resized to
 maintain performance.
 Dynamic hashing allows this resizing to happen automatically, without requiring
 manual intervention.
\end_layout

\begin_layout Itemize

\series bold
Flexibility
\series default
: Dynamic hashing allows the system to adjust the size of the hash table
 based on the specific needs of the workload.
 For example, if the workload is read-heavy, the system may allocate more
 memory to the hash table to improve read performance.
 If the workload is write-heavy, the system may allocate less memory to
 the hash table to improve write performance.
\end_layout

\begin_layout Itemize

\series bold
Collision avoidance
\series default
: Collisions can cause performance degradation and may even lead to incorrect
 results if they are not handled properly.
 Dynamic hashing helps avoid collisions by resizing the hash table as needed
 to ensure that the data is evenly distributed across the available slots.
\end_layout

\begin_layout Standard
Without dynamic hashing, the functioning is to have a hash function 
\begin_inset Formula $f\left(x\right)$
\end_inset

 and an assignation function 
\begin_inset Formula $h\left(x\right)$
\end_inset

.
 The hash function distributes the values into a determined range, ideally
 uniformly, while the assignation function decides in which region the record
 should be stored.
 A typically used assignation function is
\begin_inset Formula 
\[
h\left(x\right)=f\left(x\right)\mod\#servers
\]

\end_inset

 Note, nonetheless, that in this case, adding a new server implies modifying
 the assignation function, which implies communicating the new function
 to all servers, as well as a massive data transfer.
 Thus, the importance of dynamic hashing.
\end_layout

\begin_layout Standard
Another challenge, is that any access must go through the hash directory.
\end_layout

\begin_layout Example
Let's see what could happen with an example.
 Imagine we have three nodes, and they are working normal, until this point
 is reached:
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
N0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
N1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
N2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
9
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
11
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_deeper
\begin_layout Example
If now we want to add a new machine, because we need more resources, the
 situation after the restructuration would be as follows:
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
N0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
N1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
N2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
N3
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color red
3
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color red
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color red
5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color red
6
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color red
7
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color red
8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color red
9
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color red
10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color red
11
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color blue
12
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
Here, all red-colored records have been moved.
 We see how this situation entails high transfer costs.
\end_layout

\begin_layout Subsubsection
Linear hash
\end_layout

\begin_layout Standard

\series bold
Linear Hashing
\series default
 is a dynamic hashing technique that enables a hash table to grow or shrink
 dynamically as the number of keys increases or decreases.
 The hash table is divided into a series of buckets, each of which can hold
 one or more keys.
 When the number of keys in a bucket exceeds a certain threshold, the bucket
 is split, and a new bucket is created to hold the overflow keys.
 
\end_layout

\begin_layout Standard
We maintain a 
\series bold
pointer
\series default
 to the next bucket to split, and two hash functions are considered.
 We take 
\begin_inset Formula $n$
\end_inset

 such thath 
\begin_inset Formula $2^{n}\leq\#servers<2^{n+1}$
\end_inset

 and use the functions 
\begin_inset Formula $h_{1}\left(x\right)=x\mod2^{n}$
\end_inset

 and 
\begin_inset Formula $h_{2}\left(x\right)=x\mod2^{n+1}$
\end_inset

.
 When a bucket overflows, the pointed bucket splits.
\end_layout

\begin_layout Subsubsection
Consistent hash
\end_layout

\begin_layout Standard

\series bold
Consistent Hashing
\series default
 is a technique used to distribute data across multiple nodes in a cluster.
 It involves mapping each node and data item to a point on a circle, and
 using the position of the item on the circle to determine which node it
 should be stored on.
 When a node is added or removed from the cluster, the items that were previousl
y assigned to that node need to be redistributed across the remaining nodes.
 In this case, we choose the hash function to lie on a range that is large
 enough to cope with all our possible values.
 The circle arrangement means that, to determine the node in which to store
 an object, we do the following:
\end_layout

\begin_layout Itemize
Compute 
\begin_inset Formula $h\left(x\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Take the server 
\begin_inset Formula $j$
\end_inset

 such that 
\begin_inset Formula $id_{j-1}<h\left(x\right)\leq id_{j}$
\end_inset

.
\end_layout

\begin_layout Itemize
Store 
\begin_inset Formula $h\left(x\right)$
\end_inset

 in 
\begin_inset Formula $j$
\end_inset

.
\end_layout

\begin_layout Subsection
Explain the structure of the HBase catalog
\end_layout

\begin_layout Standard
The HBase catalog is a set of internal tables that HBase uses to store metadata
 about the tables and regions in the cluster.
 The catalog is managed by the 
\series bold
HMaster
\series default
 daemon and provides information about the location of regions, which servers
 are serving them, and which versions of the data are available.
\end_layout

\begin_layout Standard
The catalog consists of the following tables:
\end_layout

\begin_layout Itemize

\series bold
ROOT table
\series default
: The ROOT table is the first table accessed during startup and contains
 information about the location of the META table.
 It is always located on the first region server in the cluster and is stored
 in memory.
\end_layout

\begin_layout Itemize

\series bold
META table
\series default
: The META table contains information about the regions in the HBase cluster,
 including their start and end keys, the region server hosting the region,
 and the replicas for each region.
 This information is used by HBase clients to locate the regions that contain
 the data they need.
\end_layout

\begin_layout Itemize
Namespace table: The namespace table contains information about the namespaces
 in the HBase cluster, including their name and any associated configuration
 settings.
\end_layout

\begin_layout Itemize
Quota table: The quota table contains information about resource quotas
 for tables, namespaces, and users in the HBase cluster.
\end_layout

\begin_layout Itemize
ACL table: The ACL table contains information about access control lists
 (ACLs) for tables and namespaces in the HBase cluster.
\end_layout

\begin_layout Standard
All of these tables are HBase tables themselves, and are stored in the same
 way as other tables in the cluster, with regions split across the available
 region servers.
 The ROOT and META tables are special in that they are stored in memory
 on the first region server in the cluster, and are not split into regions
 like other tables.
\end_layout

\begin_layout Standard
The catalog is an essential component of the HBase architecture, as it provides
 the means for clients to locate the regions that contain the data they
 need, and for HBase to manage the distribution and replication of data
 across the cluster.
\end_layout

\begin_layout Standard
Thus, the main structure is a three-level structure, consisting on the ROOT
 table, the META table and the data itself.
\end_layout

\begin_layout Subsection
Explain the mistake compensation mechanism of the cache in HBase client
\end_layout

\begin_layout Standard
HBase implements a client cache similar to that of HDFS to avoid constantly
 disturbing the coordinator.
 Thus, only the first time a key is requested by an application, the request
 needs to go down the tree structure of metadata.
 Eventually, some RegionServer will send the corresponding data to the client,
 and this will take note in the cache of who did this.
 In successive requests, that key will be found in the client cache and
 the request will be directly addressed to the right RegionServer.
\end_layout

\begin_layout Standard
The tree structure of HBase is more volatile than that of HDFS directory,
 as well as multilevel; two facts that complicate its management.
 It can happen that when the application finds the key in the cache and
 requests it to the RegionServer, this one does not have that key anymore.
 In this case, the RegionServer itself scales the request up the tree structure
 to its parent.
\end_layout

\begin_layout Standard
In the worst case, they key would not be under the parent either, and this
 will propagate the request to the RootRegion.
 Since the RootRegion has the information of the whole domain of keys, it
 is guaranteed that it will be able to forward the request down the tree
 to the appropriate MetaRegion, and this to the user RegionServer that now
 has the key, which will directly send the corresponding value to the client.
 Since we are assuming a three-level tree, this compensation actions will
 require at most four extra calls between the different RegionServers.
\end_layout

\begin_layout Subsection
Enumerate the ACID guarantees provided by HBase
\end_layout

\begin_layout Itemize

\series bold
Atomicity
\series default
: only guaranteed at row level.
 The classical concept of transaction affecting different rows does not
 exist in HBase.
\end_layout

\begin_layout Itemize

\series bold
Consistency
\series default
: HBase does not provide any kind of integrity constraints.
 Replication management at the disk level completely relies on that provided
 by HDFS underneath.
\end_layout

\begin_deeper
\begin_layout Itemize
Configuration of persistent data is eager/primary copy.
\end_layout

\begin_layout Itemize
Synchronization of MemStores is lazy/primary copy.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Isolation
\series default
: it offers the 
\series bold
read commited ISO isolation level
\series default
 by locking all families at once for the same row.
 There is not any way to wrap a set of multi-row operations into a single
 unit.
 Consequently, there is not any guarantee of snapshot isolation, since during
 the execution of the scan rows can appear or dissapear, and the result
 may not either correspond to the state before nor after the operation.
\end_layout

\begin_layout Itemize

\series bold
Durability
\series default
: before confirming any modification in a table, the operation is annotated
 in the log file, which guarantees it is never lost.
 Thus, it follows a Write Ahead Log protocol.
\end_layout

\begin_layout Subsection
Explain the execution flow of an HBase query both at global and local levels
\end_layout

\begin_layout Standard
The execution flow of an HBase query can be divided into two levels: global
 and local.
\end_layout

\begin_layout Standard

\series bold
Global leve
\series default
l:
\end_layout

\begin_layout Enumerate
Client sends a query to the HBase RegionServer that owns the row key or
 range of row keys being queried.
\end_layout

\begin_layout Enumerate
When a query is received by a HBase RegionServer, the Yet Another Resource
 Negotiator (YARN) resource manager is consulted to determine if there are
 available resources to handle the query.
\end_layout

\begin_layout Enumerate
The RegionServer consults the HBase catalog to locate the region(s) that
 contain the data being queried.
 This allows for inter-query parallelism, as different queries can go to
 different regions.
 Moreover, if read replicas are enabled, read-only queries can be further
 parallelized.
\end_layout

\begin_layout Enumerate
The RegionServer forwards the query to each of the relevant region servers
 that are serving the region(s) containing the data.
 Here, intra-query parallelism is possible if the domain of the keys is
 appropriately set.
 Nonetheless, in general this is not possible.
\end_layout

\begin_layout Enumerate
Each RegionServer performs the query locally and sends the results back
 to the RegionServer that received the query.
\end_layout

\begin_layout Enumerate
The RegionServer aggregates the results from the different regions and sends
 the final result back to the client.
\end_layout

\begin_layout Standard

\series bold
Local level
\series default
:
\end_layout

\begin_layout Enumerate
When a query is received by a region server, it consults its own in-memory
 cache to check if the required data is already present in memory.
\end_layout

\begin_layout Enumerate
If the required data is not present in memory, the region server reads the
 required data from disk and caches it in memory for future use.
\end_layout

\begin_layout Enumerate
The region server performs the required operations on the cached data and
 returns the result to the RegionServer that received the query.
\end_layout

\begin_layout Enumerate
If required, the region server may write any changes back to disk after
 the query is complete.
\end_layout

\begin_layout Subsection
Given few queries, define the best logical structure of a table considering
 its physical implications in terms of performance
\end_layout

\begin_layout Subsection
Given the data in two leafs of a Log-Structured Merge-tree (LSM-tree), merge
 them
\end_layout

\begin_layout Subsection
Given the current structure of a Linear Hash, modify it according to insertions
 potentially adding buckets
\end_layout

\begin_layout Standard
To modify the Linear Hash structure to handle insertions potentially adding
 buckets, first we need some definitions:
\end_layout

\begin_layout Itemize

\series bold
Already split buckets
\series default
: buckets before the pointed one.
 In these we use 
\begin_inset Formula $h_{2}$
\end_inset

.
\end_layout

\begin_layout Itemize

\series bold
To be split buckets
\series default
: buckets between the pointer and 
\begin_inset Formula $2^{n}$
\end_inset

.
 In these we use 
\begin_inset Formula $h_{1}$
\end_inset

.
\end_layout

\begin_layout Itemize

\series bold
Created
\series default
: buckets between 
\begin_inset Formula $2^{n}$
\end_inset

 and the end.
 In these we use 
\begin_inset Formula $h_{2}$
\end_inset

.
\end_layout

\begin_layout Standard
When we receive a new record, we follow the steps:
\end_layout

\begin_layout Enumerate
We compute 
\begin_inset Formula $h_{1}\left(x\right)$
\end_inset

.
 If the correspondent bucket is already split, we take 
\begin_inset Formula $h_{2}\left(x\right)$
\end_inset

.
\end_layout

\begin_layout Enumerate
If 
\begin_inset Formula $h_{i}\left(x\right)$
\end_inset

 is not full, we introduce record 
\begin_inset Formula $x$
\end_inset

 in it.
\end_layout

\begin_layout Enumerate
Else, we split the pointed bucket, and create a new 
\series bold
overflow bucket
\series default
 (if it does not exist before) connected to the bucket that is full.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
We insert 
\begin_inset Formula $x$
\end_inset

 in this overflow bucket.
\end_layout

\begin_layout Enumerate
We update the values of the split bucket, taking 
\begin_inset Formula $h_{2}$
\end_inset

 to its values and moving them as needed.
\end_layout

\end_deeper
\begin_layout Standard
Eventually, we will have space to allocate the new records.
\end_layout

\begin_layout Example
Let's see one example.
 We have the following situation:
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Buckets
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
rowcolor{cyan}
\end_layout

\end_inset

B0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2,4
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
B1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3,5
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_deeper
\begin_layout Example
Here, colour blue indicates the pointed bucket.
 Now, we want to insert key 9.
 Then, the result would be:
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Buckets
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
B0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
rowcolor{cyan}
\end_layout

\end_inset

B1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3,5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Overflow
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
9
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
rowcolor{green}
\end_layout

\end_inset

B2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
Say we now insert 11.
 Then:
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Buckets
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
rowcolor{cyan}
\end_layout

\end_inset

B0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
B1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5,9
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
B2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
rowcolor{green}
\end_layout

\end_inset

B3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
As we have splitted all the initial buckets, we increase 
\begin_inset Formula $n$
\end_inset

 by 1 and reset the pointer to bucket 0.
 
\end_layout

\begin_layout Subsection
Given the current structure of a Consistent Hash, modify it in case of adding
 a bucket
\end_layout

\begin_layout Standard
If we add a bucket, 
\begin_inset Formula $k$
\end_inset

, to the current structure of a Consistent Hash, it will be located between
 two buckets, 
\begin_inset Formula $j$
\end_inset

 and 
\begin_inset Formula $j-1$
\end_inset

.
 Thus:
\end_layout

\begin_layout Itemize
Objects between 
\begin_inset Formula $id_{k}$
\end_inset

 and 
\begin_inset Formula $id_{j}$
\end_inset

 stay in bucket 
\begin_inset Formula $j$
\end_inset

.
\end_layout

\begin_layout Itemize
Objects between 
\begin_inset Formula $id_{j-1}$
\end_inset

 and 
\begin_inset Formula $id_{k}$
\end_inset

 are moved to bucket 
\begin_inset Formula $k$
\end_inset

.
\end_layout

\begin_layout Example
Imagine a range large enough is 41, and we take 
\begin_inset Formula $h\left(x\right)=x\mod41$
\end_inset

.
 We have the following structure:
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
BucketId
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Objects
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2,5,7
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
16
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8,13,14
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
32
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
18,19,21,25,27,29
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
40
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
34,37,38
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_deeper
\begin_layout Example
We want to add a new node, with id 24:
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
BucketId
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Objects
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2,5,7
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
16
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8,13,14
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
24
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
18,19,21
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
32
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
25,27,29
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
40
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
34,37,38
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Subsection
Calculate the number of round trips needed in case of mistake compensation
 of the tree metadata
\end_layout

\begin_layout Subsection
Use HBase shell to create a table and access it
\end_layout

\begin_layout Subsection
Use HBase API to create a table and access it
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Document Stores: MongoDB
\end_layout

\begin_layout Subsection
Explain the main difference between key-value and document stores
\end_layout

\begin_layout Standard
Document stores are essentially key-value stores, but in which the value
 is a document, i.e., it follows a known syntax.
 This allows to define secondary indexes.
\end_layout

\begin_layout Standard
In a 
\series bold
key-value store
\series default
, data is stored as a collection of key-value pairs, where each key is unique
 and each value is associated with that key.
 These stores are optimized for high-speed access and retrieval of data
 based on a specific key.
 They are often used for simple data retrieval tasks, such as caching or
 storing user session data.
\end_layout

\begin_layout Standard
On the other hand, 
\series bold
document stores
\series default
 are designed to handle more complex data structures, such as documents
 or JSON objects.
 In a document store, data is stored as collections of documents, which
 can contain nested data structures and arrays.
 These stores are optimized for more complex data retrieval and querying
 operations, making them a better choice for applications that require more
 advanced data processing capabilities, such as content management systems
 or e-commerce platforms.
\end_layout

\begin_layout Subsection
Explain the main resemblances and differences between XML and JSON documents
\end_layout

\begin_layout Standard
XML and JSON are both widely used data interchange formats that are used
 to represent and transmit structured data between different systems.
 While both formats serve a similar purpose, they have some key differences
 and similarities.
\end_layout

\begin_layout Standard

\series bold
Similarities
\series default
:
\end_layout

\begin_layout Itemize
Both XML and JSON can represent complex data structures, including nested
 elements and attributes.
 
\end_layout

\begin_layout Itemize
Both formats are widely supported by programming languages and frameworks.
 
\end_layout

\begin_layout Itemize
Both formats can be used to represent data in a human-readable and machine-reada
ble format.
 
\end_layout

\begin_layout Itemize
Both formats support Unicode, making it possible to represent data in different
 languages.
 
\end_layout

\begin_layout Standard

\series bold
Differences
\series default
:
\end_layout

\begin_layout Itemize
XML is a markup language, while JSON is a data interchange format.
 This means that XML has a more complex syntax, including tags and attributes,
 while JSON uses a simpler syntax consisting of key-value pairs and arrays.
 
\end_layout

\begin_layout Itemize
XML requires a closing tag for every element, while JSON uses braces to
 define the beginning and end of a data object.
 
\end_layout

\begin_layout Itemize
XML is more verbose than JSON, which makes it less efficient in terms of
 file size and network transfer time.
 
\end_layout

\begin_layout Itemize
JSON is generally easier to parse and manipulate programmatically, which
 makes it more popular for web-based applications.
 
\end_layout

\begin_layout Subsection
Explain the design principle of documents
\end_layout

\begin_layout Standard
The design principle of document stores is based on the idea of storing
 data as documents rather than in a relational table structure.
\end_layout

\begin_layout Standard
To be able to solve the impedance mismatch problem, documents break the
 1NF.
 This avoids joins, so that we can get data needed with one single fetch,
 and use indexes to identify finer data granularity.
\end_layout

\begin_layout Subsection
Name 3 consequences of the design principle of a document store
\end_layout

\begin_layout Itemize

\series bold
Massive denormalization
\series default
: Document stores typically denormalize data to a greater extent than traditiona
l relational databases.
 This means that instead of splitting data into separate tables, a document
 store may store all of the data for a single entity (such as a customer)
 in a single document.
 This can result in larger documents, but it also makes querying and updating
 the data more efficient.
\end_layout

\begin_layout Itemize

\series bold
Independent documents
\series default
: In a document store, each document is self-contained and independent.
 This means that each document can contain all of the data needed for a
 specific operation, rather than requiring joins or additional queries to
 fetch related data from other tables.
 This can make it easier to work with data in a document store, but it can
 also result in duplicated data across multiple documents.
\end_layout

\begin_layout Itemize

\series bold
Massive rearrangement of documents on changing the application layout
\series default
: One consequence of the independent nature of documents in a document store
 is that changes to the structure of an application (such as adding or removing
 fields) may require significant rearrangement of documents.
 For example, if a new field is added to a document, all existing documents
 may need to be updated to include that field, which can be a time-consuming
 process.
 This can make it more difficult to maintain consistency across the data
 in a document store over time.
\end_layout

\begin_layout Subsection
Explain the difference between relational foreign keys and document references
\end_layout

\begin_layout Standard
In a relational database, 
\series bold
foreign keys
\series default
 are used to establish relationships between tables.
 A foreign key is a field in one table that refers to the primary key of
 another table.
 For example, if you have a table of customers and a table of orders, you
 might have a foreign key field in the orders table that refers to the primary
 key of the customers table.
 This allows you to associate orders with customers and perform queries
 that join the two tables based on the foreign key relationship.
\end_layout

\begin_layout Standard
In a document store, 
\series bold
document references
\series default
 are used to establish relationships between documents.
 A document reference is a field in one document that refers to the ID of
 another document.
 For example, if you have a collection of blog posts and a collection of
 comments, you might have a document reference field in each comment document
 that refers to the ID of the blog post it relates to.
 This allows you to associate comments with blog posts and perform queries
 that fetch all comments for a given blog post.
\end_layout

\begin_layout Standard
The 
\color red
main difference
\color inherit
 between relational foreign keys and document references is that foreign
 keys are based on a strict, predefined schema, while document references
 allow for more flexible and dynamic data models.
 In a relational database, the schema is fixed and you must define foreign
 keys between tables before data can be inserted.
 In a document store, the schema is more flexible, and you can add document
 references to establish relationships between documents as needed.
\end_layout

\begin_layout Standard

\color red
Another difference
\color inherit
 is that foreign keys are typically used to enforce referential integrity
 between tables, which means that you can't insert a row in the orders table
 unless the corresponding customer exists in the customers table.
 Document references, on the other hand, do not enforce referential integrity
 in the same way, and it's possible to have document references to non-existent
 documents.
 However, some document stores do offer features to enforce referential
 integrity, such as cascading deletes or validation rules on document references.
\end_layout

\begin_layout Subsection
Exemplify 6 alternatives in deciding the structure of a document
\end_layout

\begin_layout Enumerate

\series bold
Schema variability
\series default
: potentially different schema is specific to every document in semi-structured.
 This entails:
\end_layout

\begin_deeper
\begin_layout Enumerate

\series bold
Metadata embedding
\series default
: Suppose you are designing a document to store information about a book,
 such as the title, author, and publisher.
 In a semi-structured data model, you may choose to embed metadata within
 the document itself, rather than storing it separately in a schema.
 For example, you may include fields such as "creation date" or "last updated
 by" within the document.
\end_layout

\begin_layout Enumerate

\series bold
Attribute optionality
\series default
: Continuing with the example of a book document, you may choose to make
 certain attributes optional, depending on the nature of the data.
 For example, you may not always have information about the edition or the
 ISBN number, so you may choose to make these fields optional in the document
 schema.
\end_layout

\end_deeper
\begin_layout Enumerate

\series bold
Schema declaration
\series default
: a priori schema declaration is just optional and flexible in semi-structured
 data.
 This includes the declaration of:
\end_layout

\begin_deeper
\begin_layout Enumerate

\series bold
Structure and data types
\series default
: Suppose you are designing a document to store information about a customer,
 such as their name, address, and order history.
 In a semi-structured data model, you may choose to declare a basic schema
 for the document, but allow for flexibility in the specific fields and
 data types used.
 For example, you may declare that the document should have a "name" field
 of type string, but allow for variation in the format or content of the
 name (e.g.
 first name, last name, or both).
\end_layout

\begin_layout Enumerate

\series bold
Integrity constraints
\series default
: Continuing with the example of a customer document, you may choose to
 impose certain integrity constraints on the data, such as ensuring that
 the customer's email address is unique across all documents.
 However, you may also allow for flexibility in the data model by not enforcing
 constraints that are not critical to the business logic.
\end_layout

\end_deeper
\begin_layout Enumerate

\series bold
Structure complexity
\series default
: complex nesting can be used in semi-structured data.
 This includes the representation of:
\end_layout

\begin_deeper
\begin_layout Enumerate

\series bold
Nested structures
\series default
: Suppose you are designing a document to store information about a company's
 organizational structure, such as the departments, managers, and employees.
 In a semi-structured data model, you may choose to represent this structure
 using nested objects or arrays, to allow for flexibility in the depth and
 complexity of the organizational hierarchy.
\end_layout

\begin_layout Enumerate

\series bold
Multi-valued attributes
\series default
: Continuing with the example of an organizational structure document, you
 may choose to use multi-valued attributes to represent relationships between
 entities.
 For example, you may include a field for "direct reports" in the manager
 object, which can contain an array of employee objects representing the
 manager's subordinates.
\end_layout

\end_deeper
\begin_layout Subsection
Explain the difference between JSON and BJSON
\end_layout

\begin_layout Standard
JSON (JavaScript Object Notation) and BJSON (Binary JSON) are both data
 interchange formats that are used to represent semi-structured data in
 a human-readable format.
 However, the main difference between the two is that BJSON is a binary
 format, whereas JSON is a text-based format.
\end_layout

\begin_layout Standard
In more detail, the main differences between JSON and BJSON are as follows:
\end_layout

\begin_layout Itemize

\series bold
Encoding
\series default
: JSON uses a text-based encoding format that represents data using Unicode
 characters.
 In contrast, BJSON uses a binary encoding format that represents data using
 binary values.
\end_layout

\begin_layout Itemize

\series bold
Size
\series default
: Since BJSON uses a binary encoding format, it typically results in much
 smaller file sizes compared to JSON.
 This is because binary data can be represented more efficiently in terms
 of space than text-based data.
\end_layout

\begin_layout Itemize

\series bold
Parsing
\series default
: Parsing JSON involves reading the text-based data character by character
 and interpreting it as objects and values.
 In contrast, parsing BJSON involves reading the binary data and decoding
 it into objects and values.
\end_layout

\begin_layout Itemize

\series bold
Encoding/Decoding speed
\series default
: BJSON can be encoded and decoded much faster than JSON due to its binary
 nature.
 This makes it a more suitable format for applications that require high-speed
 processing and low network latency.
\end_layout

\begin_layout Itemize

\series bold
Human-readability
\series default
: JSON is more human-readable than BJSON because it uses a text-based format
 that is easy to understand and edit.
 BJSON, on the other hand, is not as human-readable because it uses binary
 encoding.
\end_layout

\begin_layout Subsection
Name the main functional components of MongoDB architecture
\end_layout

\begin_layout Standard
We distinguish between machines that contain data, organized in 
\series bold
replica sets
\series default
 and those that purely route queries, known as 
\series bold
mongos
\series default
.
\end_layout

\begin_layout Enumerate

\series bold
Replica sets
\series default
:
\end_layout

\begin_deeper
\begin_layout Enumerate

\series bold
Config servers
\series default
: contain the global catalog, which keeps track of existing shards.
\end_layout

\begin_layout Enumerate

\series bold
Shards
\series default
: the components that actually store data.
\end_layout

\begin_layout Enumerate

\series bold
Balancer
\series default
: a process inside the primary config server in charge of detecting unbalanced
 shards and moving chunks from one shard to another.
 This allows shards to split or migrate chunks of data between different
 machines.
\end_layout

\end_deeper
\begin_layout Enumerate

\series bold
Mongos
\series default
: they split the queries and merge back the results.
 To make it more efifcient and avoid disturbing the coordinator, they maintain
 a 
\series bold
cache of shards
\series default
, which is lazily synchronized.
 Typically, mongos sit in the client machine to avoid network traffic.
\end_layout

\begin_layout Standard
In addition, we have:
\end_layout

\begin_layout Enumerate

\series bold
Clients
\series default
: MongoDB clients are applications or tools that interact with the MongoDB
 database.
 Clients can communicate with MongoDB using various drivers and APIs provided
 by MongoDB.
\end_layout

\begin_layout Enumerate

\series bold
Mongod
\series default
: The mongod process is the main component of the MongoDB server.
 It manages the data stored in the database, handles read and write requests,
 and interacts with clients.
\end_layout

\begin_layout Subsection
Explain the role of 
\emph on
'mongos'
\emph default
 in query processing
\end_layout

\begin_layout Standard
In MongoDB, 'mongos' is a component of the architecture that acts as a 
\color red
query router in sharded clusters
\color inherit
.
 Its main role is to route incoming client requests to the appropriate shard(s)
 in the cluster.
\end_layout

\begin_layout Standard
When a client sends a query request to the mongos process, mongos 
\color blue
first checks whether the query includes the shard key
\color inherit
.
 If the query includes the shard key, mongos routes the request directly
 to the appropriate shard(s) based on the shard key value.
 If the query does not include the shard key, mongos sends the query to
 all shards in the cluster and aggregates the results before returning them
 to the client.
\end_layout

\begin_layout Standard
Mongos also performs other important functions in query processing, including
 load balancing and query optimization.
 Specifically, mongos 
\color blue
balances the load
\color inherit
 across different shards by distributing incoming queries evenly across
 all available shards.
 This helps to ensure that no single shard is overloaded with too many queries,
 which can lead to performance issues.
\end_layout

\begin_layout Standard
In addition, mongos 
\color blue
performs query optimization
\color inherit
 by analyzing incoming queries and determining the most efficient way to
 process them.
 This involves selecting the appropriate indexes to use and optimizing the
 order of operations to minimize the number of documents that need to be
 scanned.
 The mechanism is rather simple and just pushes the first selection and
 projection to the shards.
\end_layout

\begin_layout Standard
This provides 
\color teal
inter-query parallelism
\color inherit
, since different routers and replicas can serve different users, but not
 inter-operator parallelism, since all are run in the router after the first
 operation in the shards is finished.
\end_layout

\begin_layout Standard
It also offers 
\color teal
intra-operator parallelism
\color inherit
 in case of static fragmentation, as different shards would serve different
 pieces of the collection in parallel for the same query.
\end_layout

\begin_layout Subsection
Explain what a replica set is in MongoDB
\end_layout

\begin_layout Standard
In MongoDB, a replica set is a group of MongoDB servers that maintain identical
 copies of the same data.
 A replica set provides high availability and automatic failover in case
 of server failures.
\end_layout

\begin_layout Standard
A replica set consists of 
\color red
several MongoDB instances
\color inherit
, or nodes, that are configured to communicate with each other.
 One node is designated as the primary node, and the others are secondary
 nodes.
 The 
\color blue
primary node
\color inherit
 is responsible for receiving write operations and applying them to the
 data set.
 The 
\color blue
secondary nodes
\color inherit
 replicate the data from the primary node and can serve read operations.
\end_layout

\begin_layout Standard

\color orange
If the primary node fails
\color inherit
 or becomes unavailable, one of the secondary nodes is automatically elected
 as the new primary node.
 This process is called 
\series bold
failover
\series default
, and it ensures that the replica set can continue to function even in the
 event of a node failure.
 Once the failed node is restored, it can rejoin the replica set and serve
 as a secondary node.
\end_layout

\begin_layout Standard
Replica sets provide several benefits in MongoDB, including high availability,
 fault tolerance, and scalability.
 They are often used in production environments to ensure that the database
 can continue to operate even in the face of hardware or network failures.
\end_layout

\begin_layout Subsection
Name the three storage engines in MongoDB
\end_layout

\begin_layout Standard
MongoDB provides three storage engines to manage data:
\end_layout

\begin_layout Itemize

\series bold
WiredTiger
\series default
: The default storage engine since MongoDB 3.2.
 It is a modern, efficient, and high-performance storage engine that provides
 document-level concurrency control, compression, and support for transactions.
\end_layout

\begin_layout Itemize

\series bold
In-Memory
\series default
: This storage engine stores all data in memory, which provides very fast
 read and write operations.
 However, it is not suitable for large datasets, and all data is lost if
 the server is restarted or shuts down.
\end_layout

\begin_layout Itemize

\series bold
MMAPv1
\series default
: This is the legacy storage engine in MongoDB, which uses memory-mapped
 files to store data on disk.
 It is still available in MongoDB, but it has been deprecated and is no
 longer actively developed.
 It does not support document-level concurrency control or compression,
 and it is not recommended for new deployments.
\end_layout

\begin_layout Standard
Each storage engine has its own advantages and disadvantages, and the choice
 of storage engine depends on the specific use case and workload.
 For most workloads, the WiredTiger storage engine is recommended as it
 provides a good balance of performance, scalability, and reliability.
\end_layout

\begin_layout Subsection
Explain what shard and chunk are in MongoDB
\end_layout

\begin_layout Standard
In MongoDB, 
\series bold
sharding
\series default
 is a method for distributing data across multiple servers, or shards, to
 improve performance and scalability.
 A 
\series bold
shard
\series default
 is a single MongoDB instance that stores a portion of the data.
\end_layout

\begin_layout Standard
A 
\series bold
chunk
\series default
 is a contiguous range of data within a shard that is assigned to a specific
 shard key range.
 The 
\color red
shard key
\color inherit
 is a unique identifier used to distribute data across shards in a sharded
 cluster.
 The data in a MongoDB collection is partitioned into chunks based on the
 shard key value.
 Each chunk represents a range of shard key values that is managed by a
 specific shard.
 The 
\color red
chunk size
\color inherit
 is dynamically managed by the MongoDB 
\color blue
balancer
\color inherit
, which redistributes chunks across shards as the distribution of data changes
 over time.
 When the size of a chunk grows beyond a certain limit, the 
\color teal
balancer splits the chunk
\color inherit
 into two smaller chunks, which are then assigned to different shards.
 Similarly, when the size of a chunk shrinks below a certain limit, the
 
\color teal
balancer merges the chunk
\color inherit
 with an adjacent chunk and assigns the new larger chunk to a single shard.
\end_layout

\begin_layout Standard
The use of shards and chunks allows MongoDB to 
\color blue
horizontally scale
\color inherit
 databases to handle large volumes of data and high write and read request
 rates.
 By distributing data across multiple shards, MongoDB can provide better
 performance, availability, and scalability compared to a single monolithic
 database.
\end_layout

\begin_layout Subsection
Explain the two horizontal fragmentation mechanisms in MongoDB
\end_layout

\begin_layout Standard
In MongoDB, 
\series bold
horizontal fragmentation
\series default
, or sharding, is a mechanism for dividing data across multiple servers,
 or shards, to improve performance and scalability.
 MongoDB provides two mechanisms for horizontal fragmentation:
\end_layout

\begin_layout Enumerate

\color red
Range-based sharding
\color inherit
: This mechanism partitions data based on a specified shard key range.
 Each shard is responsible for a specific range of shard key values.
 For example, if the shard key is a timestamp, a range-based sharding strategy
 can be used to split data by time intervals such as hours, days, or months.
\end_layout

\begin_layout Enumerate

\color red
Hash-based sharding
\color inherit
: This mechanism partitions data based on a hash of the shard key value.
 Each shard is responsible for a specific range of hash values.
 Hash-based sharding can be useful when the shard key values are not evenly
 distributed, as it ensures a more balanced distribution of data across
 shards.
\end_layout

\begin_layout Standard
Both range-based and hash-based sharding mechanisms have their own 
\color blue
advantages and disadvantages
\color inherit
.
 Range-based sharding is useful when the data is naturally partitioned into
 ranges, such as by time, and it can be easier to manage and monitor.
 Hash-based sharding can provide a more balanced distribution of data across
 shards and can be more flexible in handling changes in the data distribution
 over time.
\end_layout

\begin_layout Standard
In both mechanisms, MongoDB uses a 
\color blue
shard key
\color inherit
 to determine how to distribute the data across shards.
 The shard key is a field in the data that is used to partition the data
 into chunks, which are then distributed across the shards.
 The choice of shard key is an important factor in determining the performance
 and scalability of a sharded MongoDB cluster.
 It is a mandatory attribute in all the documents of the collection, and
 must be indexed.
 It can be chosen by calling
\begin_inset Formula 
\[
sh.shardCollection\left(\left\langle namespace\right\rangle ,\left\langle key\right\rangle \right).
\]

\end_inset


\end_layout

\begin_layout Standard
Note: there is 
\color orange
no vertical fragmentation
\color inherit
.
\end_layout

\begin_layout Subsection
Explain how the catalog works in MongoDB
\end_layout

\begin_layout Standard
In MongoDB, the 
\series bold
catalog
\series default
 is a metadata repository that stores information about the databases, collectio
ns, indexes, and other objects in the database.
 The catalog is used by MongoDB to 
\color blue
manage and optimize the performance 
\color inherit
of database operations.
\end_layout

\begin_layout Standard
The
\color red
 catalog information
\color inherit
 in MongoDB is treated like any other piece of data.
 The data is stored in a replica set, and consequently enjoys all its synchroniz
ation benefits and consequences.
 The only specificity is that its information is cached in the routers.
 The behavior in MongoDB is also Lazy/Primary-copy.
\end_layout

\begin_layout Standard
The catalog is 
\color blue
updated automaticall
\color inherit
y by MongoDB as new databases, collections, and indexes are created, modified,
 or deleted.
 MongoDB also uses the catalog to optimize database operations, such as
 query planning and execution, by analyzing the metadata stored in the catalog.
\end_layout

\begin_layout Standard
Developers and database administrators can also query the catalog to retrieve
 information about the databases, collections, indexes, and other objects
 in the MongoDB instance.
 This information can be used to 
\color blue
monitor and troubleshoot
\color inherit
 the performance of database operations and to optimize the schema and indexing
 strategies for the MongoDB collections.
\end_layout

\begin_layout Subsection
Identify the characteristics of the replica synchronization management in
 MongoDB
\end_layout

\begin_layout Standard
Replication is based on 
\color red
Replica Sets
\color inherit
, which are sets of 
\color red
mongod instances
\color inherit
 (typically three) that act coordinately.
 A shard siting in a replica set means that its data is mirrored in all
 the nodes that belong to that replica set.
 Since replica sets are disjoint, this means that scaling by sharding results
 very expensive in terms of the number of machines.
 It may be better to simply add more memory to a single machine.
\end_layout

\begin_layout Standard
The replica synchronization management in MongoDB has the following characterist
ics:
\end_layout

\begin_layout Itemize

\series bold
Asynchronous replication
\series default
: MongoDB replica sets use asynchronous replication to propagate data changes
 from the primary to the secondary nodes.
 This means that the primary node does not wait for the secondary nodes
 to confirm the receipt of the data changes before returning the acknowledgement
 to the client.
 Asynchronous replication can improve the performance and availability of
 the database by reducing the latency of write operations.
\end_layout

\begin_layout Itemize

\series bold
Oplog
\series default
: The MongoDB replication mechanism uses an 
\color blue
operation log (oplog) to record all data changes
\color inherit
 made to the primary node.
 The oplog is a capped collection that stores a rolling window of the most
 recent data changes.
 Secondary nodes use the oplog to catch up with the primary node by replaying
 the data changes in the oplog in order.
\end_layout

\begin_layout Itemize

\series bold
Data consistency
\series default
: MongoDB replica sets use a 
\color blue
consensus-based protocol
\color inherit
 to ensure data consistency across the nodes.
 When a primary node receives a write operation, it records the operation
 in its oplog and sends it to the secondary nodes.
 The secondary nodes apply the data changes to their local copies of the
 data and then send an acknowledgement back to the primary node.
 The primary node waits for a majority of the secondary nodes to confirm
 the receipt of the data changes before returning the acknowledgement to
 the client.
 This ensures that the data changes are committed to a majority of the nodes
 in the replica set before they are considered to be fully written.
\end_layout

\begin_layout Itemize

\series bold
Automatic failover
\series default
: MongoDB replica sets provide automatic failover in the event that the
 primary node fails.
 When the primary node fails, the replica set elects a new primary node
 based on a consensus-based protocol.
 The new primary node takes over the role of the primary node and begins
 accepting write operations.
\end_layout

\begin_layout Itemize

\series bold
Priority and voting
\series default
: MongoDB replica sets allow administrators to set the priority and voting
 power of the nodes in the replica set.
 The priority and voting power of a node determine its eligibility to become
 the primary node in the event of a failover.
 Nodes with higher priority and voting power have a greater chance of being
 elected as the primary node.
\end_layout

\begin_layout Subsection
Explain how primary copy failure is managed in MongoDB
\end_layout

\begin_layout Standard
In MongoDB, a primary copy failure is managed through a process called 
\color red
automatic failover
\color inherit
.
 When the primary node fails, the other nodes in the replica set elect a
 new primary node.
 The election process is based on a consensus protocol that ensures that
 the new primary node is chosen by a majority of the nodes in the replica
 set.
\end_layout

\begin_layout Standard
The 
\color red
election process
\color inherit
 works as follows:
\end_layout

\begin_layout Enumerate
The nodes in the replica set communicate with each other to determine the
 status of the primary node.
 If the primary node fails to respond, the other nodes 
\color blue
detect the failure and initiate an election
\color inherit
.
\end_layout

\begin_layout Enumerate
Each node that is eligible to become the primary node (i.e., a node that has
 a copy of the data and is up-to-date with the oplog) 
\color blue
casts a vote for itself
\color inherit
.
\end_layout

\begin_layout Enumerate
The nodes communicate with each other to 
\color blue
determine the node with the most votes
\color inherit
.
 If a node receives a majority of the votes (i.e., more than half of the nodes
 in the replica set), it becomes the new primary node.
\end_layout

\begin_layout Enumerate
If no node receives a majority of the votes, the election fails and the
 replica set cannot elect a new primary node.
 In this case, the 
\color blue
administrators must intervene
\color inherit
 to resolve the issue.
\end_layout

\begin_layout Standard
Once a new primary node is elected, the other nodes in the replica set update
 their configurations to recognize the new primary node.
 The new primary node then starts accepting write operations and propagating
 data changes to the other nodes in the replica set.
\end_layout

\begin_layout Standard
In addition to automatic failover, MongoDB provides several features to
 
\color orange
minimize the risk of primary copy failure
\color inherit
.
 These include:
\end_layout

\begin_layout Itemize

\series bold
Data replication
\series default
: MongoDB replica sets replicate data across multiple nodes to ensure that
 there are multiple copies of the data in the system.
\end_layout

\begin_layout Itemize

\series bold
Oplog replication
\series default
: MongoDB replica sets replicate the oplog (the log of data changes) across
 all nodes in the replica set to ensure that the nodes are up-to-date with
 the data changes.
\end_layout

\begin_layout Itemize

\series bold
Health checks
\series default
: MongoDB replica sets regularly check the status of the nodes in the replica
 set to ensure that they are functioning properly.
 If a node fails to respond, the other nodes detect the failure and initiate
 an election to choose a new primary node.
\end_layout

\begin_layout Subsection
Name the three query mechanisms of MongoDB
\end_layout

\begin_layout Standard
The three query mechanisms of MongoDB are:
\end_layout

\begin_layout Itemize

\color red
Find method
\color inherit
: This is the primary method for querying MongoDB.
 It searches for documents in a collection that match a specified query
 filter and returns the results in a cursor.
\end_layout

\begin_layout Itemize

\color red
Aggregation framework
\color inherit
: This provides a more powerful and flexible mechanism for querying MongoDB
 by combining multiple documents and performing operations on them, such
 as grouping, sorting, and filtering.
\end_layout

\begin_layout Itemize

\color red
MapReduce
\color inherit
: This is a method for processing large datasets in parallel across multiple
 nodes in a MongoDB cluster.
 It breaks down the data into smaller chunks and distributes the processing
 to the nodes, then combines the results into a final output.
\end_layout

\begin_layout Subsection
Explain the query optimization mechanism of MongoDB
\end_layout

\begin_layout Standard
MongoDB's query optimization mechanism is designed to minimize the time
 and resources required to execute queries.
 The 
\color red
query optimizer
\color inherit
 takes into account various factors when choosing an execution plan, such
 as the available indexes, the size of the collection, and the complexity
 of the query.
\end_layout

\begin_layout Standard
When a query is submitted to MongoDB, the query optimizer analyzes the query
 and selects an execution plan that is optimal for the query.
 The 
\color red
execution plan
\color inherit
 consists of a sequence of operations that MongoDB performs to retrieve
 the requested data.
 These operations can include scanning indexes, filtering data, and sorting
 data.
\end_layout

\begin_layout Standard
The query optimizer chooses the execution plan that 
\color blue
minimizes the number of operations required to execute the query and the
 amount of data that needs to be scanned
\color inherit

\begin_inset Foot
status open

\begin_layout Plain Layout
According to the professor the optimizer is not cost based, but I am not
 very sure about that based on what I am reading.
\end_layout

\end_inset

.
 This results in faster query execution and reduces the load on the system.
\end_layout

\begin_layout Standard
MongoDB's query optimizer uses a variety of techniques to optimize queries,
 including:
\end_layout

\begin_layout Itemize

\series bold
Index selection
\series default
: The query optimizer selects the optimal index to use based on the query's
 filter and sort criteria.
 MongoDB offers different kinds of indexes (B+tree, hash, geospatial and
 textual), which can be multi-attribute, or even defined over arrays.
\end_layout

\begin_layout Itemize

\series bold
Query reordering
\series default
: The query optimizer rearranges the order of query operations to minimize
 the amount of data that needs to be scanned.
\end_layout

\begin_layout Itemize

\series bold
Query pruning
\series default
: The query optimizer eliminates unnecessary operations that do not contribute
 to the query result.
\end_layout

\begin_layout Itemize

\series bold
Query caching
\series default
: The query optimizer caches frequently executed queries and their execution
 plans to reduce the time required to execute them.
 MongoDB query optimizer does not launch different executions in parallel
 using alternative access paths.
 Instead, it compares plan execution for a query pattern every ~1,000 write
 operations and then caches the “winning” query plan until the next time
 the optimizer runs or you explicitly call an explain() on that query.
 The winning query plan is based on the number of “work units” (works) performed
 by the query execution plan when evaluating candidate plans.
 The works value represents an approximate measure of how much work a query
 plan requires to execute.
 The optimizer selects the plan with lowest works value as the winning plan
\begin_inset Foot
status open

\begin_layout Plain Layout
According to the professor: When a query is received, the cache is visited
 to see if there is a matching entry.
 If it is found, it is used to generate a plan for the current one, which
 will be then tested for some time.
 If the generated plan takes too long or there is not any query pattern
 in the cache matching the current query, the system launches different
 executions in parallel using alternative access paths.
 Eventually, one of such executions will finish and the others will be killed.
 The executed one will be the one kept in the cache.
\end_layout

\end_inset

.
\end_layout

\begin_layout Subsection
Given two alternative structures of a document, explain the performance
 impact of the choice in a given setting
\end_layout

\begin_layout Subsection
Simulate splitting and migration of chunks in MongoDB
\end_layout

\begin_layout Subsection
Configure the number of replicas needed for confirmation on both reading
 and writing in a given scenario
\end_layout

\begin_layout Subsection
Perform some queries on MongoDB through the shell and aggregation framework
\end_layout

\begin_layout Subsection
Compare the access costs given different document designs
\end_layout

\begin_layout Subsection
Compare the access costs with different indexing strategies (i.e.
 hash and range based)
\end_layout

\begin_layout Subsection
Compare the access costs with different sharding distributions (i.e.
 balanced and unbalanced)
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
MapReduce I
\end_layout

\begin_layout Subsection
Enumerate several use cases of MapReduce
\end_layout

\begin_layout Standard
MapReduce was based on Google development and was originally conceived to
 compute the page rank.
 Nonetheless, it can be used in many different situations:
\end_layout

\begin_layout Itemize
Batch processing: MapReduce can be used to process large volumes of data
 in batches, such as log files, customer records, or social media data.
\end_layout

\begin_layout Itemize
Data warehousing: MapReduce can be used to extract, transform, and load
 (ETL) data into a data warehouse, such as Hadoop Distributed File System
 (HDFS).
\end_layout

\begin_layout Itemize
Search indexing: MapReduce can be used to index and search large volumes
 of unstructured data, such as web pages, documents, or social media posts.
\end_layout

\begin_layout Itemize
Machine learning: MapReduce can be used to train and evaluate machine learning
 models on large datasets, such as image or speech recognition.
\end_layout

\begin_layout Itemize
Fraud detection: MapReduce can be used to detect fraudulent activities in
 financial transactions or insurance claims.
\end_layout

\begin_layout Itemize
Recommendation engines: MapReduce can be used to analyze user behavior and
 generate personalized recommendations, such as in e-commerce or online
 media.
\end_layout

\begin_layout Itemize
Log analysis: MapReduce can be used to analyze and visualize system logs,
 such as web server logs, to identify patterns and anomalies.
\end_layout

\begin_layout Itemize
Social network analysis: MapReduce can be used to analyze social networks
 and identify communities, influencers, or trends.
\end_layout

\begin_layout Itemize
Image and video processing: MapReduce can be used to process and analyze
 large volumes of multimedia data, such as images or videos, for content-based
 retrieval or object recognition.
\end_layout

\begin_layout Itemize
Natural language processing: MapReduce can be used to process and analyze
 natural language data, such as text or speech, for sentiment analysis,
 topic modeling, or language translation.
\end_layout

\begin_layout Subsection
Explain 6 benefits of using MapReduce
\end_layout

\begin_layout Itemize

\series bold
Facilitates scalability
\series default
: MapReduce is designed to process large volumes of data and is highly scalable.
 It can be used to handle increasing amounts of data by adding more machines
 to the cluster, making it ideal for big data processing.
\end_layout

\begin_layout Itemize

\series bold
Hidden parallelism
\series default
: MapReduce hides the complexities of parallel processing by dividing the
 workload into smaller tasks and distributing them across multiple machines.
 This allows for faster processing times and better resource utilization.
\end_layout

\begin_layout Itemize

\series bold
Transparent distribution
\series default
: MapReduce abstracts away the details of distributed computing, making
 it easier for developers to write code without worrying about the underlying
 infrastructure.
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Exploit data locality
\series default
: MapReduce schedules tasks on nodes that are in close proximity to the
 data being processed, reducing network overhead and improving performance.
 This is achieved by bringing the computation to the data instead of moving
 the data to the computation.
\end_layout

\begin_layout Itemize

\series bold
Balance workload
\series default
: MapReduce automatically balances the workload across nodes in the cluster,
 ensuring that each machine is processing roughly the same amount of data.
 This helps prevent resource bottlenecks and ensures that the job completes
 in a reasonable amount of time.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Resilience to failure
\series default
: MapReduce is designed to be fault-tolerant and can handle failures gracefully.
 If a machine fails, the job is automatically rescheduled on another machine.
 
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Fine-grained fault tolerance
\series default
: MapReduce can recover from individual task failures, allowing jobs to
 continue processing without needing to restart the entire job.
 This reduces the impact of failures on the overall job completion time.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Useful in any domain
\series default
: MapReduce is a general-purpose processing framework that can be used in
 any domain where there is a need to process large volumes of data in a
 distributed manner.
 The framework is not specific to any one industry or application, and can
 be applied to a wide range of use cases, such as financial analysis, scientific
 research, social media analytics, e-commerce, and more.
 Additionally, MapReduce is not limited to structured data and can be used
 to process unstructured and semi-structured data as well.
 Its flexibility and versatility make it a popular choice for organizations
 of all sizes and industries.
\end_layout

\begin_layout Subsection
Describe what the MapReduce is in the context of a DDBMS
\end_layout

\begin_layout Standard
In the context of a DDBMS, this framework introduces some changes:
\end_layout

\begin_layout Enumerate
Elimination of the globar scheduler: as data was only going to be read,
 no inconsistencies could arise, making the scheduler unnecessary.
\end_layout

\begin_layout Enumerate
Elimination of the global query manager: it is the task of the developer
 to decide how a query should be executed.
\end_layout

\begin_layout Enumerate
MapReduce substitutes the Global Execution Manager.
\end_layout

\begin_layout Enumerate
Disregard the complexities of the DDBMS, by implementing MapReduce on top
 of Hadoop.
\end_layout

\begin_layout Subsection
Recognize the signature of Map and Reduce functions
\end_layout

\begin_layout Standard
A 
\series bold
Map
\series default
 is a function that takes a pair 
\begin_inset Formula $\left\langle key,value\right\rangle $
\end_inset

 of the input domain and obtain a set of zero or more new 
\begin_inset Formula $\left\langle key,value\right\rangle $
\end_inset

 pairs of the output domain:
\begin_inset Formula 
\[
\begin{array}{cccc}
f: & T_{IK}\times T_{IV} & \longrightarrow & 2^{T_{OK}\times T_{OV}}\\
 & \left\langle k,v\right\rangle  & \mapsto & \left\{ \left\langle k_{1}',v_{1}'\right\rangle ,...,\left\langle k_{n}',v_{n}'\right\rangle \right\} 
\end{array},
\]

\end_inset

 here 
\begin_inset Formula $T_{IK}$
\end_inset

 is the domain of input keys, 
\begin_inset Formula $T_{IV}$
\end_inset

 the domain of input values, 
\begin_inset Formula $T_{OK}$
\end_inset

 the domain of output keys and 
\begin_inset Formula $T_{OV}$
\end_inset

 the domain of output values.
\end_layout

\begin_layout Standard
A 
\series bold
Reduce
\series default
 is a function that takes all the pairs with the same value, and return
 a new set of 
\begin_inset Formula $\left\langle key,value\right\rangle $
\end_inset

 pairs combining them:
\begin_inset Formula 
\[
\begin{array}{cccc}
f: & T_{OK}\times2^{T_{OV}} & \longrightarrow & 2^{T_{FK},T_{FV}}\\
 & \left\langle k,\left\{ v_{1},...,v_{k}\right\} \right\rangle  & \mapsto & \left\{ \left\langle k_{1}',v_{1}'\right\rangle ,...,\left\langle k_{n}',v_{n}'\right\rangle \right\} 
\end{array},
\]

\end_inset

 here 
\begin_inset Formula $T_{FK}$
\end_inset

 are the final keys and 
\begin_inset Formula $T_{FV}$
\end_inset

 the final values.
\end_layout

\begin_layout Subsection
Explain the phases of a MapReduce operation
\end_layout

\begin_layout Itemize

\series bold
Input
\series default
: reads data from a DFS.
\end_layout

\begin_deeper
\begin_layout Itemize
If the input file format is already fragmented into a key-value structure
 (such as SequenceFile), the key and value are taken from the file.
\end_layout

\begin_layout Itemize
If the input file is raw, the framework constructs a key-value structure,
 where the key is the tuple offset in the file, and the value is the row
 itself.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Map
\series default
: for each input key-value pair, the 
\color red
Mapper machines
\color inherit
 execute the user-provided 
\series bold
map
\series default
 function, which can return 0 or more new pairs.
\end_layout

\begin_layout Itemize

\series bold
Partition
\series default
: the generated key-value pairs are assigned to 
\color blue
Reduce machines
\color inherit
 based on their key.
 This phase guarantees that all ocurrences of the same key will be assigned
 to the same Reducer.
 Note, however, that data are not shipped yet.
\end_layout

\begin_layout Itemize

\series bold
Shuffle
\series default
: the 
\color blue
Reduce machines
\color inherit
 pull the generated key-value pairs from the Mapper machines.
\end_layout

\begin_layout Itemize

\series bold
Sort & Comparison
\series default
: each 
\color blue
Reduce machine
\color inherit
 sorts their input key-value pairs based on the key.
 This allows to generate, for each distinct key, their corresponding set
 of values via the merge-sort algorithm.
\end_layout

\begin_layout Itemize

\series bold
Reduce
\series default
: for each input structure key-set of values, the 
\series bold
reduce 
\series default
function is executed, which can return 0 or more key-value pairs.
\end_layout

\begin_layout Itemize

\series bold
Output
\series default
: the result of the 
\series bold
reduce
\series default
 function is written locally at each 
\color blue
Reduce machine
\color inherit
 leveraging on the DFS.
\end_layout

\begin_layout Subsection
Justify to which extent MapReduce is generic
\end_layout

\begin_layout Itemize
MapReduce is supported in many store systems, such as HBase, MongoDB or
 CouchDB.
\end_layout

\begin_layout Itemize
Its programming paradigm is computationally complete, which means that any
 data process can be adapted to it.
 Note, however, that some tasks adapt better than others, and it is not
 necessarily efficient, partly because optimization is very limited because
 of lack of expressivity.
\end_layout

\begin_layout Itemize
Its signature is closed, so iterations can be chained.
 However, fault tolerance is not guaranteed in between, and resources and
 released to be requested again immediately, which is an inneficient handling
 of resources.
\end_layout

\begin_layout Itemize
It is criticized for being too low-level.
 There are APIs for Ruby, Python, Java, C++, etc.
 And there have been attemps to build declarative languages on top, like
 HiveQL or Cassandra Query Language (CQL).
\end_layout

\begin_layout Subsection
Simulate the execution of a simple MapReduce algorithm from the user (agnostic
 of implementation details) perspective
\end_layout

\begin_layout Subsection
Identify the usefulness of MapReduce in a given use case
\end_layout

\begin_layout Subsection
Define the key in the output of the map for a simple problem
\end_layout

\begin_layout Subsection
Provide the pseudo-code of map and reduce functions for a simple problem
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
MapReduce II
\end_layout

\begin_layout Subsection
Enumerate the different kind of processes in Hadoop MapReduce
\end_layout

\begin_layout Standard
In the Hadoop MapReduce framework, there are primarily three types of processes:
\end_layout

\begin_layout Itemize

\series bold
Client program
\series default
: This is the user program that submits the MapReduce job, specifying the
 mapper, reducer, and other configurations.
\end_layout

\begin_layout Itemize

\series bold
Mappers
\series default
: These are tasks that perform the map function on the input data, processing
 each data block independently and generating intermediate key-value pairs.
\end_layout

\begin_layout Itemize

\series bold
Reducers
\series default
: These are tasks that perform the reduce function on the intermediate key-value
 pairs produced by the mappers, aggregating the values based on their correspond
ing keys.
\end_layout

\begin_layout Subsection
Draw the hierarchy of Hadoop MapReduce objects
\end_layout

\begin_layout Subsection
Explain the information kept in the Hadoop MapReduce coordinator node
\end_layout

\begin_layout Standard
The Hadoop MapReduce coordinator node, also known as the JobTracker, keeps
 track of the following information:
\end_layout

\begin_layout Enumerate

\series bold
Map and Reduce tasks
\series default
: It maintains the status (idle, in-progress, or completed) and the identity
 of each worker machine assigned to map or reduce tasks.
\end_layout

\begin_layout Enumerate

\series bold
Intermediate file regions
\series default
: It records the location and size of each intermediate file region produced
 by each map task, storing 
\begin_inset Formula $O\left(M\times R\right)$
\end_inset

 states in memory, as there are 
\begin_inset Formula $R$
\end_inset

 files generated by each of the 
\begin_inset Formula $M$
\end_inset

 mappers.
\end_layout

\begin_layout Subsection
Explain how to decide the number of mappers and reducers
\end_layout

\begin_layout Standard
The 
\series bold
number of map tasks
\series default
 depends on the splitting of the input, which is by default performed per
 HDFS block.
 A general rule of thumb is to have one map task per HDFS block.
 Ideally, each node should run between ten and a hundred mappers, each taking
 more than one minute to execute to justify its creation overhead.
\end_layout

\begin_layout Standard
For 
\series bold
reducers
\series default
, there are two suggested configurations:
\end_layout

\begin_layout Itemize
Minimize the number of tasks (and thus intermediate files), by creating
 slightly fewer reducers than the total number of available processors.
\end_layout

\begin_layout Itemize
Balance the workload, by creating less than double the amount of available
 processors.
\end_layout

\begin_layout Subsection
Explain the fault tolerance mechanisms in Hadoop MapReduce
\end_layout

\begin_layout Subsubsection
Worker Failure
\end_layout

\begin_layout Standard
The corresponding task is immediately reassigned, which is possible because
 all files are replicated in HDFS.
\end_layout

\begin_layout Subsubsection
Master Failure
\end_layout

\begin_layout Standard
Master failure is less likely but can be mitigated by creating checkpoints
 of its in-memory structure that tracks the status of tasks and intermediate
 files.
 In case of failure, another node can be designated as the new master and
 continue the execution from the last checkpoint.
\end_layout

\begin_layout Subsection
Identify query shipping and data shipping in MapReduce
\end_layout

\begin_layout Standard
Query shipping occurs in the map phase of MapReduce, where the map tasks
 are shipped to the nodes where the input data is stored.
\end_layout

\begin_layout Standard
Data shipping occurs during the shuffle phase of MapReduce, when intermediate
 data is moved from the mapper nodes to the corresponding reducer nodes.
\end_layout

\begin_layout Subsection
Explain the effect of using the combine function in MapReduce
\end_layout

\begin_layout Standard
The combine function acts as a local reduce function for each mapper, helping
 to minimize the amount of key-value pairs stored on disk and transferred
 over the network.
 The combine function can be the same as the reduce function if it is commutativ
e and associative.
 If not, it must be designed appropriately to ensure the final outcome remains
 unchanged while reducing intermediate computations.
\end_layout

\begin_layout Subsection
Identify the synchronization barriers of MapReduce
\end_layout

\begin_layout Enumerate
All input data must be uploaded to the HDFS before any processing can begin.
\end_layout

\begin_layout Enumerate
All mappers must finish before reducers can start processing the data.
\end_layout

\begin_layout Enumerate
When chaining multiple MapReduce jobs, the subsequent job cannot start until
 the previous one finishes writing its output.
\end_layout

\begin_layout Subsection
Explain the main problems and limitations of Hadoop MapReduce
\end_layout

\begin_layout Itemize
Startup time is very high, as it requires starting multiple JVM in different
 nodes of the cluster.
\end_layout

\begin_layout Itemize
The master is a single point of failure.
\end_layout

\begin_layout Itemize
Reassigning tasks to the workers in case of failure is expensive and requires
 redefining the execution plan on the fly, scheduling chunks one by one.
\end_layout

\begin_layout Itemize
Tasks are assigned locally where the chunks are, but if they are not evenly
 distributed in the cluster, some of the chunks need to be moved to the
 available processors.
\end_layout

\begin_layout Itemize
Intermediate results are written to disk for fault tolerance, and this entails
 a cost.
\end_layout

\begin_layout Itemize
Reducers fetch all the data from remote nodes.
\end_layout

\begin_layout Itemize
Even if data is compressed in the disk, it will be decompressed before processin
g.
\end_layout

\begin_layout Subsection
Apply the different steps of a MapReduce execution at the implementation
 level
\end_layout

\begin_layout Subsection
Decide on the use of the combine function
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Spark I
\end_layout

\begin_layout Subsection
Name the main Spark contributions and characteristics
\end_layout

\begin_layout Itemize

\series bold
In-memory processing
\series default
: Spark stores data in memory, allowing faster processing and reducing the
 time spent on reading and writing to disk.
 
\end_layout

\begin_layout Itemize

\series bold
Fault tolerance
\series default
: Spark uses Resilient Distributed Datasets (RDDs) to provide fault tolerance
 and maintain data consistency.
 
\end_layout

\begin_layout Itemize

\series bold
Lazy evaluation
\series default
: Spark delays execution until an action is called, optimizing the execution
 plan for better performance.
 
\end_layout

\begin_layout Itemize

\series bold
Support for multiple languages
\series default
: Spark provides APIs for Scala, Java, Python, and R.
 
\end_layout

\begin_layout Itemize

\series bold
Advanced analytics
\series default
: Spark includes libraries for machine learning (MLlib), graph processing
 (GraphX), and stream processing (Structured Streaming).
 
\end_layout

\begin_layout Itemize

\series bold
Unified platform
\series default
: Spark combines batch processing, interactive queries, streaming, and machine
 learning in a single platform.
\end_layout

\begin_layout Subsection
Compare MapReduce and Spark
\end_layout

\begin_layout Itemize

\series bold
Data storage
\series default
: MapReduce stores data on disk, while Spark stores data in-memory, leading
 to faster processing in Spark.
 
\end_layout

\begin_layout Itemize

\series bold
Processing model
\series default
: MapReduce uses a two-stage model (Map and Reduce), while Spark supports
 multi-stage in-memory processing with DAGs (Directed Acyclic Graphs).
 
\end_layout

\begin_layout Itemize

\series bold
Ease of use
\series default
: Spark provides high-level APIs for multiple languages, while MapReduce
 primarily uses Java, making Spark more accessible.
 
\end_layout

\begin_layout Itemize

\series bold
Libraries
\series default
: Spark includes built-in libraries for machine learning, graph processing,
 and stream processing, whereas MapReduce relies on external libraries.
 
\end_layout

\begin_layout Itemize

\series bold
Iterative processing
\series default
: Spark is more suitable for iterative algorithms, as it can cache intermediate
 results in memory, while MapReduce has to read and write to disk for each
 iteration.
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
MapReduce
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Spark
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Data Storage
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Disk
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
In-memory
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Processing model
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
two-stage
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
multi-stage with DAGS
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Ease of use
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Java
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Multiple languages
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Libraries
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Relies on external libraries
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Built-in libraries for ML, graphs, streams
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Iterative processing
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Costly
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Cheaper
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Subsection
Define a dataframe
\end_layout

\begin_layout Standard
Dataframes offer a symmetrical treatment of rows and columns, both of which
 can be referenced explicitly by position or by name.
 The data stored in a dataframe has to adhere to a schema, but this is defined
 at runtime, making it useful for data cleaning.
 Dataframes offer a great variety of operations, enabling us to perform
 relational-like operations, speadsheet operations and linear algebra operations.
 Also, their query syntax is incementally composable and dataframes can
 be natively embedded in an imperative language.
\end_layout

\begin_layout Subsection
Distinguish dataframe from relation and matrix
\end_layout

\begin_layout Standard
\begin_inset Tabular
<lyxtabular version="3" rows="11" columns="4">
<features rotate="90" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Dataframe
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Matrix
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Relation
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Pandas DF
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Spark DF
\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Heterogeneously typed
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Homogeneously typed
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Heterogeneously typed
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Lazily-induced schema
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Rigid schema
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Numeric and non-numeric types
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Numeric types
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Numeric and non-numeric types
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Explicit column names
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
No names
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Explicit column names
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Supports relational algebra
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Does not support relational algebra
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Supports relational algebra
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Ordered
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Unordered
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Ordered
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Unordered
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Named rows
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Unnamed rows
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
No names
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Unnamed rows
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Column-row symmetry
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Columns and rows are different
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Column-row symmetry
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Columns and rows are different
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Support linear algebra
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Does not support linear algebra
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Support linear algebra
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Does not support linear algebra
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Subsection
Distinguish Spark and Pandas dataframe
\end_layout

\begin_layout Itemize

\series bold
Distributed vs.
 local
\series default
: Spark DataFrames are distributed across a cluster, requiring a Spark session
 and enabling parallel processing, while Pandas DataFrames are local, single-nod
e data structures.
 
\end_layout

\begin_layout Itemize

\series bold
Language
\series default
: Spark DataFrames are available in Scala, Java, Python, and R, while Pandas
 DataFrames are specific to the Python language.
 
\end_layout

\begin_layout Itemize

\series bold
Immutability
\series default
 
\series bold
vs mutability
\series default
: Spark DataFrames are immutable, meaning once created, they cannot be changed,
 while Pandas DataFrames can be modified.
 
\end_layout

\begin_layout Itemize

\series bold
Lazy evaluation
\series default
 
\series bold
vs eager evaluation
\series default
: Spark DataFrames use lazy evaluation, while Pandas DataFrames execute
 operations immediately (eager evaluation).
\end_layout

\begin_layout Itemize

\series bold
Scalability
\series default
: pandas DataFrames are not scalable, even if multithread oeprators exist,
 manual splitting is required, while Sprak DataFrames are transparently
 scalable in the Cloud.
\end_layout

\begin_layout Itemize

\series bold
Transposability
\series default
: Pandas DataFrames are transposable, but Spark DataFrames are not.
\end_layout

\begin_layout Subsection
Enumerate some abstraction on top of Spark
\end_layout

\begin_layout Enumerate

\series bold
Resilient Distributed Datasets (RDDs)
\series default
: RDDs are the fundamental abstraction in Spark, representing an immutable
 distributed collection of objects that can be processed in parallel.
 RDDs provide fault tolerance through lineage information and can be cached
 across multiple stages for iterative algorithms.
 They support low-level operations like map, filter, and reduce, allowing
 developers to have fine-grained control over data processing.
\end_layout

\begin_layout Enumerate

\series bold
DataFrames
\series default
: DataFrames are a higher-level abstraction built on top of RDDs.
 They represent a distributed collection of data organized into named columns,
 similar to a relational database table.
 DataFrames provide a convenient API for handling structured and semi-structured
 data and allow for optimizations through the Catalyst query optimizer and
 the Tungsten execution engine.
 Operations like filtering, aggregation, and transformation are available
 through the DataFrame API.
\end_layout

\begin_layout Enumerate

\series bold
Spark SQL
\series default
: Spark SQL is a module that provides a programming interface for working
 with structured and semi-structured data.
 It allows users to query data using SQL as well as the DataFrame API.
 Spark SQL integrates with the Spark ecosystem, enabling the use of SQL
 queries with other Spark libraries like MLlib and GraphX.
 It also supports reading from and writing to various data formats and storage
 systems, such as Parquet, Avro, JSON, Hive, HBase, and JDBC.
\end_layout

\begin_layout Enumerate

\series bold
MLlib
\series default
: MLlib is Spark's built-in library for scalable machine learning.
 It provides various machine learning algorithms for classification, regression,
 clustering, and recommendation, as well as tools for feature extraction,
 transformation, and model evaluation.
 MLlib is designed to scale out across a cluster, enabling the processing
 of large datasets for training and prediction tasks.
 It supports both RDD-based and DataFrame-based APIs.
\end_layout

\begin_layout Enumerate

\series bold
GraphX
\series default
: GraphX is a library for graph processing and computation built on top
 of Spark.
 It allows users to work with graphs and perform graph-parallel computations
 at scale.
 GraphX provides a flexible graph computation API that enables users to
 express graph algorithms like PageRank, connected components, and triangle
 counting.
 It also includes a collection of graph algorithms and builders to simplify
 graph analytics tasks.
\end_layout

\begin_layout Enumerate

\series bold
Structured Streaming
\series default
: Structured Streaming is a module for processing real-time data streams
 in a fault-tolerant and scalable manner.
 It provides a high-level API built on top of DataFrames, allowing users
 to express complex streaming computations using the same operations as
 batch processing.
 Structured Streaming handles the incremental processing of data streams,
 providing exactly-once processing guarantees and allowing for event-time
 and late-data processing.
 It supports various sources and sinks, such as Kafka, HDFS, and Delta Lake.
\end_layout

\begin_layout Subsection
Provide the Spark pseudo-code for a simple problem using dataframes
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Spark II
\end_layout

\begin_layout Subsection
Define RDD
\end_layout

\begin_layout Standard
RDD stands for 
\series bold
Resilient Distributed Dataset
\series default
.
 It is the fundamental abstraction in Spark, representing an immutable distribut
ed collection of objects that can be processed in parallel.
 RDDs provide fault tolerance through lineage information, allowing for
 data recovery in case of failures.
\end_layout

\begin_layout Subsection
Distinguish between Base RDD and Pair RDD
\end_layout

\begin_layout Itemize

\series bold
Base RDD
\series default
: A basic RDD consisting of a distributed collection of objects.
 Base RDDs can be created from data stored in external storage systems or
 by parallelizing a collection in the driver program.
 
\end_layout

\begin_layout Itemize

\series bold
Pair RDD
\series default
: A special type of RDD where each element is a key-value pair.
 Pair RDDs enable operations like grouping, reducing, and aggregating by
 keys.
 They are useful for tasks like counting words or computing sums per group.
\end_layout

\begin_layout Subsection
Distinguish between transformations and actions
\end_layout

\begin_layout Standard

\series bold
Transformations
\series default
 are operations that create a new RDD from an existing one.
 They are performed lazily, meaning they are only executed when an action
 is called.
 Examples of transformations include map, filter, and reduceByKey.
\end_layout

\begin_layout Standard

\series bold
Actions
\series default
 are operations that return a value to the driver program or write data
 to external storage systems.
 Actions trigger the execution of transformations.
 Examples of actions include count, collect, and saveAsTextFile.
\end_layout

\begin_layout Subsection
Explain available transformations
\end_layout

\begin_layout Itemize
map: Applies a function to each element in the RDD, creating a new RDD.
 
\end_layout

\begin_layout Itemize
filter: Returns a new RDD containing only the elements that satisfy a given
 predicate.
 
\end_layout

\begin_layout Itemize
flatMap: Applies a function to each element and flattens the results into
 a single RDD.
 
\end_layout

\begin_layout Itemize
union: Returns a new RDD that contains the union of the elements in the
 source RDD and another RDD.
 
\end_layout

\begin_layout Itemize
distinct: Returns a new RDD containing the distinct elements of the original
 RDD.
 
\end_layout

\begin_layout Itemize
groupByKey: Groups the elements of the RDD by key.
 
\end_layout

\begin_layout Itemize
reduceByKey: Groups the elements by key and reduces the values for each
 key using a specified reduce function.
 
\end_layout

\begin_layout Itemize
sortBy: Returns a new RDD that is sorted by the specified key function.
\end_layout

\begin_layout Subsection
Explain available actions
\end_layout

\begin_layout Itemize
count: Returns the number of elements in the RDD.
 
\end_layout

\begin_layout Itemize
collect: Returns all elements of the RDD as an array to the driver program.
 Be cautious with this action as it can cause the driver to run out of memory
 for large RDDs.
 
\end_layout

\begin_layout Itemize
take: Returns the first n elements of the RDD as an array.
 
\end_layout

\begin_layout Itemize
first: Returns the first element of the RDD.
 
\end_layout

\begin_layout Itemize
reduce: Aggregates the elements of the RDD using a given function.
 
\end_layout

\begin_layout Itemize
saveAsTextFile: Writes the elements of the RDD to a text file in the specified
 directory.
\end_layout

\begin_layout Subsection
Name the main Spark runtime components
\end_layout

\begin_layout Itemize

\series bold
Driver program
\series default
: The program that runs the main function and defines one or more Spark
 operations.
 
\end_layout

\begin_layout Itemize

\series bold
Cluster manager
\series default
: The component responsible for managing resources and scheduling tasks
 in a cluster, such as YARN, Mesos, or the standalone Spark cluster manager.
 
\end_layout

\begin_layout Itemize

\series bold
Executor
\series default
: A process that runs on a worker node and executes tasks on behalf of the
 driver program.
 
\end_layout

\begin_layout Itemize

\series bold
Task
\series default
: A unit of work that runs on an executor, representing a single stage of
 a Spark operation.
\end_layout

\begin_layout Subsection
Explain how to manage parallelism in Spark
\end_layout

\begin_layout Standard
Parallelism in Spark can be managed by controlling the number of partitions
 and the number of cores used by each executor.
 You can set the default number of partitions when creating an RDD or by
 repartitioning an existing RDD.
 You can also configure the number of cores used by each executor in the
 Spark configuration.
\end_layout

\begin_layout Subsection
Explain how recoverability works in Spark
\end_layout

\begin_layout Standard
Recoverability in Spark is achieved through RDD lineage information, which
 records the sequence of transformations used to create an RDD.
 If a partition of an RDD is lost due to a node failure, Spark can recompute
 the partition using the lineage information and the transformations applied
 to the original data.
 This allows Spark to recover lost data without the need for data replication,
 reducing overhead and improving fault tolerance.
\end_layout

\begin_layout Standard
Also, data can be cached/persisted in up to two nodes.
 As a rule of thumb, we should cache an RDD if it is parent of more than
 one RDD.
\end_layout

\begin_layout Subsection
Distinguish between narrow and wide dependencies
\end_layout

\begin_layout Itemize

\series bold
Narrow dependencies
\series default
: In these dependencies, each partition of the parent RDD is used by at
 most one partition of the child RDD.
 This means that the data required for a single partition in the child RDD
 can be found within a single partition of the parent RDD.
 Examples of operations with narrow dependencies include map and filter.
 Narrow dependencies allow for pipelining, reducing the overhead of data
 shuffling.
\end_layout

\begin_layout Itemize

\series bold
Wide dependencies
\series default
: In these dependencies, each partition of the parent RDD may be used by
 multiple partitions of the child RDD.
 This means that the data required for a single partition in the child RDD
 can be spread across multiple partitions of the parent RDD.
 Examples of operations with wide dependencies include groupByKey and reduceByKe
y.
 Wide dependencies require data shuffling, which can be expensive in terms
 of both time and resources.
\end_layout

\begin_layout Subsection
Name the two mechanisms to share variables
\end_layout

\begin_layout Itemize

\series bold
Broadcast variables
\series default
: These are read-only variables that are cached on each worker node, allowing
 tasks to efficiently access large read-only data structures such as lookup
 tables or dictionaries.
\end_layout

\begin_layout Itemize

\series bold
Accumulators
\series default
: These are variables that can be updated by tasks running on worker nodes
 in a parallel and fault-tolerant manner.
 They are typically used for counters and sums.
 Accumulators can be updated only by associative and commutative operations
 to ensure that Spark can combine their values correctly across multiple
 tasks.
\end_layout

\begin_layout Subsection
Provide the Spark pseudo-code for a simple problem using RDDs
\end_layout

\begin_layout Standard
Problem: Word count using RDDs.
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "language=Python"
inline false
status open

\begin_layout Plain Layout

from pyspark import SparkContext
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Create a Spark context
\end_layout

\begin_layout Plain Layout

sc = SparkContext("local", "WordCount")
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Read the input text file
\end_layout

\begin_layout Plain Layout

text_file = sc.textFile("input.txt")
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Split each line into words, and map each word to a (word, 1) tuple
\end_layout

\begin_layout Plain Layout

word_counts = (text_file.flatMap(lambda line: line.split(" "))
\end_layout

\begin_layout Plain Layout

                        .map(lambda word: (word, 1))
\end_layout

\begin_layout Plain Layout

                        .reduceByKey(lambda a, b: a + b))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Save the word count results as a text file
\end_layout

\begin_layout Plain Layout

word_counts.saveAsTextFile("output")
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Stream Data Management
\end_layout

\begin_layout Subsection
Define a data stream
\end_layout

\begin_layout Standard
A 
\series bold
data stream
\series default
 is a dataset that is produced incrementally over time, instead of being
 fully available before its processing begins.
\end_layout

\begin_layout Subsection
Distinguish the two kinds of stream management systems
\end_layout

\begin_layout Itemize

\series bold
Stream Processing Engines (SPE)
\series default
: focus on highly available near-real-time processing and scalability.
 They are designed to process data non-stop, mainly using relatively simple
 
\color blue
window aggregates
\color inherit
.
 They facilitate plugging the stream to other databases.
 For example, Spark streaming or S4.
\end_layout

\begin_layout Itemize

\series bold
Complex Event Processing (CEP)
\series default
: offer 
\color blue
rich windowing operations
\color inherit
 to define indicators based on thresholds and express 
\color blue
complex temporal correlations or patterns
\color inherit
.
 These patterns sometimes need to be detected in long periods of time.
 Therefore, the system needs to keep the corresponding logs and complex
 states, complicating distribution and parallelization.
 For example, Esper or T-Rex.
\end_layout

\begin_layout Subsection
Recognize the importance of stream management
\end_layout

\begin_layout Standard
Stream management is essential for handling continuous and dynamic data
 sources, enabling real-time decision making, and providing insights into
 the behavior of the system or environment.
 It is crucial in many applications, such as monitoring and control systems,
 financial markets, network traffic analysis, social media analysis, and
 Internet of Things (IoT) environments.
\end_layout

\begin_layout Subsection
Enumerate the most relevant chracteristics of streams
\end_layout

\begin_layout Itemize
The arrival rate of data is not under the control of the system.
 The pace is too fast to persist all data, but sometimes it is even too
 fast to process every arriving element.
\end_layout

\begin_layout Itemize
The number of elements is unbounded, requiring virtually an unbounded memory.
 This means that drastic reduction is needed.
\end_layout

\begin_layout Itemize
We need to keep data moving, using only volatile storage.
\end_layout

\begin_layout Itemize
Support for near-real time application, so there is a need to scale and
 parallelize.
\end_layout

\begin_layout Itemize
Arrival order is not guaranteed to be the same as generation order, as some
 data can be delayed for several reasons.
\end_layout

\begin_layout Itemize
Imperfections must be assumed, as some data will be missing for different
 reasons.
\end_layout

\begin_layout Itemize
There is temporal locality, having a temporal evolution of data characteristics.
\end_layout

\begin_layout Itemize
Approximate answers are acceptable, but keeping determinism.
\end_layout

\begin_layout Subsection
Explain to which extent a DBMS can manage streams
\end_layout

\begin_layout Standard
Using an RDBMS is possible in some exceptional cases in which some requirements
 can be relaxed.
\end_layout

\begin_layout Standard
Actually, 
\series bold
active databases
\series default
 were the precursors of SPEs.
 Active Databases have the goal of automatically trigerring a response to
 monitored events such as database updates, points in time or events external
 to the database.
 The operations provided by these databases are 
\series bold
ECA rules
\series default
, usually implemented via 
\series bold
triggers
\series default
, with the main objective of maintaining integrity constraints and derived
 information.
\end_layout

\begin_layout Standard
However, they fall short for more complex aggregations oveer time.
 Moreover, ACID transactions encompass a large overhead on data ingestion,
 hindering the capacity of processing large data streams arriving at a very
 high pace.
\end_layout

\begin_layout Standard

\color blue
If the arrival rate is not very high
\color inherit
, we can use temporary tables, available in many RDBMSs, whose operations
 are much faster, since they are single user and kept in memory.
 This allows to keep a tuple for every message in the stream, making the
 table analogous to a sliding window over the stream.
\end_layout

\begin_layout Subsection
Name 10 differences between DBMS and SPE
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="11" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
DBMS
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
SPE
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Data
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Persistent
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Volatile
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Access
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Random
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Sequential
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Queries
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
One-time
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Continuous
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Support
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Unlimited disk
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Limited RAM
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Order
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Current state
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Sorted
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Ingestion rate
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Relatively low
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Extremely high
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Temporal requirements
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Little
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Near-real-time
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Accuracy
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Exact data
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Imprecise data
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Heterogeneity
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Structured data
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Imperfections
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Algorithms
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Multiple passes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
One pass
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Subsection
Characterize the kinds of queries in an SPE
\end_layout

\begin_layout Standard
We can classify stream operations attending to three independent criteria:
\end_layout

\begin_layout Itemize

\series bold
Trigger
\series default
: we can either keep the queries continuously running or launch them at
 some concrete instant.
\end_layout

\begin_layout Itemize

\series bold
Outputs
\series default
: the result of a query can be a set of elements, but can also be a simple
 boolean interpreted as some kind of alert that detects a change in the
 behavior of the stream.
\end_layout

\begin_layout Itemize

\series bold
Inputs
\series default
: queries cna obviously be evaluated over a subset of data in the stream,
 which is somehow equivalent to analysing small datasets.
 If we want to analyse the unbounded past stream as awhole, we need to rely
 in some summary structure.
\end_layout

\begin_layout Standard
The two most characteristics queries in stream processing are:
\end_layout

\begin_layout Itemize

\series bold
Window-based operations
\series default
: we can see them as taking a snapshot of the stream.
 Then, we can perform any operation as if we were working with a list of
 static messages.
 
\end_layout

\begin_deeper
\begin_layout Standard
The point of this is getting rid of the problem of having the data on the
 move.
 However, simply taking a snapshot and never changing it would result oversimpli
stic.
 Thus, we take a neverending sequence of snapshots, and perform the analysis
 one snapshot at a time.
\end_layout

\begin_layout Standard
Therefore, a 
\series bold
window
\series default
 is defined by both its duration (size) and its sliding interval.
 If both coincide, it is called a 
\series bold
tumbling window
\series default
.
 
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Binary operations
\series default
: there are four possible ways of joininig relations and streams: Relation-Relat
ion, Relation-Stream, Stream-Relation and Stream-Stream.
 
\end_layout

\begin_deeper
\begin_layout Standard
Note that 
\color red
Stream-Stream
\color inherit
 does not make much sense, because it is very unlikely that given two streams,
 the two current elements can be joined.
 If we want to join two streams, the most common approach is to take a snapshot
 of one of them and use a Stream-Relation approach.
\end_layout

\begin_layout Standard
Also, 
\color red
Relation-Relation
\color inherit
 are not interesting from the point of view of stream data processing.
\end_layout

\begin_layout Standard
The two remaining options are actually the same.
 Then, considering 
\color blue
Stream-Relation operations
\color inherit
, there are specific algorithms required which are different from those
 that only deal with Relations, because one of the inputs is not static.
\end_layout

\end_deeper
\begin_layout Subsection
Explain the two parameters of a sliding window
\end_layout

\begin_layout Standard
The window size is the interval of time in seconds for how much historical
 data shall be contained in RDD before processing.
 The sliding interval is the amount of time in seconds for how much the
 window will shift.
 
\end_layout

\begin_layout Standard
In more plain terms: the 
\color blue
sliding interval
\color inherit
 indicates when we process, and the 
\color red
window size
\color inherit
 indicates how much time back we take into account.
 Note that if the interval is bigger than the size, we would be letting
 some data go without processing it.
 This info is exemplified in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Window-parameters-visualization."
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 In the image, the second windowed stream is done with a tumbling window.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename stream_window.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Window parameters visualization.
\begin_inset CommandInset label
LatexCommand label
name "fig:Window-parameters-visualization."

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Explain the three stream architectural patterns
\end_layout

\begin_layout Itemize

\series bold
Stream ingestion
\series default
: the objective of this pattern is to not lose any event.
 For this, we can use queuing mechanisms as Kafka.
\end_layout

\begin_layout Itemize

\series bold
Near-real-time event processing
\series default
: the objective of this pattern is to react to events as soon as possible.
\end_layout

\begin_deeper
\begin_layout Itemize
If we have a 
\color blue
light-weight processing system
\color inherit
, we can use Flume.
\end_layout

\begin_layout Itemize
If the
\color blue
 system is more complex
\color inherit
, we might need to use a distribution approach that guarantees some locality
 is needed.
 For this, we can use different queues, with Kafka, to efficiently distribute
 the data to multiple engine instances.
 
\color red
Complex events
\color inherit
 are characterized by:
\end_layout

\begin_deeper
\begin_layout Itemize
Pattern matching: state keeps all potential matches, using a Tree of a Non-Deter
ministic Finite Automata.
\end_layout

\begin_layout Itemize
Hard to distribute
\end_layout

\begin_layout Itemize
Consider time constraints, absence of events, re-emision of events.
\end_layout

\begin_layout Standard
The groupings defined for 
\color red
complex topologies
\color inherit
 can be:
\end_layout

\begin_layout Itemize
Shuffle grouping: random.
\end_layout

\begin_layout Itemize
Fields grouping: same value, same task.
\end_layout

\begin_layout Itemize
All grouping: broadcast to all tasks.
\end_layout

\begin_layout Itemize
Global groupin: all data to a single task.
\end_layout

\begin_layout Itemize
None grouping: execution stays in the thread of origin of data, whenever
 possible.
\end_layout

\begin_layout Itemize
Direct grouping: producers direct the output to concrete predefined tasks.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection
Explain the goals of Spark streaming architecture
\end_layout

\begin_layout Standard
Spark Streaming is an extension of the core Spark API, designed to handle
 near-real-time data processing by dividing incoming data streams into small
 batches and processing them using Spark's core engine.
 The goals of Spark Streaming architecture are as follows:
\end_layout

\begin_layout Itemize

\series bold
Scalability
\series default
: To handle high volumes of data and scale horizontally across multiple
 nodes, providing the ability to process large data streams in parallel.
\end_layout

\begin_layout Itemize

\series bold
Fault-tolerance
\series default
: To ensure data reliability and system resiliency by providing mechanisms
 like data replication, lineage information, and checkpointing, which help
 recover from failures without losing data or computation progress.
\end_layout

\begin_layout Itemize

\series bold
High-throughput
\series default
: To efficiently process large amounts of data with minimal latency, enabling
 near-real-time processing and analytics for various use cases.
\end_layout

\begin_layout Itemize

\series bold
Unified programming model
\series default
: To offer a single programming model for both batch processing and stream
 processing, simplifying the development and maintenance of applications
 that involve both types of processing.
\end_layout

\begin_layout Itemize

\series bold
Integration with the Spark ecosystem
\series default
: To leverage the existing Spark ecosystem, including libraries like MLlib
 for machine learning, GraphX for graph processing, and SQL support through
 Spark SQL, allowing users to combine multiple types of data processing
 and analytics within a single application.
\end_layout

\begin_layout Itemize

\series bold
Ease of use
\series default
: To provide an accessible API that makes it easy for developers to build
 and deploy streaming applications, enabling them to focus on the application
 logic rather than the complexities of distributed computing.
\end_layout

\begin_layout Itemize

\series bold
Flexibility
\series default
: To support various data sources, sinks, and processing patterns, allowing
 developers to build a wide range of applications for different domains
 and use cases.
\end_layout

\begin_layout Subsection
Draw the architecture of Spark streaming
\end_layout

\begin_layout Subsection
Identify the need of a stream ingestion pattern
\end_layout

\begin_layout Standard
The need for a stream ingestion pattern arises when you have to:
\end_layout

\begin_layout Enumerate
Capture and store high-velocity data streams from various sources.
 
\end_layout

\begin_layout Enumerate
Ensure data durability and availability for processing.
 
\end_layout

\begin_layout Enumerate
Handle backpressure and avoid data loss due to sudden spikes in data rates.
 
\end_layout

\begin_layout Enumerate
Manage data partitioning and replication for fault tolerance.
\end_layout

\begin_layout Subsection
Identify the need of a near-real time processing pattern
\end_layout

\begin_layout Standard
The need for a near-real-time processing pattern arises when you have to:
\end_layout

\begin_layout Enumerate
Process data as it arrives, providing quick insights and enabling real-time
 decision-making.
 
\end_layout

\begin_layout Enumerate
Respond to events or anomalies in the data immediately.
 
\end_layout

\begin_layout Enumerate
Continuously update the application state based on incoming data.
 
\end_layout

\begin_layout Enumerate
Handle large-scale data streams with low latency requirements.
\end_layout

\begin_layout Subsection
Identify the kind of message exchange pattern
\end_layout

\begin_layout Standard
The kind of message exchange pattern depends on the specific use case and
 system requirements.
 Common patterns include:
\end_layout

\begin_layout Enumerate

\series bold
Publish-subscribe
\series default
: Producers publish messages to a shared topic, and consumers subscribe
 to topics to receive messages.
 
\end_layout

\begin_layout Enumerate

\series bold
Point-to-point
\series default
: Producers send messages directly to specific consumers, with each message
 consumed by a single consumer.
 
\end_layout

\begin_layout Enumerate

\series bold
Request-reply
\series default
: A synchronous pattern where the sender expects a response for each sent
 message.
\end_layout

\begin_layout Subsection
Simulate the mesh-join algorithm
\end_layout

\begin_layout Subsection
Estimate the cost of the mesh-join algorithm
\end_layout

\begin_layout Subsection
Use windowing transformations in Spark streaming
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "language=Python"
inline false
status open

\begin_layout Plain Layout

from pyspark.streaming import StreamingContext
\end_layout

\begin_layout Plain Layout

from pyspark import SparkContext
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Initialize Spark context and streaming context with a batch interval of
 1 second
\end_layout

\begin_layout Plain Layout

sc = SparkContext("local[*]", "WindowedWordCount")
\end_layout

\begin_layout Plain Layout

ssc = StreamingContext(sc, 1)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Set checkpoint directory for fault tolerance
\end_layout

\begin_layout Plain Layout

ssc.checkpoint("checkpoint")
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Read data from a text stream
\end_layout

\begin_layout Plain Layout

lines = ssc.socketTextStream("localhost", 9999)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Split lines into words and create pairs (word, 1)
\end_layout

\begin_layout Plain Layout

words = lines.flatMap(lambda line: line.split(" ")).map(lambda word: (word,
 1))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Reduce word pairs within a window of 10 seconds and sliding interval of
 2 seconds
\end_layout

\begin_layout Plain Layout

windowed_word_counts = words.reduceByKeyAndWindow(lambda a, b: a + b, lambda
 a, b: a - b, windowDuration=10, slideDuration=2)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Print the word counts within the window
\end_layout

\begin_layout Plain Layout

windowed_word_counts.pprint()
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Start the streaming context and wait for it to terminate
\end_layout

\begin_layout Plain Layout

ssc.start()
\end_layout

\begin_layout Plain Layout

ssc.awaitTermination()
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this example, we first create a DStream from a socket text stream.
 We then split each line into words and create key-value pairs for each
 word.
 The key is the word itself, and the value is 1, representing a single occurrenc
e of the word.
\end_layout

\begin_layout Standard
We then use the reduceByKeyAndWindow function to calculate the word count
 within a sliding window.
 In this case, the window duration is set to 10 seconds, and the slide duration
 is set to 2 seconds.
 The reduceByKeyAndWindow function takes two lambda functions as arguments
 - the first one is used to add the values when a new RDD enters the window,
 and the second one is used to subtract the values when an RDD exits the
 window.
\end_layout

\begin_layout Standard
Finally, we print the word counts within the window using the pprint function
 and start the streaming context.
 The streaming context will run until terminated by the user or an error
 occurs.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Data Stream Analysis
\end_layout

\begin_layout Subsection
Explain the difference between generic one-pass algorithms and stream processing
\end_layout

\begin_layout Standard
One-pass algorithms and stream processing are both designed to handle data
 that can't be stored in its entirety due to its size.
 However, they differ in their approach and specific use cases.
 A 
\color red
one-pass algorithm
\color inherit
, as the name suggests, processes the data in one pass, typically reading
 from a dataset that's stored in a file or a database.
 It aims to make the best decision or computation at each step with the
 information available, without the ability to revisit the data.
\end_layout

\begin_layout Standard

\color red
Stream processing
\color inherit
, on the other hand, is a type of computing that deals with continuously
 incoming data streams.
 It processes the data on-the-go, often in real-time or near-real-time,
 without waiting for all data to arrive or be stored.
 Stream processing can handle 'infinite' data streams, which are never-ending
 and timely, while one-pass algorithms are generally used with 'finite'
 data sets.
\end_layout

\begin_layout Subsection
Name the two challenges of stream processing
\end_layout

\begin_layout Enumerate
Limited computation capacity: Stream processing involves real-time or near-real-
time processing of data.
 The rate at which the data arrives might exceed the processing capability
 of the system, causing data loss or delayed processing.
\end_layout

\begin_layout Enumerate
Limited memory capacity: In stream processing, data is continuously arriving.
 The system might not have enough memory to store all arriving elements,
 especially if the data arrival rate is very high, posing a challenge to
 keep track of the entire history of the stream.
\end_layout

\begin_layout Subsection
Name two solutions to limited processing capacity
\end_layout

\begin_layout Enumerate
Probabilistically drop some stream elements through uniform random sampling
 on the stream.
 A widely known technique is 
\color blue
Load shedding
\color inherit
.
\end_layout

\begin_layout Enumerate
Filter out some elements of the stream based on some characteristic of the
 data.
 For example, using 
\color blue
Bloom filters
\color inherit
.
\end_layout

\begin_layout Subsection
Name three solutions to limited memory capacity
\end_layout

\begin_layout Enumerate
Focus on only part of the stream, ignoring the rest, by means of a Sliding
 window.
\end_layout

\begin_layout Enumerate
Weight the elements of the stream depending on their position, so that we
 can keep track of the aggregate without keeping each and every element.
 Moreover, if we are not interested in the elements not present in the stream
 recently, we can define a threshold to filter them out.
 This is done with an exponentially decaying window.
\end_layout

\begin_layout Enumerate
Keeping a summary structure that allows to approximate the response to some
 queries.
 These structures are usually called Synopsis.
\end_layout

\begin_layout Subsection
Decide the probability of keeping a new element or removing an old one from
 memory to keep equi-probability on load shedding
\end_layout

\begin_layout Standard
If there are 
\begin_inset Formula $p$
\end_inset

 memory positions and we have seen 
\begin_inset Formula $n$
\end_inset

 elements in the stream, then the probability of keeping the new element
 
\begin_inset Formula $n+1$
\end_inset

 is
\begin_inset Formula 
\[
Pr_{in}=\frac{p}{n+1},
\]

\end_inset

 and if it is kept, then we remove elements from memory with probability
\begin_inset Formula 
\[
Pr_{out}=\frac{1}{p}.
\]

\end_inset


\end_layout

\begin_layout Subsection
Decide the parameters of the hash function to get a representative result
 on load shedding
\end_layout

\begin_layout Standard
We should take a hash function mapping to a large number of values, 
\begin_inset Formula $M$
\end_inset

, and keep only elements mapping to a value below 
\begin_inset Formula $t$
\end_inset

.
 We dynamically reduce 
\begin_inset Formula $t$
\end_inset

 as we run out of memory.
\end_layout

\begin_layout Subsection
Decide the optimum number of hash functions in a Bloom filter
\end_layout

\begin_layout Standard
If the filter has 
\begin_inset Formula $n$
\end_inset

 bits and we expect to have 
\begin_inset Formula $m$
\end_inset

 different key values, then the optimal amount of hash functions is
\begin_inset Formula 
\[
k=\frac{n}{m}\log2.
\]

\end_inset


\end_layout

\begin_layout Subsection
Approximate the probability of false positives in a Bloom filter
\end_layout

\begin_layout Standard
The probability of false positives is around
\begin_inset Formula 
\[
Pr_{FP}=\left(1-e^{-\frac{km}{n}}\right)^{k},
\]

\end_inset

 where 
\begin_inset Formula $n$
\end_inset

 is the size of the filter, 
\begin_inset Formula $m$
\end_inset

 is the number of keys and 
\begin_inset Formula $k$
\end_inset

 is the number of hash functions.
 In the case of optimal 
\begin_inset Formula $k$
\end_inset

, it is
\begin_inset Formula 
\[
Pr_{FP}=\left(\frac{1}{2}\right)^{k}\sim0.618^{\frac{n}{m}}.
\]

\end_inset


\end_layout

\begin_layout Subsection
Calculate the weighted average of an attribute considering and exponentially
 decaying function
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Avg_{T+1}=\sum_{i=0}^{T+1}a_{i}\left(1-c\right)^{T+1-i}=\left(\sum_{i=0}^{T}a_{i}\left(1-c\right)^{T-i}\right)\cdot\left(1-c\right)+a_{T+1}=Avg_{T}\cdot\left(1-c\right)+a_{T+1},
\]

\end_inset

 where 
\begin_inset Formula $c$
\end_inset

 is a small constant, 
\begin_inset Formula $T$
\end_inset

 is the current time, 
\begin_inset Formula $a_{t}$
\end_inset

 is the element at time 
\begin_inset Formula $t$
\end_inset

 or 0 if there is no element, and 
\begin_inset Formula $g\left(T-t\right)=\left(1-c\right)^{T-t}$
\end_inset

 is the weight at time 
\begin_inset Formula $T$
\end_inset

 of an item obtained at time 
\begin_inset Formula $t$
\end_inset

.
\end_layout

\begin_layout Subsection
Decide if heavy hitters will show false positives
\end_layout

\begin_layout Standard
It is possible to show false positives, depending on the stream.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Big Data Architectures
\end_layout

\begin_layout Subsection
Explain the problem of a spaguetti architecture
\end_layout

\begin_layout Standard
In Big Data contexts, it is common to find different requirements for different
 tasks, as well as different tools that provide means to perform these tasks,
 with different capabilities.
 Therefore, we can have different tasks spread over different independent
 and heterogeneous systems.
 For example, just looking at Hadoop, we find HDFS and HBase for storage,
 HCatalog for modeling, Sqoop for ingestion, Spark for processing and SparQL
 for querying.
\end_layout

\begin_layout Standard
The spaguetti architecture arises when we solve our tasks' requirements
 one by one, finding what we consider the best option for that specific
 task at each moment.
 This implies that we will end up using tens of different tools, making
 integration more costly, creating artificially complex pipelines and difficulti
ng the scalability of the system as a whole.
 Therefore, architectural designs arise to avoid this problem.
\end_layout

\begin_layout Subsection
Explain the need of the Lambda Architecture
\end_layout

\begin_layout Standard
The Lambda Architecture addresses the problem of computing arbitrary functions
 on arbitrary data in real-time by combining both batch and stream processing
 methods.
 This makes it possible to handle both low-latency real-time data and keep
 the benefits of batch processing for more complex analytics.
\end_layout

\begin_layout Standard
Batch layer stores all incoming data and allows for computation over large
 data sets, while the speed layer deals with recent data to provide real-time
 views.
 This allows the system to provide precomputed views from the batch layer
 and recent data views from the speed layer, meeting the demands of both
 latency and accuracy.
\end_layout

\begin_layout Subsection
Explain the difference between Kappa and Lambda Architectures
\end_layout

\begin_layout Standard
The Lambda Architecture includes two layers, the batch and the speed layer,
 while the Kappa Architecture only uses a streaming layer.
 Lambda uses batch processing for handling large data sets and stream processing
 for real-time data, while Kappa uses stream processing for all data.
 Kappa Architecture is often simpler because it only requires maintaining
 one system, but it may not be suitable for all kinds of computations, especiall
y for those that are naturally easier to express as batch computations.
\end_layout

\begin_layout Subsection
Justify the need of a Data Lake
\end_layout

\begin_layout Standard
The main idea of a Data Lake is 
\series bold
Load-first,Model-later
\series default
.
 In traditional analytical frameworks, the approach is to model the schema
 beforehand, in a Model-first,Load-later manner.
 This approach restricts the potential analysis that could be done in the
 data, since all information that is not compliant with the defined schema
 is lost.
 Therefore, the idea is to store the raw data and then create on-demand
 views of this raw data to handle all analysis that we might need in the
 future.
 The problems of this approach, usual in traditional Data Warehouses systems
 are that the 
\color red
transformations become permanent
\color inherit
, the 
\color red
schema is fixed
\color inherit
 and that the users of the data need to comply with the schema as well,
 imposing them to learn the schema, what can be done, and what cannot (
\color red
high entry barriers
\color inherit
).
\end_layout

\begin_layout Subsection
Identify the difficulties of a Data Lake
\end_layout

\begin_layout Standard
If we don't organize properly the Data Lake, then it could become really
 hard to track all the data that is stored in it, effectively transforming
 the Data Lake into what is called a 
\color red
Data Swamp
\color inherit
.
 The second problem is that, since each required analysis will need 
\color red
specific processing
\color inherit
, it is easy to end up having lots of different transformation pipelines,
 which are very case-specific and can be hard to reuse.
\end_layout

\begin_layout Subsection
Explain the need of each component in the Bolster Architecture
\end_layout

\begin_layout Standard
The Bolster Architecture is a reference architecture for Big Data systems.
 It was designed to facilitate the design of Big Data systems in multiple
 organizations and performed by a research-oriented team.
 The architecture is based on two families of Big Data architectures and
 leverages the Big Data dimensions (the five "V's") and the requirements
 defined for each of them.
\end_layout

\begin_layout Standard
The Bolster Architecture consists of several components, each with a specific
 function:
\end_layout

\begin_layout Itemize

\series bold
Semantic Layer
\series default
: This layer contains the Metadata Management System (MDM), which is responsible
 for providing the other components with the necessary information to describe
 and model raw data, as well as keeping the footprint about data usage.
 The MDM contains all the metadata artifacts, represented by means of RDF
 ontologies leveraging the benefits provided by Semantic Web technologies,
 needed to deal with data governance and assist data exploitation.
\end_layout

\begin_layout Itemize

\series bold
Batch Layer
\series default
: This layer stores a copy of the master data set in raw format as data
 are ingested.
 This layer also pre-computes Batch Views that are provided to the Serving
 Layer.
\end_layout

\begin_layout Itemize

\series bold
Speed Layer
\series default
: This layer ingests and processes real-time data in the form of streams.
 Results are then stored, indexed, and published in Real-time Views.
\end_layout

\begin_layout Itemize

\series bold
Serving Layer
\series default
: Similarly to the Speed Layer, this layer also stores, indexes, and publishes
 data resulting from the Batch Layer processing in Batch Views.
\end_layout

\begin_layout Standard
The Bolster Architecture addresses the two main drawbacks identified in
 the 
\begin_inset Formula $\lambda$
\end_inset

-architecture: it considers Variety, Variability, and Veracity as first-class
 citizens and refines the 
\begin_inset Formula $\lambda$
\end_inset

-architecture to facilitate its instantiation.
 These changes boil down to a precise definition of the components and their
 interconnections.
\end_layout

\begin_layout Subsection
Map the components of the Bolster Architecture to the RDBMS Architecture
\end_layout

\begin_layout Standard
The 
\color blue
Semantic Layer
\color inherit
 in the Bolster Architecture, which contains the Metadata Management System
 (MDM), can be compared to the 
\color red
system catalog
\color inherit
 in an RDBMS.
 The MDM stores machine-readable semantic annotations, similar to how the
 system catalog in an RDBMS stores metadata about the database.
\end_layout

\begin_layout Standard
The 
\color blue
Batch Layer, Speed Layer, and Serving Layer
\color inherit
 in the Bolster Architecture handle data storage and processing.
 These layers can be compared to the 
\color red
data storage and processing
\color inherit
 components of an RDBMS.
\end_layout

\begin_layout Standard
The Batch Layer stores a copy of the master data set in raw format as data
 are ingested and pre-computes Batch Views that are provided to the Serving
 Layer.
 This is similar to how an RDBMS stores and processes data.
\end_layout

\begin_layout Standard
The Speed Layer ingests and processes real-time data in the form of streams.
 Results are then stored, indexed, and published in Real-time Views.
 This can be compared to the real-time data processing capabilities of some
 RDBMSs.
\end_layout

\begin_layout Standard
The Serving Layer stores, indexes, and publishes data resulting from the
 Batch Layer processing in Batch Views.
 This is similar to how an RDBMS stores, indexes, and retrieves data.
\end_layout

\begin_layout Standard
However, it's important to note that the Bolster Architecture and an RDBMS
 have different purposes and are designed to handle different types of data
 and workloads.
 The Bolster Architecture is designed for Big Data systems, which often
 involve processing large volumes of unstructured or semi-structured data,
 while an RDBMS is typically used for structured data and transactional
 workloads.
\end_layout

\begin_layout Subsection
Given a use case, define its software architecture
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
In-Memory Columnar Databases (New Relational Architecture)
\end_layout

\begin_layout Subsection
Justify the viability of in-memory databases
\end_layout

\begin_layout Standard
In-memory databases store data in the main memory (RAM) of a computer rather
 than on disk, which results in much faster access times and thus faster
 query processing.
 Here are some reasons justifying their viability:
\end_layout

\begin_layout Itemize
Increased performance: Accessing data in memory is orders of magnitude faster
 than disk-based storage, reducing response times for data retrieval and
 analysis.
\end_layout

\begin_layout Itemize
Real-time analytics: In-memory databases can handle complex queries and
 perform transactions in real-time, which is crucial for applications requiring
 immediate insight, like financial trading systems or telecommunications
 networks.
\end_layout

\begin_layout Itemize
Scalability: Modern systems provide capabilities to distribute data across
 multiple servers, allowing in-memory databases to scale horizontally to
 accommodate larger data sets.
\end_layout

\begin_layout Itemize
Reduced complexity: Because data is stored in memory, there is no need to
 maintain indexes or perform complex optimizations that are typically required
 with disk-based databases, simplifying database management.
\end_layout

\begin_layout Standard
However, it's worth noting that in-memory databases also have their trade-offs.
 They can be more expensive due to the cost of RAM, and they can also face
 challenges with data persistence, as data in memory is volatile.
 But these challenges can often be overcome with techniques like database
 replication and persistent storage backups.
\end_layout

\begin_layout Subsection
Explain the principles of NUMA architecture
\end_layout

\begin_layout Standard
NUMA (Non-Uniform Memory Access) is a computer memory design used in multiproces
sing, where the memory access time depends on the memory location relative
 to the processor.
 Under NUMA, a processor can access its own local memory faster than non-local
 memory (memory local to another processor or memory shared between processors).
 This facilitates scaling up by growing a single machine.
 There are two main trade-offs:
\end_layout

\begin_layout Itemize
The smaller the memory, the less latency and higher bandwidth.
\end_layout

\begin_layout Itemize
The more performant the memory, the more expensive it is.
\end_layout

\begin_layout Standard
Also, this architecture allows parallelism in the different cores and CPUs.
\end_layout

\begin_layout Standard
This architecture is also much faster and expensive than a cluster of shared-not
hing machines, but it's clear that they are also much harder to manage if
 we want to push it to its full potential.
 The principles of NUMA are:
\end_layout

\begin_layout Itemize
Local Memory: Each processor in a NUMA architecture has its own local memory.
 When a processor needs to access data from its own local memory, it can
 do so quickly.
\end_layout

\begin_layout Itemize
Non-local Memory: When a processor needs to access memory that isn't local
 (i.e., memory that is attached to a different processor), it can take longer.
 This is because the processor must go over the interconnect bus to reach
 the memory associated with the other processor.
\end_layout

\begin_layout Itemize
Processor Interconnect: The processors in a NUMA system are connected through
 an interconnect bus.
 This interconnect can be used to share data between processors.
\end_layout

\begin_layout Itemize
Scalability: NUMA architecture is designed to scale.
 As more processors are added, more local memory is added to the system,
 potentially improving overall system performance.
\end_layout

\begin_layout Standard
NUMA architectures are used in systems with large numbers of processors,
 providing benefits over traditional symmetric multiprocessing (SMP) systems.
 However, to get the best performance from a NUMA system, software (like
 a database management system) should be NUMA-aware and designed to minimize
 non-local memory accesses.
\end_layout

\begin_layout Subsection
Enumerate 3 techniques to optimize cache usage
\end_layout

\begin_layout Enumerate

\series bold
Locality awareness
\series default
: aims at reducing the number of idle CPU cycles waiting for the arrival
 of data.
 The system can benefit from 
\color blue
spacial locality
\color inherit
, i.e., there is a high chance of accessing data contiguous to data already
 accessed; and 
\color blue
temporal locality
\color inherit
, i.e., there is a high chance of accessing data that has been recently accessed.
\end_layout

\begin_layout Enumerate

\series bold
Flexible caching
\series default
: has the purpose of bringing and keeping as much relevant data as possible
 in cache.
 It is related to associativity which describes how different blocks of
 memory map to the same cache location.
 A fully associative cache allows a memory block to be placed in any cache
 location.
 This flexibility allows the cache to store more relevant data, but it comes
 at the cost of increased complexity and slower lookup times.
 Therefore: low associativity facilitates searching, since a disk block
 can only be found in few places, but complicates replacement.
 High associativity facilitates replacement, but makes it more difficult
 for searching.
\end_layout

\begin_layout Enumerate

\series bold
Cache-concious design
\series default
: this is relevant for DBMS developers, that need to be aware of cache line
 size, to only use multiple of that size in the code, optimizing access
 to data.
\end_layout

\begin_layout Subsection
Give 4 arguments supporting columnar storage
\end_layout

\begin_layout Itemize
Higher data compression ratios.
 Columnar storage allows for better data compression, as the data stored
 in a column is usually of the same type, leading to a high degree of redundancy.
 Techniques like run-length encoding, dictionary encoding, and bitmap encoding
 can be applied for more effective data compression.
\end_layout

\begin_layout Itemize
Higher performance for column operations.
 Columnar storage is ideal for OLAP (Online Analytical Processing) queries
 which typically scan and aggregate over a small subset of the total columns
 in the table.
 Since data is stored column-wise, it can lead to significant IO reduction
 and faster execution times.
\end_layout

\begin_layout Itemize
Parallelization.
 Columnar storage enables efficient parallel processing, as operations can
 be performed on individual columns independently.
 This allows for more efficient use of modern hardware architectures, including
 multi-core CPUs and vectorized instruction sets.
\end_layout

\begin_layout Itemize
Elimination of additional indexes.
\end_layout

\begin_layout Subsection
Explain 3 classical optimization techniques in RDBMS related to column storage
\end_layout

\begin_layout Itemize
Vertical partitioning: each table can be split as a set of two-column partitions
 
\begin_inset Formula $\left(key,attributes\right)$
\end_inset

, which improves useful reads ratio.
 This helps in optimizing query performance by reducing the amount of data
 scanned for a particular query.
 If a query only requires a subset of the total columns in a table, it's
 more efficient to scan a vertically partitioned table that contains only
 those necessary columns.
\end_layout

\begin_layout Itemize
Use index-only query plans: we can create a collection of indexes that cover
 all columns used in a query, so no table access is needed.
\end_layout

\begin_layout Itemize
Use a collection of materialized views, such that there is a view with exactly
 the columns needed to answer the queries.
\end_layout

\begin_layout Subsection
Sketch the functional architecture of SanssouciuDB
\end_layout

\begin_layout Subsection
Explain 4 optimizations implemented in SanssouciuDB to improve data access
\end_layout

\begin_layout Itemize
Use stored procedures.
\end_layout

\begin_layout Itemize
Data ages by dynamic horizontal partitioning, depending on the lifecycle
 of objects.
 By default, only active data is incorporated into query processing.
 The definition of active data is given by the application.
\end_layout

\begin_layout Itemize
Modifications are performed on a differential buffer, with a merge process
 carried out per table.
 This implies decompressing the table and compressing everything back again.
 This is done on-line.
\end_layout

\begin_layout Itemize
There are append-only tables, being timepoint representation for OLTP and
 interval representation for OLAP.
\end_layout

\begin_layout Subsection
Explain how to choose the best layout
\end_layout

\begin_layout Standard
We should choose 
\series bold
columnar storage
\series default
 when:
\end_layout

\begin_layout Itemize
Calculations are executed on a single column or a few columns only.
\end_layout

\begin_layout Itemize
The table is searched based on the values of a few columns.
\end_layout

\begin_layout Itemize
The table has a large number of columns.
\end_layout

\begin_layout Itemize
The table has a large number of rows, and columnar operations are required.
\end_layout

\begin_layout Itemize
The majority of columns contain only a few distinct values, resulting in
 high compression rates.
\end_layout

\begin_layout Standard
We should choose 
\series bold
row storage
\series default
 when:
\end_layout

\begin_layout Itemize
The table has a small number of rows, such as configuration tables.
\end_layout

\begin_layout Itemize
The application needs to process only a single record at a time (many selects
 of updates of single records).
\end_layout

\begin_layout Itemize
The application typically needs to access the complete records.
\end_layout

\begin_layout Itemize
The columns contain mainly distinct values, so compression rates would be
 low.
\end_layout

\begin_layout Itemize
Aggregations and fast searching are not required.
\end_layout

\begin_layout Subsection
Identify the difference between column-stores and NOSQL related to transactions
\end_layout

\begin_layout Standard
In column-stores, we avoid replication management by only storing atomic
 data, relying on the performance obtained in aggregate queries.
 With on-the-fly aggregation, the aggregate values are always up-to-date.
 Also, we reach this way higher concurrency, since we can aggregate different
 measures concurrently for different columns.
 We can still replicate some small and static tables.
 Since these systems provide full ACID support, the consistency is strong,
 in a eager/secondary-copy synchronization mechanism scheme.
\end_layout

\begin_layout Subsection
Explain 3 difficulties of pipelining in column-stores
\end_layout

\begin_layout Enumerate

\series bold
Short process trees
\series default
: In column-stores, operations are often applied to each column independently,
 which can result in a shallow process tree (with few levels of nested operation
s).
 This can limit the opportunities for pipelining, as each operation may
 complete quickly and there may be fewer opportunities to overlap operations.
\end_layout

\begin_layout Enumerate
Some operators need 
\series bold
all input data at once
\series default
: Certain operators, like sort or aggregate, need to process the entire
 input column before they can produce any output.
 This can make it more difficult to pipeline operations, as you have to
 wait for these operators to complete before passing their output to the
 next operator in the pipeline.
\end_layout

\begin_layout Enumerate

\series bold
Skewed cost of operations
\series default
: In column-stores, the cost of operations can vary significantly depending
 on the specific column and operation.
 For example, operations on a column with a high cardinality (many unique
 values) or a large amount of data may be more expensive than operations
 on a column with low cardinality or less data.
 This skew can make it harder to evenly distribute work across a pipeline
 and maintain high throughput.
\end_layout

\begin_layout Subsection
Explain 3 problems to implement parallelism in in-memory column-stores
\end_layout

\begin_layout Enumerate

\series bold
High startup cost
\series default
: Parallelism involves dividing a task among multiple processing units.
 However, there is a startup cost associated with setting up these parallel
 tasks, including the time to divide the task and synchronize the results.
 If the tasks are too small or the number of tasks is too high, the overhead
 of managing these tasks can outweigh the benefits of parallel execution.
\end_layout

\begin_layout Enumerate

\series bold
Contention
\series default
 (at harware level): Parallel tasks often need to access shared resources,
 such as memory or input/output devices.
 If multiple tasks try to access these resources at the same time, they
 can interfere with each other, leading to contention.
 This can degrade performance and limit the effectiveness of parallel execution.
\end_layout

\begin_layout Enumerate

\series bold
Skewness
\series default
: refers to a situation where the data or the workload is not evenly distributed
 across the tasks.
 For example, if one task gets assigned a larger portion of the data or
 a more complex operation, it can take longer to complete than the other
 tasks.
 This can cause the other tasks to sit idle while waiting for the skewed
 task to complete, reducing the overall efficiency of parallel execution.
\end_layout

\begin_layout Subsection
Explain 5 query optimization techniques specific to columnar storage
\end_layout

\begin_layout Itemize

\series bold
Late materialization
\series default
: In column-stores, a query result is usually composed by combining values
 from multiple columns.
 With late materialization, the database delays combining these values as
 long as possible.
 This allows the database to operate on smaller amounts of data, which can
 significantly improve performance for queries that only need a subset of
 a table's columns.
\end_layout

\begin_layout Itemize

\series bold
Tuples are identified by position
\series default
: In columnar databases, tuples (or rows) are often identified by their
 position in the column rather than by a unique key.
 This allows the database to directly access the relevant data in each column
 without having to perform an additional lookup, which can improve performance.
\end_layout

\begin_layout Itemize

\series bold
Column-specific compression techniques
\series default
: Because all the values in a column are of the same type, columnar databases
 can use specialized compression techniques that take advantage of this
 uniformity.
 Techniques such as dictionary encoding, run-length encoding, and bitmap
 encoding can achieve high compression rates and improve query performance
 by reducing I/O.
\end_layout

\begin_layout Itemize

\series bold
Block iteration
\series default
: Columnar databases often store data in blocks, each containing a subset
 of a column's values.
 By iterating over these blocks instead of individual values, the database
 can reduce CPU cache misses and improve performance.
 Values inside a block can be iterated as in an array (they are fixed-width),
 codified/compressed together, and exploited for parallelism of pipelining.
 When we combine block iteration with late materialization, we are doing
 what is called 
\series bold
vectorized query processing
\series default
.
\end_layout

\begin_layout Itemize

\series bold
Specific join algorithms
\series default
: Joining tables is a common operation in relational databases.
 In columnar databases, join operations can be optimized by using algorithms
 that take advantage of the columnar storage format.
 For example, a hash join can be more efficient in a columnar database because
 it operates on individual columns and can take advantage of column-specific
 compression.
\end_layout

\begin_layout Subsection
Given a data setting, justify the choice of either row or column storage
\end_layout

\begin_layout Subsection
Given the data in a column, use run-length encoding with dictionary to compress
 it
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "upc"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
