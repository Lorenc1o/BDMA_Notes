{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mI-_EvHopCX"
      },
      "source": [
        "# TP1: Basics of Deep Learning and Pytorch \n",
        "\n",
        "**Authors:** \n",
        "- tom.dupuis@centralesupelec.fr\n",
        "\n",
        "\n",
        "If you have questions or suggestions, contact us and we will gladly answer and take into account your remarks.\n",
        "\n",
        "$\\newcommand{\\underbr}[2]{\\underbrace{#1}_{\\scriptscriptstyle{#2}}}$\n",
        "\n",
        "## Objective:\n",
        "We want to implement a two layers Multi-Layer Perceptron (MLP) with 1 hidden layer in Pytorch, for a binary classification problem.\n",
        "\n",
        "The output of the network is simply the output of several cascaded functions :\n",
        "- Linear transformations. We note the weights of a linear transformation with $W$\n",
        "- Additive biases. We note the parameters of additive biases  with $b$\n",
        "- Non-linearities.\n",
        "\n",
        "For this, we will implement in the first part of the TP:\n",
        "- the forward propagation\n",
        "- the computation of the loss\n",
        "- the backward propagation (to obtain the gradients)\n",
        "- the update of the parameters\n",
        "\n",
        "In the second part we will simply use pytorch API with a multi-classification problem.\n",
        "\n",
        "Furthermore, we define the following sizes :\n",
        "\n",
        "- $n^{[0]}$ : number of input neurons\n",
        "- $n^{[1]}$ : number of neurons in hidden layer\n",
        "- $n^{[2]}$ : number of neurons in output layer\n",
        "- $m$ : number of training datapoints\n",
        "\n",
        "### Loss function \n",
        "\n",
        "We want to solve a binary classification problem. Therefore we will use the binary cross-entropy loss function. The total loss function will be the average of the **loss** over the training data.\n",
        "\n",
        "$\\mathcal{L} = - \\left( y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y}) \\right),$\n",
        "\n",
        "where \n",
        "- $y$ is the ground-truth labels of the data \n",
        "- $\\hat{y}$ the predicted labels outputed by the network.\n",
        "\n",
        "### Forward propagation\n",
        "\n",
        "- $\\large \\underbr{Z^{[1]}}{(m,n^{[1]})} = \\underbr{X}{(m,n^{[0]})} \\underbr{W^{[1]}}{(n^{[0]},n^{[1]})}  + \\underbr{b^{[1]}}{n^{(1)}} $\n",
        "- $\\large \\underbr{A^{[1]}}{(m,n^{[1]})} = f(Z^{[1]})$\n",
        "- $\\large \\underbr{Z^{[2]}}{(m,n^{[2]})} = \\underbr{A^{[1]}}{(m,n^{[1]})} \\underbr{W^{[2]}}{(n^{[1]},n^{[2]})}  + \\underbr{b^{[2]}}{n^{(2)}}$\n",
        "- $\\large \\underbr{A^{[2]}}{(m,n^{[2]})} = \\sigma(Z^{[2]})$\n",
        "\n",
        "where \n",
        "- $f$ is a ```Relu``` function (the code is provided)\n",
        "- $\\sigma$ is a sigmoid function (the code is provided)\n",
        "\n",
        "### Backward propagation\n",
        "\n",
        "Notation:\n",
        "we note partial derivatives of the loss w.r.t. parameters as the following\n",
        "- $dA^{[i]}: \\frac{dLoss}{dA^{[i]}}$\n",
        "\n",
        "- $dZ^{[i]}: \\frac{dLoss}{dZ^{[i]}}$\n",
        "- $dW^{[i]}: \\frac{dLoss}{dW^{[i]}}$\n",
        "- $db^{[i]}: \\frac{dLoss}{db^{[i]}}$\n",
        "\n",
        "It is an abusive notation that simplifies variable naming in code.\n",
        "\n",
        "The backward propagation can be calculated as\n",
        "\n",
        "- $\\large \\underbr{dZ^{[2]}}{(m,n^{[2]})} = \\underbr{A^{[2]}}{(m,n^{[2]})} - \\underbr{Y}{(m,n^{[2]})}$\n",
        "- $\\large \\underbr{dW^{[2]}}{(n^{[1]},n^{[2]})} = \\frac{1}{m} {\\underbr{A^{[1]}}{(m,n^{[1]})}}^{T} \\underbr{dZ^{[2]}}{(m,n^{[2]})} $\n",
        "- $\\large \\underbr{db^{[2]}}{(n^{[2]})} = \\frac{1}{m} \\sum_{i=1}^{m} \\underbr{dZ^{[2]}}{(m,n^{[2]})}$\n",
        "\n",
        "- $\\large \\underbr{dA^{[1]}}{(m,n^{[1]})} = \\underbr{dZ^{[2]}}{(m,n^{[2]})} {\\underbr{W^{[2]}}{(n^{[1]},n^{[2]})}}^{T} $\n",
        "- $\\large \\underbr{dZ^{[1]}}{(m,n^{[1]})} = \\underbr{dA^{[1]}}{(m,n^{[1]})} \\: \\odot \\: f' (\\underbr{Z^{[1]}}{(m,n^{[1]})})$\n",
        "- $\\large \\underbr{dW^{[1]}}{(n^{[0]},n^{[1]})} = \\frac{1}{m} {\\underbr{X}{(m,n^{[0]})}}^{T} \\underbr{dZ^{[1]}}{(m,n^{[1]})} $\n",
        "- $\\large \\underbr{db^{[1]}}{(n^{[1]})} = \\frac{1}{m} \\sum_{i=1}^{m} \\underbr{dZ^{[1]}}{(m,n^{[1]})}$\n",
        "\n",
        "The $\\odot$ operator refers to the point-wise multiplication operation.\n",
        "\n",
        "### Backward propagation\n",
        "\n",
        "Based on the previous formulas, write the corresponding backpropagation algorithm.\n",
        "\n",
        "### Parameters update\n",
        "\n",
        "- Implement a **first version** in which the parameters are updated using a **simple gradient descent**:\n",
        "    - $W = W - \\alpha dW$\n",
        "\n",
        "\n",
        "- Implement a **second version** in which the parameters are updated using the **momentum method**:\n",
        "    - $V_{dW}(t) = \\beta V_{dW}(t-1) + (1-\\beta) dW$\n",
        "    - $W(t) = W(t-1) - \\alpha V_{dW}(t)$\n",
        "\n",
        "## Your task:\n",
        "\n",
        "You need to add the missing parts in the code (parts between ```# --- START CODE HERE``` and ```# --- END CODE HERE```)\n",
        "\n",
        "## Testing\n",
        "\n",
        "For testing your code, you can use the code provided in the last cells (loop over epochs and display of the loss decrease).\n",
        "You should observe a loss which decreases over epochs and see higher training accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OpFxAFiopCY"
      },
      "source": [
        "# Load packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c05fkCgFopCY"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzrpnPCEopCa"
      },
      "source": [
        "# Define a set of functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDDxBp_UopCb"
      },
      "outputs": [],
      "source": [
        "def F_standardize(X):\n",
        "    \"\"\"\n",
        "    standardize X, i.e. subtract mean (over data) and divide by standard-deviation (over data)\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X: torch.Tensor of size (m, n_0)\n",
        "        matrix containing the observation data\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    X: torch.Tensor of size (m, n_0)\n",
        "        standardize version of X\n",
        "    \"\"\"\n",
        "    \n",
        "    X -= torch.mean(X, dim=[0,1], keepdims=True) \n",
        "    X /= (torch.std(X, dim=[0,1], keepdims=True) + 1e-16)\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHfEONyMopCd"
      },
      "outputs": [],
      "source": [
        "def F_sigmoid(x):\n",
        "    \"\"\"Compute the value of the sigmoid activation function\"\"\"\n",
        "    return 1 / (1 + torch.exp(-x))\n",
        "\n",
        "def F_relu(x):\n",
        "    \"\"\"Compute the value of the Rectified Linear Unit activation function\"\"\"\n",
        "    return x * (x > 0)\n",
        "\n",
        "def F_dRelu(x):\n",
        "    \"\"\"Compute the derivative of the Rectified Linear Unit activation function\"\"\"\n",
        "    x[x<=0] = 0\n",
        "    x[x>0] = 1\n",
        "    return x\n",
        "\n",
        "def F_computeLoss(preds, y):\n",
        "    \"\"\"Compute the loss (sum of the unit losses)\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    preds: (m, 1)\n",
        "        predicted value by the MLP\n",
        "    y: (m, 1)\n",
        "        ground-truth class to predict\n",
        "    \"\"\"\n",
        "    m = y.shape[0]\n",
        "     \n",
        "    # --- START CODE HERE (01)\n",
        "    loss = \n",
        "    # --- END CODE HERE\n",
        "\n",
        "    loss = torch.sum(loss) / m\n",
        "    return loss\n",
        "\n",
        "def F_computeAccuracy(preds, y):\n",
        "    \"\"\"Compute the accuracy\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    preds: (m, 1)\n",
        "        predicted value by the MLP\n",
        "    y: (m, 1)\n",
        "        ground-truth class to predict\n",
        "    \"\"\"\n",
        "    \n",
        "    m = y.shape[0]\n",
        "    if preds.shape[1] == 1:\n",
        "      hard_preds = (preds > 0.5).to(torch.int64)\n",
        "    else:\n",
        "      hard_preds = (torch.argmax(preds, 1)).to(torch.int64)\n",
        "    return torch.sum(hard_preds==y) / m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8xKaXHlopCe"
      },
      "source": [
        "# Load dataset and pre-process it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLPLtzRcYk3Z"
      },
      "source": [
        "We will use the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset that is a classification dataset widely known for its ease of use and the low ressources recquired.\n",
        "\n",
        "It is composed of black and white hand-written digits with a resolution of 28x28. It has 60k training images and 10k test images. We will take the torchvision dataset implementation to load the dataset that allows us to download it and index elements.\n",
        "\n",
        "To simplify our problem we will only keep digits 0 and 1 of MNIST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbzAk6EuZjJV"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms.functional as F\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "def show(imgs):\n",
        "    if not isinstance(imgs, list):\n",
        "        imgs = [imgs]\n",
        "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
        "    for i, img in enumerate(imgs):\n",
        "        img = img.detach()\n",
        "        img = F.to_pil_image(img)\n",
        "        axs[0, i].imshow(np.asarray(img))\n",
        "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kU3HU2mnopCf"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "dataset_train = torchvision.datasets.MNIST(root='', train=True, transform=ToTensor(), download=True) # Load the training part of MNIST dataset\n",
        "dataset_test = torchvision.datasets.MNIST(root='', train=False, transform=ToTensor(), download=True) # Load the testing part of MNIST dataset\n",
        "\n",
        "digit_index_0, label_index_0 = dataset_train[0]\n",
        "print(f\"Shape of first digit: {digit_index_0.shape}, label of first digit: {label_index_0}\")\n",
        "\n",
        "show(digit_index_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHy9ZW9EV4xy"
      },
      "outputs": [],
      "source": [
        "random_train_samples = [dataset_train[idx] for idx in np.random.choice(len(dataset_train), 16)]\n",
        "random_train_X = [sample[0] for sample in random_train_samples]\n",
        "random_train_y = [sample[1] for sample in random_train_samples]\n",
        "train_grid = make_grid(random_train_X)\n",
        "\n",
        "print(f\"Shape of train images: {random_train_X[0].shape}.\")\n",
        "print(f\"Labels:\\n{random_train_y}.\\n\")\n",
        "print(f\"Grid of train images:\")\n",
        "show(train_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR8CAGBnV6ja"
      },
      "outputs": [],
      "source": [
        "random_test_samples = [dataset_test[idx] for idx in np.random.choice(len(dataset_test), 16)]\n",
        "random_test_X = [sample[0] for sample in random_test_samples]\n",
        "random_test_y = [sample[1] for sample in random_test_samples]\n",
        "test_grid = make_grid(random_test_X)\n",
        "\n",
        "print(f\"Shape of test images: {random_test_X[0].shape}.\")\n",
        "print(f\"Labels:\\n{random_test_y}.\\n\")\n",
        "print(f\"Grid of test images:\")\n",
        "\n",
        "show(test_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DgWMmqOdny-"
      },
      "outputs": [],
      "source": [
        "def get_decoded_data_from_dataset(dataset):\n",
        "  samples = [sample for sample in dataset]\n",
        "  X_split = [sample[0] for sample in samples]\n",
        "  y_split = [sample[1] for sample in samples]\n",
        "\n",
        "  return torch.cat(X_split), torch.tensor(y_split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xR1zNuEWopCh"
      },
      "outputs": [],
      "source": [
        "# --- Split between training set and test set\n",
        "# --- (m, n_0)\n",
        "\n",
        "X_train, y_train = get_decoded_data_from_dataset(dataset_train)\n",
        "X_test, y_test = get_decoded_data_from_dataset(dataset_test)\n",
        "\n",
        "# Flatten data\n",
        "# --- Convert to proper shape: (m, 1, 28, 28) -> (m, 784)\n",
        "y_train = y_train.view((-1,1))\n",
        "y_test = y_test.view((-1, 1))\n",
        "\n",
        "X_train = X_train.view((X_train.shape[0], -1))\n",
        "X_test = X_test.view((X_test.shape[0], -1))\n",
        "\n",
        "# Only select classes 0 and 1\n",
        "indexes_train = torch.logical_or(y_train == 0, y_train == 1)\n",
        "indexes_test = torch.logical_or(y_test == 0, y_test == 1)\n",
        "\n",
        "X_train, y_train = X_train[indexes_train[:, 0]], y_train[indexes_train[:, 0]]\n",
        "X_test, y_test = X_test[indexes_test[:, 0]], y_test[indexes_test[:, 0]]\n",
        "\n",
        "# --- Standardize data\n",
        "\n",
        "X_train = F_standardize(X_train)\n",
        "X_test = F_standardize(X_test)\n",
        "\n",
        "# --- Convert to oneHotEncoding: (nbExamples, 1) -> (nbExamples, nbClass)\n",
        "n_0 = X_train.shape[1]\n",
        "n_2 = 1\n",
        "\n",
        "print(\"X_train.shape: {}\".format(X_train.shape))\n",
        "print(\"X_test.shape: {}\".format(X_test.shape))\n",
        "print(\"y_train.shape: {}\".format(y_train.shape))\n",
        "print(\"y_test.shape: {}\".format(y_test.shape))\n",
        "print(\"y_train.shape: {}\".format(y_train.shape))\n",
        "print(\"y_test.shape: {}\".format(y_test.shape))\n",
        "print(\"n_0=n_in: {} n_2=n_out: {}\".format(n_0, n_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFeLTEtqopCj"
      },
      "source": [
        "# Define the MLP class with forward, backward and update methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMbOr4aEopCk"
      },
      "outputs": [],
      "source": [
        "class CustomMLP():\n",
        "    \"\"\"\n",
        "    A class used to represent a Multi-Layer Perceptron with 1 hidden layer\n",
        "\n",
        "    ...\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    W1, b1, W2, b2:\n",
        "        weights and biases to be learnt\n",
        "    Z1, A1, Z2, A2:\n",
        "        values of the internal neurons to be used for backpropagation\n",
        "    dW1, db1, dW2, db2, dZ1, dZ2:\n",
        "        partial derivatives of the loss w.r.t. parameters\n",
        "    VdW1, Vdb1, VdW2, Vdb2:\n",
        "        momentum terms\n",
        "        \n",
        "    Methods\n",
        "    -------\n",
        "    forward_propagation\n",
        "    \n",
        "    backward_propagation\n",
        "    \n",
        "    update_parameters\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    #W1, b1, W2, b2 = [], [], [], []\n",
        "    #A0, Z1, A1, Z2, A2 = [], [], [], [], []\n",
        "    #dW1, db1, dW2, db2 = [], [], [], []   \n",
        "    #dZ1, dA1, dZ2 = [], [], []\n",
        "    # --- for momentum\n",
        "    #VdW1, Vdb1, VdW2, Vdb2 = [], [], [], []     \n",
        "    \n",
        "    def __init__(self, n_0, n_1, n_2):\n",
        "        self.W1 = torch.randn(n_0, n_1) * 0.01\n",
        "        self.b1 = torch.zeros((1, n_1))\n",
        "        self.W2 = torch.randn(n_1, n_2) * 0.01\n",
        "        self.b2 = torch.zeros((1, n_2))        \n",
        "        # --- for momentum\n",
        "        self.VdW1 = torch.zeros((n_0, n_1)) \n",
        "        self.Vdb1 = torch.zeros((1, n_1))\n",
        "        self.VdW2 = torch.zeros((n_1, n_2))\n",
        "        self.Vdb2 = torch.zeros((1, n_2))\n",
        "        return            \n",
        "\n",
        "    def M_forwardPropagation(self, X):\n",
        "        \"\"\"Forward propagation in the MLP\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: torch.Tensor (nbData, nbDim)\n",
        "            observation data\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        preds: torch.Tensor (nbData, 1)\n",
        "            predicted value by the MLP\n",
        "        \"\"\"\n",
        "        \n",
        "        # --- START CODE HERE (02)\n",
        "        self.A0 = \n",
        "        \n",
        "        self.Z1 =\n",
        "        self.A1 = \n",
        "        \n",
        "        self.Z2 =\n",
        "        self.A2 =\n",
        "        \n",
        "        preds = \n",
        "        # --- END CODE HERE\n",
        "        \n",
        "        return preds\n",
        "\n",
        "\n",
        "    def M_backwardPropagation(self, X, y):\n",
        "        \"\"\"Backward propagation in the MLP\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: torch.Tensor (nbData, nbDim)\n",
        "            observation data\n",
        "        y: torch.Tensor (nbData, 1)\n",
        "            ground-truth class to predict\n",
        "            \n",
        "        \"\"\"\n",
        "        \n",
        "        m = y.shape[0]\n",
        "        \n",
        "        # --- START CODE HERE (03)\n",
        "\n",
        "        self.dZ2 =\n",
        "        self.dW2 =\n",
        "        self.db2 =\n",
        "        self.dA1 =\n",
        "\n",
        "        self.dZ1 =\n",
        "        self.dW1 =\n",
        "        self.db1 =\n",
        "\n",
        "        # --- END CODE HERE\n",
        "\n",
        "        return\n",
        "\n",
        "    \n",
        "    def M_gradientDescent(self, alpha):\n",
        "        \"\"\"Update the parameters of the network using gradient descent\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        alpha: float scalar\n",
        "            amount of update at each step of the gradient descent\n",
        "            \n",
        "        \"\"\"\n",
        "        # --- START CODE HERE (04)\n",
        "        self.W1 -=\n",
        "        self.b1 -= \n",
        "        self.W2 -= \n",
        "        self.b2 -= \n",
        "        # --- END CODE HERE\n",
        "            \n",
        "        return\n",
        "\n",
        "    \n",
        "    def M_momentum(self, alpha, beta):\n",
        "        \"\"\"Update the parameters of the network using momentum method\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        alpha: float scalar\n",
        "            amount of update at each step of the gradient descent\n",
        "        beta: float scalar\n",
        "            momentum term \n",
        "        \"\"\"\n",
        "        \n",
        "        # --- START CODE HERE (05)\n",
        "        self.VdW1 =\n",
        "        self.W1 -=\n",
        "\n",
        "        self.Vdb1 =\n",
        "        self.b1 -=\n",
        "\n",
        "        self.VdW2 =\n",
        "        self.W2 -=\n",
        "\n",
        "        self.Vdb2 =\n",
        "        self.b2 -=\n",
        "        # --- END CODE HERE\n",
        "                \n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aRPi2oQopCm"
      },
      "source": [
        "# Perform training using batch-gradiant and epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9vEi2gsopCm"
      },
      "outputs": [],
      "source": [
        "# hyper-parameters\n",
        "n_1 = 10 # number of hidden neurons\n",
        "nb_epoch = 10 # number of epochs (number of iterations over full training set)\n",
        "alpha=0.1 # learning rate\n",
        "beta=0.9 # beta parameters for momentum\n",
        "\n",
        "\n",
        "# Instantiate the class MLP with providing \n",
        "# the size of the various layers (n_0=n_input, n_1=n_hidden, n_2=n_output) \n",
        "myMLP = CustomMLP(n_0, n_1, n_2)\n",
        "\n",
        "train_loss, train_accuracy, test_loss, test_accuracy = [], [], [], []\n",
        "\n",
        "# Run over epochs\n",
        "for num_epoch in range(1, nb_epoch + 1):\n",
        "    \n",
        "    # --- Forward\n",
        "    train_preds = myMLP.M_forwardPropagation(X_train)\n",
        "    # --- Store results on train\n",
        "    train_loss.append(F_computeLoss(train_preds, y_train))\n",
        "    train_accuracy.append(F_computeAccuracy(train_preds, y_train))\n",
        "    \n",
        "    # --- Backward\n",
        "    myMLP.M_backwardPropagation(X_train, y_train)\n",
        "    \n",
        "    # --- Update\n",
        "    myMLP.M_gradientDescent(alpha)\n",
        "    #myMLP.M_momentum(alpha, beta)\n",
        "\n",
        "    # --- Store results on test\n",
        "    test_preds = myMLP.M_forwardPropagation(X_test)\n",
        "    test_loss.append(F_computeLoss(test_preds, y_test))    \n",
        "    test_accuracy.append(F_computeAccuracy(test_preds, y_test))\n",
        "    \n",
        "    if (num_epoch % 1)==0: \n",
        "        print(\"epoch: {0:d} (loss: train {1:.2f} test {2:.2f}) (accuracy: train {3:.2f} test {4:.2f})\".format(num_epoch, train_loss[-1], test_loss[-1], train_accuracy[-1], test_accuracy[-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWH7NblhopCo"
      },
      "source": [
        "## Display train/test loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTQXet5HmKCw"
      },
      "outputs": [],
      "source": [
        "def display_losses_and_accuracies(train_loss, test_loss, train_accuracy, test_accuracy):\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(train_loss, 'r')\n",
        "  plt.plot(test_loss, 'g--')\n",
        "  plt.xlabel('# epoch')\n",
        "  plt.ylabel('loss')\n",
        "  plt.grid(True)\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(train_accuracy, 'r')\n",
        "  plt.plot(test_accuracy, 'g--')\n",
        "  plt.xlabel('# epoch')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.grid(True)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLLjoteXopCp"
      },
      "outputs": [],
      "source": [
        "display_losses_and_accuracies(train_loss, test_loss, train_accuracy, test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFMzlIH8ngsi"
      },
      "source": [
        "## Pytorch MNIST classification using a one-hidden layer MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_tP4IYSd3Ek"
      },
      "source": [
        "In this section, you will train a MLP with one hidden layer to perform the classification of all MNIST images.\n",
        "\n",
        "The objective function becomes the cross entropy and $n^{[2]}=10$ for 10 classes of digits.\n",
        "\n",
        "The code to load the normalized dataset is provided, the missing code parts are inside the training loop.\n",
        "\n",
        "The structure of the model is the following:\n",
        "- Flatten layer to reshape the input to the shape (B, $n_0$)\n",
        "- Linear layer that takes as input $n_0$ features and outputs $n_1$ features\n",
        "- ReLU activation\n",
        "- Linear layer that takes as input $n_1$ features and outputs $n_2$ logits\n",
        "\n",
        "To create the model, use the [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential) model and pass the adequate [layers](https://pytorch.org/docs/stable/nn.html).\n",
        "\n",
        "In our case the parameters are:\n",
        "- $n_0 = 28 \\times 28 \\times 1 = 784$ (number of pixels)\n",
        "- $n_1 = 10$\n",
        "- $n_2 = 10$ (digits of MNIST 0 to 9)\n",
        "\n",
        "The optimizer to update the model is [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html). You can test to use momentum or not.\n",
        "\n",
        "The [loss](https://pytorch.org/docs/stable/nn.html#loss-functions) function is the cross entropy that is a class that needs to be instantiate in Pytorch.\n",
        "\n",
        "You will implement a training loop that updates the parameters at each iteration based on an [optimizer loop](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#optimization-loop). You will need to reset gradients to zero at each iteration before computing new gradients thanks to backpropagation to avoid accumulating gradients throughout training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDVs9bHWfcZ1"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQYnibs_opCs"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Flatten, Sequential, Linear, ReLU, CrossEntropyLoss\n",
        "from torch.optim import SGD\n",
        "from torchvision.transforms import Compose, Normalize\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "transform = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))]) # Convert the images to tensor and normalize the inputs.\n",
        "dataset_train = torchvision.datasets.MNIST(root='', train=True, transform=transform, download=True)\n",
        "dataset_test = torchvision.datasets.MNIST(root='', train=False, transform=transform, download=True)\n",
        "\n",
        "train_dataloader = DataLoader(dataset_train, shuffle=False, batch_size=len(dataset_train), drop_last=False)\n",
        "test_dataloader = DataLoader(dataset_test, shuffle=False, batch_size=len(dataset_test), drop_last=False)\n",
        "\n",
        "### Uncomment for using mini batches, however you would have to update the training loop for computing epoch accuracies and losses.\n",
        "#train_dataloader = DataLoader(dataset_train, shuffle=True, batch_size=256, drop_last=True)\n",
        "#train_dataloader = DataLoader(dataset_train, shuffle=False, batch_size=256, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBzFzXHifZ_r"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "The slowness of the training loop in comparison with the handcrafted MLP is expected. It is due to the fact that in the first part of the TP, the images were decoded before going in training.\n",
        "\n",
        "Generally in deep learning, because of RAM limitations, the images are decoded at each iteration, therefore it takes longer due to the overhead of the image decoded. This is how it is performed here through the common dataset into dataloader pipeline.\n",
        "\n",
        "\n",
        "\n",
        "**Cells instructions**:\n",
        "\n",
        "- Cell 06: \n",
        "  - instantiate the model\n",
        "  - instantiate the optimizer \n",
        "  - instantiate the loss function.\n",
        "- Cell 07: \n",
        "  - compute the model outputs \n",
        "  - feed to the loss function the outputs aswell as the targets (or labels) to retrieve the loss.\n",
        "- Cell 08: \n",
        "  - reset the optimizer gradients\n",
        "  - compute the backpropagation\n",
        "- Cell 09:\n",
        "  - update the parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNlU81ZGpwlV"
      },
      "outputs": [],
      "source": [
        "# hyper-parameters\n",
        "n_1 = 10 # number of hidden neurons\n",
        "n_2 = 10 # number of classes\n",
        "nb_epoch = 10 # number of epochs (number of iterations over full training set)\n",
        "alpha=0.1 # learning rate\n",
        "beta=0.9 # beta parameters for momentum\n",
        "\n",
        "\n",
        "# --- START CODE HERE (06)\n",
        "\n",
        "model = # Define the model for 10 classes\n",
        "optimizer = # Define the optimizer\n",
        "loss_fn = # Define the criterion\n",
        "\n",
        "# --- END CODE HERE\n",
        "\n",
        "print(\"model:\\n\", model)\n",
        "print(\"optimizer:\\n\", optimizer)\n",
        "\n",
        "train_loss, train_accuracy, test_loss, test_accuracy = [], [], [], []\n",
        "\n",
        "# Run over epochs\n",
        "for num_epoch in range(1, nb_epoch + 1):\n",
        "  model.train()\n",
        "  for batch_idx, batch in enumerate(train_dataloader):\n",
        "    # --- Forward\n",
        "    X, y = batch\n",
        "\n",
        "    # --- START CODE HERE (07)\n",
        "\n",
        "    train_preds =\n",
        "    loss = \n",
        "\n",
        "    # --- END CODE HERE\n",
        "\n",
        "    accuracy = F_computeAccuracy(train_preds, y)\n",
        "    \n",
        "    # --- Store results on train\n",
        "    train_loss.append(loss.item())\n",
        "    train_accuracy.append(accuracy)\n",
        "    \n",
        "    # --- Backward\n",
        "    # --- START CODE HERE (08)\n",
        "\n",
        "    \n",
        "\n",
        "    # --- END CODE HERE\n",
        "    \n",
        "    # --- Update parameters\n",
        "    # --- START CODE HERE (09)\n",
        "\n",
        "\n",
        "\n",
        "    # --- END CODE HERE\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    for batch_idx, batch in enumerate(test_dataloader):\n",
        "      # --- Store results on test\n",
        "      X, y = batch\n",
        "      test_preds = model(X)\n",
        "      loss = loss_fn(test_preds, y)\n",
        "      accuracy = F_computeAccuracy(test_preds, y)\n",
        "      test_loss.append(loss.item())    \n",
        "      test_accuracy.append(accuracy)\n",
        "    \n",
        "  if (num_epoch % 1)==0: \n",
        "      print(\"epoch: {0:d} (loss: train {1:.2f} test {2:.2f}) (accuracy: train {3:.2f} test {4:.2f})\".format(num_epoch, train_loss[-1], test_loss[-1], train_accuracy[-1], test_accuracy[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ndv7Ovhyr05V"
      },
      "outputs": [],
      "source": [
        "display_losses_and_accuracies(train_loss, test_loss, train_accuracy, test_accuracy)"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
