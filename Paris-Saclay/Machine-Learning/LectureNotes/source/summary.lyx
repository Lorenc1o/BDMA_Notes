#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{footmisc}
\usepackage{listings}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1,0.99,0.94}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,  
    frame=single,
    tabsize=2
}

\lstset{style=mystyle}
\end_preamble
\use_default_options true
\begin_modules
tcolorbox
customHeadersFooters
theorems-ams-bytype
theorems-sec-bytype
algorithm2e
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "linkcolor=blue, urlcolor=blue, citecolor=blue, pdfstartview={FitH}, unicode=true"
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\boxbgcolor #62a0ea
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 3cm
\rightmargin 2cm
\bottommargin 3cm
\headheight 2cm
\headsep 1cm
\footskip 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
BDMA - Machine Learning
\end_layout

\begin_layout Date
Fall 2023
\end_layout

\begin_layout Author
Jose Antonio Lorencio Abril
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../../../Decision-Modeling/LectureNotes/source/CS-logo.png
	scale 70

\end_inset


\end_layout

\begin_layout Standard
\align right
Professor: Tom Dupuis
\end_layout

\begin_layout Standard
\align right
Student e-mail: jose-antonio.lorencio-abril@student-cs.fr
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Address
This is a summary of the course 
\emph on
Machine Learning
\emph default
 taught at the Université Paris Saclay - CentraleSupélec by Professor Tom
 Dupuis in the academic year 23/24.
 Most of the content of this document is adapted from the course notes by
 Dupuis, 
\begin_inset CommandInset citation
LatexCommand cite
key "Dupuis2023"
literal "false"

\end_inset

, so I won't be citing it all the time.
 Other references will be provided when used.
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Part
Deep Learning
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard

\series bold
Artificial Intelligence
\series default
 is a wide concept, encompassing different aspects and fields.
 We can understand the term AI as the multidisciplinary field of study that
 aims at recreating human intelligence using artificial means.
 This is a bit abstract, and, in fact, there is no single definition for
 what this means.
 Intelligence is not fully understood, and thus it is hard to assess whether
 an artificial invention has achieved intelligence, further than intuitively
 thinking so.
\end_layout

\begin_layout Standard
For instance, AI involves a whole variety of fields:
\end_layout

\begin_layout Itemize
Perception
\end_layout

\begin_layout Itemize
Knowledge
\end_layout

\begin_layout Itemize
Cognitive System
\end_layout

\begin_layout Itemize
Planning
\end_layout

\begin_layout Itemize
Robotics
\end_layout

\begin_layout Itemize
Machine Learning (Neural Networks)
\end_layout

\begin_layout Itemize
Natural Language Processing
\end_layout

\begin_layout Standard
Leveraging all of these, people try to recreate or even surpass human performanc
e in different tasks.
 For example, a computer program that can play chess better than any human
 could ever possibly play, such as Stockfish, or a system that is able to
 understand our messages and reply, based on the knowledge that it has learnt
 in the past, such as ChatGPT and similar tools.
 Other examples are self-driving cars, auto-controlled robots, etc.
\end_layout

\begin_layout Standard
Therefore, AI is a very wide term, which merges many different scientific
 fields.
 
\series bold
Machine Learning
\series default
, on the other side, is a narrower term, which deals with the study of the
 techniques that we can use to make a computer learn to perform some task.
 It takes concepts from Statistics, Optimization Theory, Computer Science,
 Algorithms, etc.
 A relevant subclass of Machine Learning, which has come to be one of the
 most prominent fields of research in the recent years, is 
\series bold
Neural Networks 
\series default
or 
\series bold
Deep Learning
\series default
, which consists on an ML technique based on the human brain.
 Many amazing use cases that we see everywhere, like Siri (Apple assistant),
 Cortana (Windows assistant), Amazon recommender system, Dall-E (OpenAI
 image generation system), etc.
 Not only this, but the trend is growing, and the interest in DL is continuously
 increasing.
\end_layout

\begin_layout Standard
This is partly also due to the increase in computing resources, and the
 continuous optimization that different techniques are constantly experiencing.
 For instance, for a model trained on one trillion data points, in 2021
 the training process required around 16500x less compute than a model trained
 in 2012.
\end_layout

\begin_layout Standard
But not everything is sweet and roses when using DL.
 Since these systems are being involved in decision making processes, there
 are some questions that arise, like whose responsibility is it when a model
 fails? Moreover, data is needed to train the models, so it is relevant
 to address how datasets should be collected, and to respect the privacy
 of the people that produce data.
 In addition, the recent technologies that are able to generate new content
 and to modify real content, make it a new issue that AI can create false
 information, mistrust, and even violence or paranoia.
\end_layout

\begin_layout Standard
Nonetheless, let's not focus on the negative, there are lots of nice application
s of DL, and it is a key component to deal with data, achieving higher performan
ce than traditional ML techniques for huge amount of data.
\end_layout

\begin_layout Subsection
AI History
\end_layout

\begin_layout Standard
In 1950, Alan turing aimed to answer the question '
\emph on
Can machines think?
\emph default
' through a test, which came to be named the 
\series bold
Turing Test
\series default
, and consists in a 3 players game.
 First, a similar game is the following: 2 talkers, a man and a female,
 and 1 interrogator.
 The interrogator asks questions to the talkers, with the aim of determining
 who is the man and who is the female.
 The man tries to trick the interrogator, while the woman tries to help
 him to identify her.
\end_layout

\begin_layout Standard
Then, the Turing Test consists in replacing the man by an artificial machine.
 Turing thought that a machine that could trick a human interrogator, should
 be considered intelligent.
\end_layout

\begin_layout Standard
Later, in 1956, in the Dartmouth Workshop organized by IBM, the term 
\series bold
Artificial Intelligence
\series default
 was first used to describe 
\emph on
every aspect of learning or any other feature of intelligence can be so
 precisely described that a machine can be made to simulate it
\emph default
.
\end_layout

\begin_layout Standard
From this year on, there was a focus on researching about 
\series bold
Symbolic AI
\series default
, specially in three areas of research:
\end_layout

\begin_layout Itemize
Reasoning as search: a different set of actions leads to a certain goal,
 so we can try to find the best choice of action to obtain the best possible
 outcome.
\end_layout

\begin_layout Itemize
Natural Language: different tools were developed, following grammar and
 language rules.
\end_layout

\begin_layout Itemize
Micro world: small block based worlds, that the system can identify and
 move.
\end_layout

\begin_layout Standard
In 1958, the 
\series bold
Perceptron
\series default
 was conceived, giving birth to what is called the connectionism, an approach
 to AI based on the human brain, and a big hype that encouraged funding
 to support AI research.
 At this era, scientists experience a bit of lack of perspective, thinking
 that the power of AI was much higher than it was.
 For instance, H.
 A.
 Simon stated in 1965 that '
\emph on
machines will be capable, within twenty years, of doing any work a man can
 do.
\emph default
' We can relate to our time, with the huge hype that AI is experiencing,
 as well as the many apocaliptic theories that some people are making.
 Maybe we are again overestimating the power of AI.
\end_layout

\begin_layout Standard
The time from 1974 to 1980 is seen as the first winter of AI, in which research
 was slowed down and funding was reduced.
 This was due to several problems found at the time:
\end_layout

\begin_layout Itemize
There were few computational resources.
\end_layout

\begin_layout Itemize
The models at the time were not scalable.
\end_layout

\begin_layout Itemize
The Moravec's paradox: it is comparatively easy to make computers exhibit
 adult level performance on intelligence test or playing checkers, and difficult
 or impossible to give them the skills of a one-year-old when it comes to
 perception and mobility.
\end_layout

\begin_layout Itemize
Marvin Minsky made some devastating critics to connectionism, compared to
 symbolic, rule-based models:
\end_layout

\begin_deeper
\begin_layout Itemize
Limited capacity: Minsky showed that single-layer perceptrons (a simple
 kind of neural network) could not solve certain classes of problems, like
 the XOR problem.
 While it was later shown that multi-layer perceptrons could solve these
 problems, Minsky's work resulted in a shift away from neural networks for
 a time.
\end_layout

\begin_layout Itemize
Lack of clear symbols: Minsky believed that human cognition operates at
 a higher level with symbols and structures (like frames and scripts), rather
 than just distributed patterns of activation.
 He often argued that connectionist models lacked a clear way to represent
 these symbolic structures.
\end_layout

\begin_layout Itemize
Generalization and Abstraction: Minsky was concerned that connectionist
 models struggled with generalizing beyond specific training examples or
 abstracting high-level concepts from raw data.
\end_layout

\begin_layout Itemize
Inefficiency: Minsky pointed out that many problems which seemed simple
 for symbolic models could be extremely computationally intensive for connection
ist models.
\end_layout

\begin_layout Itemize
Lack of explanation: Connectionist models, especially when they become complex,
 can be seen as "black boxes", making it difficult to interpret how they
 arrive at specific conclusions.
\end_layout

\begin_layout Itemize
Over-reliance on learning: Minsky believed that not all knowledge comes
 from learning from scratch, and some of it might be innate or structured
 in advance.
 He felt connectionism put too much emphasis on learning from raw data.
\end_layout

\end_deeper
\begin_layout Standard
In 1980, there was a boom in expert knowledge systems that made AI recover
 interest.
 An 
\series bold
expert system
\series default
 solves specific tasks following an ensemble of rules based on knowledge
 facilitated by experts.
 A remarkable use case was the XCON sorting system, developed for the Digital
 Equipment Corporation, which helped them save 40M$ per year.
 In addition, connectionism also came again on scene, thanks to the development
 of 
\series bold
backpropagation
\series default
 applied to neurons, by Geoffrey Hinton.
 All these achievement made funding to come back to the field.
\end_layout

\begin_layout Standard
Nonetheless, there came a second winter of AI, from 1987 to 1994, mainly
 because several companies were disappointed and AI was seen as a technology
 that couldn't solve wide varieties of tasks.
 The funding was withdrawn from the field and a lot AI companies went bankrupt.
\end_layout

\begin_layout Standard
Luckily, from 1995 there started a new return of AI in the industry.
 The Moore's Law states that speed and memory of computer doubles every
 two years, and so computing power and memory was rapidly increasing, making
 the use of AI systems more feasible each year.
 During this time, many new concepts were introduced, such as 
\series bold
intelligent agents
\series default
 as systems that perceive their environment and take actions which maximize
 their chances of success; or different 
\series bold
probabilistic reasoning tools
\series default
 such as Bayesian networks, hidden Markov models, information theory, SVM,...
 In addition, AI researchers started to reframe their work in terms of mathemati
cs, computer science, physics, etc., making the field more attractive for
 funding.
 A remarkable milestone during this time was the victory of Deep Blue against
 Garry Kasparov.
\end_layout

\begin_layout Standard
The last era of AI comes from 2011 to today, with the advent and popularization
 of 
\series bold
Deep Learning
\series default
 (DL), which are deep graph processing layers mimicking human neurons interactio
ns.
 This happened thanks to the advances of hardware technologies, that have
 enabled the enormous computing requirements needed for DL.
 The huge hype comes from the spectacular results shown by this kind of
 systems in a huge variety of tasks, such as computer vision, natural language
 processing, anomaly detection,...
\end_layout

\begin_layout Standard
In summary, we can see how the history of AI has been a succession of hype
 and dissapointment cycles, with many actors involved and the industry as
 a very important part of the process.
\end_layout

\begin_layout Section
Machine Learning Basics
\end_layout

\begin_layout Section
Deep Neural Networks
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Part
Reinforcement Learning
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "ps-cs"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
